{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Our import: \n",
    "import nltk\n",
    "from nltk.probability import ConditionalFreqDist\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import math\n",
    "from nltk import word_tokenize\n",
    "import numpy as np\n",
    "import numpy\n",
    "import re\n",
    "\n",
    "import sys\n",
    "\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "##Our defines: \n",
    "q1Verbose=1\n",
    "q113_verbose=0\n",
    "q2Verbose=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. Document Classification\n",
    "#### Q1.1. Reuters Dataset\n",
    "\n",
    "##### Q1.1.1 Turn the code of the Sklearn tutorial above into a notebook.\n",
    "\n",
    "This code is taken from the out of core classification guide given in the assigmnent. \n",
    "http://scikit-learn.org/dev/auto_examples/applications/plot_out_of_core_classification.html#example-applications-plot-out-of-core-classification-py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Authors: Eustache Diemert <eustache@diemert.fr>\n",
    "#          @FedericoV <https://github.com/FedericoV/>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from glob import glob\n",
    "import itertools\n",
    "import os.path\n",
    "import re\n",
    "import tarfile\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "\n",
    "from sklearn.externals.six.moves import html_parser\n",
    "from sklearn.externals.six.moves import urllib\n",
    "from sklearn.datasets import get_data_home\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _not_in_sphinx():\n",
    "    # Hack to detect whether we are running by the sphinx builder\n",
    "    return '__file__' in globals()\n",
    "\n",
    "%matplotlib inline\n",
    "###############################################################################\n",
    "# Reuters Dataset related routines\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "class ReutersParser(html_parser.HTMLParser):\n",
    "    \"\"\"Utility class to parse a SGML file and yield documents one at a time.\"\"\"\n",
    "\n",
    "    def __init__(self, encoding='latin-1'):\n",
    "        html_parser.HTMLParser.__init__(self)\n",
    "        self._reset()\n",
    "        self.encoding = encoding\n",
    "\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        method = 'start_' + tag\n",
    "        getattr(self, method, lambda x: None)(attrs)\n",
    "\n",
    "    def handle_endtag(self, tag):\n",
    "        method = 'end_' + tag\n",
    "        getattr(self, method, lambda: None)()\n",
    "\n",
    "    def _reset(self):\n",
    "        self.in_title = 0\n",
    "        self.in_body = 0\n",
    "        self.in_topics = 0\n",
    "        self.in_topic_d = 0\n",
    "        self.title = \"\"\n",
    "        self.body = \"\"\n",
    "        self.topics = []\n",
    "        self.topic_d = \"\"\n",
    "\n",
    "    def parse(self, fd):\n",
    "        self.docs = []\n",
    "        for chunk in fd:\n",
    "            self.feed(chunk.decode(self.encoding))\n",
    "            for doc in self.docs:\n",
    "                yield doc\n",
    "            self.docs = []\n",
    "        self.close()\n",
    "\n",
    "    def handle_data(self, data):\n",
    "        if self.in_body:\n",
    "            self.body += data\n",
    "        elif self.in_title:\n",
    "            self.title += data\n",
    "        elif self.in_topic_d:\n",
    "            self.topic_d += data\n",
    "\n",
    "    def start_reuters(self, attributes):\n",
    "        pass\n",
    "\n",
    "    def end_reuters(self):\n",
    "        self.body = re.sub(r'\\s+', r' ', self.body)\n",
    "        self.docs.append({'title': self.title,\n",
    "                          'body': self.body,\n",
    "                          'topics': self.topics})\n",
    "        self._reset()\n",
    "\n",
    "    def start_title(self, attributes):\n",
    "        self.in_title = 1\n",
    "\n",
    "    def end_title(self):\n",
    "        self.in_title = 0\n",
    "\n",
    "    def start_body(self, attributes):\n",
    "        self.in_body = 1\n",
    "\n",
    "    def end_body(self):\n",
    "        self.in_body = 0\n",
    "\n",
    "    def start_topics(self, attributes):\n",
    "        self.in_topics = 1\n",
    "\n",
    "    def end_topics(self):\n",
    "        self.in_topics = 0\n",
    "\n",
    "    def start_d(self, attributes):\n",
    "        self.in_topic_d = 1\n",
    "\n",
    "    def end_d(self):\n",
    "        self.in_topic_d = 0\n",
    "        self.topics.append(self.topic_d)\n",
    "        self.topic_d = \"\"\n",
    "\n",
    "\n",
    "def stream_reuters_documents(data_path=None):\n",
    "    \"\"\"Iterate over documents of the Reuters dataset.\n",
    "\n",
    "    The Reuters archive will automatically be downloaded and uncompressed if\n",
    "    the `data_path` directory does not exist.\n",
    "\n",
    "    Documents are represented as dictionaries with 'body' (str),\n",
    "    'title' (str), 'topics' (list(str)) keys.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    DOWNLOAD_URL = ('http://archive.ics.uci.edu/ml/machine-learning-databases/'\n",
    "                    'reuters21578-mld/reuters21578.tar.gz')\n",
    "    ARCHIVE_FILENAME = 'reuters21578.tar.gz'\n",
    "\n",
    "    if data_path is None:\n",
    "        data_path = os.path.join(get_data_home(), \"reuters\")\n",
    "    if not os.path.exists(data_path):\n",
    "        \"\"\"Download the dataset.\"\"\"\n",
    "        print(\"downloading dataset (once and for all) into %s\" %\n",
    "              data_path)\n",
    "        os.mkdir(data_path)\n",
    "\n",
    "        def progress(blocknum, bs, size):\n",
    "            total_sz_mb = '%.2f MB' % (size / 1e6)\n",
    "            current_sz_mb = '%.2f MB' % ((blocknum * bs) / 1e6)\n",
    "            if _not_in_sphinx():\n",
    "                print('\\rdownloaded %s / %s' % (current_sz_mb, total_sz_mb),\n",
    "                      end='')\n",
    "\n",
    "        archive_path = os.path.join(data_path, ARCHIVE_FILENAME)\n",
    "        urllib.request.urlretrieve(DOWNLOAD_URL, filename=archive_path,\n",
    "                                   reporthook=progress)\n",
    "        if _not_in_sphinx():\n",
    "            print('\\r', end='')\n",
    "        print(\"untarring Reuters dataset...\")\n",
    "        tarfile.open(archive_path, 'r:gz').extractall(data_path)\n",
    "        print(\"done.\")\n",
    "\n",
    "    parser = ReutersParser()\n",
    "    for filename in glob(os.path.join(data_path, \"*.sgm\")):\n",
    "        for doc in parser.parse(open(filename, 'rb')):\n",
    "            yield doc\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Main\n",
    "###############################################################################\n",
    "# Create the vectorizer and limit the number of features to a reasonable\n",
    "# maximum\n",
    "vectorizer = HashingVectorizer(decode_error='ignore', n_features=2 ** 18,\n",
    "                               non_negative=True)\n",
    "\n",
    "\n",
    "# Iterator over parsed Reuters SGML files.\n",
    "data_stream = stream_reuters_documents()\n",
    "\n",
    "# We learn a binary classification between the \"acq\" class and all the others.\n",
    "# \"acq\" was chosen as it is more or less evenly distributed in the Reuters\n",
    "# files. For other datasets, one should take care of creating a test set with\n",
    "# a realistic portion of positive instances.\n",
    "all_classes = np.array([0, 1])\n",
    "positive_class = 'acq'\n",
    "\n",
    "# Here are some classifiers that support the `partial_fit` method\n",
    "partial_fit_classifiers = {\n",
    "    'SGD': SGDClassifier(),\n",
    "    'Perceptron': Perceptron(),\n",
    "    'NB Multinomial': MultinomialNB(alpha=0.01),\n",
    "    'Passive-Aggressive': PassiveAggressiveClassifier(),\n",
    "}\n",
    "\n",
    "\n",
    "def get_minibatch(doc_iter, size, pos_class=positive_class):\n",
    "    \"\"\"Extract a minibatch of examples, return a tuple X_text, y.\n",
    "\n",
    "    Note: size is before excluding invalid docs with no topics assigned.\n",
    "\n",
    "    \"\"\"\n",
    "    data = [(u'{title}\\n\\n{body}'.format(**doc), pos_class in doc['topics'])\n",
    "            for doc in itertools.islice(doc_iter, size)\n",
    "            if doc['topics']]\n",
    "    if not len(data):\n",
    "        return np.asarray([], dtype=int), np.asarray([], dtype=int)\n",
    "    X_text, y = zip(*data)\n",
    "    return X_text, np.asarray(y, dtype=int)\n",
    "\n",
    "\n",
    "def iter_minibatches(doc_iter, minibatch_size):\n",
    "    \"\"\"Generator of minibatches.\"\"\"\n",
    "    X_text, y = get_minibatch(doc_iter, minibatch_size)\n",
    "    while len(X_text):\n",
    "        yield X_text, y\n",
    "        X_text, y = get_minibatch(doc_iter, minibatch_size)\n",
    "\n",
    "\n",
    "# test data statistics\n",
    "test_stats = {'n_test': 0, 'n_test_pos': 0}\n",
    "\n",
    "# First we hold out a number of examples to estimate accuracy\n",
    "n_test_documents = 1000\n",
    "tick = time.time()\n",
    "X_test_text, y_test = get_minibatch(data_stream, 1000)\n",
    "parsing_time = time.time() - tick\n",
    "tick = time.time()\n",
    "X_test = vectorizer.transform(X_test_text)\n",
    "vectorizing_time = time.time() - tick\n",
    "test_stats['n_test'] += len(y_test)\n",
    "test_stats['n_test_pos'] += sum(y_test)\n",
    "print(\"Test set is %d documents (%d positive)\" % (len(y_test), sum(y_test)))\n",
    "\n",
    "\n",
    "def progress(cls_name, stats):\n",
    "    \"\"\"Report progress information, return a string.\"\"\"\n",
    "    duration = time.time() - stats['t0']\n",
    "    s = \"%20s classifier : \\t\" % cls_name\n",
    "    s += \"%(n_train)6d train docs (%(n_train_pos)6d positive) \" % stats\n",
    "    s += \"%(n_test)6d test docs (%(n_test_pos)6d positive) \" % test_stats\n",
    "    s += \"accuracy: %(accuracy).3f \" % stats\n",
    "    s += \"in %.2fs (%5d docs/s)\" % (duration, stats['n_train'] / duration)\n",
    "    return s\n",
    "\n",
    "\n",
    "cls_stats = {}\n",
    "\n",
    "for cls_name in partial_fit_classifiers:\n",
    "    stats = {'n_train': 0, 'n_train_pos': 0,\n",
    "             'accuracy': 0.0, 'accuracy_history': [(0, 0)], 't0': time.time(),\n",
    "             'runtime_history': [(0, 0)], 'total_fit_time': 0.0}\n",
    "    cls_stats[cls_name] = stats\n",
    "\n",
    "get_minibatch(data_stream, n_test_documents)\n",
    "# Discard test set\n",
    "\n",
    "# We will feed the classifier with mini-batches of 1000 documents; this means\n",
    "# we have at most 1000 docs in memory at any time.  The smaller the document\n",
    "# batch, the bigger the relative overhead of the partial fit methods.\n",
    "minibatch_size = 1000\n",
    "\n",
    "# Create the data_stream that parses Reuters SGML files and iterates on\n",
    "# documents as a stream.\n",
    "minibatch_iterators = iter_minibatches(data_stream, minibatch_size)\n",
    "total_vect_time = 0.0\n",
    "\n",
    "# Main loop : iterate on mini-batchs of examples\n",
    "for i, (X_train_text, y_train) in enumerate(minibatch_iterators):\n",
    "\n",
    "    tick = time.time()\n",
    "    X_train = vectorizer.transform(X_train_text)\n",
    "    total_vect_time += time.time() - tick\n",
    "\n",
    "    for cls_name, cls in partial_fit_classifiers.items():\n",
    "        tick = time.time()\n",
    "        # update estimator with examples in the current mini-batch\n",
    "        cls.partial_fit(X_train, y_train, classes=all_classes)\n",
    "\n",
    "        # accumulate test accuracy stats\n",
    "        cls_stats[cls_name]['total_fit_time'] += time.time() - tick\n",
    "        cls_stats[cls_name]['n_train'] += X_train.shape[0]\n",
    "        cls_stats[cls_name]['n_train_pos'] += sum(y_train)\n",
    "        tick = time.time()\n",
    "        cls_stats[cls_name]['accuracy'] = cls.score(X_test, y_test)\n",
    "        cls_stats[cls_name]['prediction_time'] = time.time() - tick\n",
    "        acc_history = (cls_stats[cls_name]['accuracy'],\n",
    "                       cls_stats[cls_name]['n_train'])\n",
    "        cls_stats[cls_name]['accuracy_history'].append(acc_history)\n",
    "        run_history = (cls_stats[cls_name]['accuracy'],\n",
    "                       total_vect_time + cls_stats[cls_name]['total_fit_time'])\n",
    "        cls_stats[cls_name]['runtime_history'].append(run_history)\n",
    "\n",
    "        if i % 3 == 0:\n",
    "            print(progress(cls_name, cls_stats[cls_name]))\n",
    "    if i % 3 == 0:\n",
    "        print('\\n')\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Plot results\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "def plot_accuracy(x, y, x_legend):\n",
    "    \"\"\"Plot accuracy as a function of x.\"\"\"\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    plt.title('Classification accuracy as a function of %s' % x_legend)\n",
    "    plt.xlabel('%s' % x_legend)\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.grid(True)\n",
    "    plt.plot(x, y)\n",
    "\n",
    "rcParams['legend.fontsize'] = 10\n",
    "cls_names = list(sorted(cls_stats.keys()))\n",
    "\n",
    "# Plot accuracy evolution\n",
    "plt.figure()\n",
    "for _, stats in sorted(cls_stats.items()):\n",
    "    # Plot accuracy evolution with #examples\n",
    "    accuracy, n_examples = zip(*stats['accuracy_history'])\n",
    "    plot_accuracy(n_examples, accuracy, \"training examples (#)\")\n",
    "    ax = plt.gca()\n",
    "    ax.set_ylim((0.8, 1))\n",
    "plt.legend(cls_names, loc='best')\n",
    "\n",
    "plt.figure()\n",
    "for _, stats in sorted(cls_stats.items()):\n",
    "    # Plot accuracy evolution with runtime\n",
    "    accuracy, runtime = zip(*stats['runtime_history'])\n",
    "    plot_accuracy(runtime, accuracy, 'runtime (s)')\n",
    "    ax = plt.gca()\n",
    "    ax.set_ylim((0.8, 1))\n",
    "plt.legend(cls_names, loc='best')\n",
    "\n",
    "# Plot fitting times\n",
    "plt.figure()\n",
    "fig = plt.gcf()\n",
    "cls_runtime = []\n",
    "for cls_name, stats in sorted(cls_stats.items()):\n",
    "    cls_runtime.append(stats['total_fit_time'])\n",
    "\n",
    "cls_runtime.append(total_vect_time)\n",
    "cls_names.append('Vectorization')\n",
    "bar_colors = rcParams['axes.color_cycle'][:len(cls_names)]\n",
    "\n",
    "ax = plt.subplot(111)\n",
    "rectangles = plt.bar(range(len(cls_names)), cls_runtime, width=0.5,\n",
    "                     color=bar_colors)\n",
    "\n",
    "ax.set_xticks(np.linspace(0.25, len(cls_names) - 0.75, len(cls_names)))\n",
    "ax.set_xticklabels(cls_names, fontsize=10)\n",
    "ymax = max(cls_runtime) * 1.2\n",
    "ax.set_ylim((0, ymax))\n",
    "ax.set_ylabel('runtime (s)')\n",
    "ax.set_title('Training Times')\n",
    "\n",
    "\n",
    "def autolabel(rectangles):\n",
    "    \"\"\"attach some text vi autolabel on rectangles.\"\"\"\n",
    "    for rect in rectangles:\n",
    "        height = rect.get_height()\n",
    "        ax.text(rect.get_x() + rect.get_width() / 2.,\n",
    "                1.05 * height, '%.4f' % height,\n",
    "                ha='center', va='bottom')\n",
    "\n",
    "autolabel(rectangles)\n",
    "plt.show()\n",
    "\n",
    "# Plot prediction times\n",
    "plt.figure()\n",
    "#fig = plt.gcf()\n",
    "cls_runtime = []\n",
    "cls_names = list(sorted(cls_stats.keys()))\n",
    "for cls_name, stats in sorted(cls_stats.items()):\n",
    "    cls_runtime.append(stats['prediction_time'])\n",
    "cls_runtime.append(parsing_time)\n",
    "cls_names.append('Read/Parse\\n+Feat.Extr.')\n",
    "cls_runtime.append(vectorizing_time)\n",
    "cls_names.append('Hashing\\n+Vect.')\n",
    "bar_colors = rcParams['axes.color_cycle'][:len(cls_names)]\n",
    "\n",
    "ax = plt.subplot(111)\n",
    "rectangles = plt.bar(range(len(cls_names)), cls_runtime, width=0.5,\n",
    "                     color=bar_colors)\n",
    "\n",
    "ax.set_xticks(np.linspace(0.25, len(cls_names) - 0.75, len(cls_names)))\n",
    "ax.set_xticklabels(cls_names, fontsize=8)\n",
    "plt.setp(plt.xticks()[1], rotation=30)\n",
    "ymax = max(cls_runtime) * 1.2\n",
    "ax.set_ylim((0, ymax))\n",
    "ax.set_ylabel('runtime (s)')\n",
    "ax.set_title('Prediction Times (%d instances)' % n_test_documents)\n",
    "autolabel(rectangles)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q1.1.2 Explore how many documents are in the dataset, how many categories, how many documents per categories, provide mean and standard deviation, min and max. (Hint: use the pandas library to explore the dataset, use the dataframe.describe() method.)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Note: installed modules: pandas, request, dict, public, self, get, query_string, post\n",
    "~~Tried to import panda instead of pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of df is:  <class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>title</th>\n",
       "      <th>topics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chrysler Corp said car sales for the March 21-...</td>\n",
       "      <td>CHRYSLER &lt;C&gt; LATE MARCH U.S. CAR SALES UP</td>\n",
       "      <td>[usa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Compaq Computer Corp, IBM's chief rival in the...</td>\n",
       "      <td>WALL STREET STOCKS/COMPAQ COMPUTER &lt;CPQ&gt;</td>\n",
       "      <td>[usa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;Noranda Inc&gt; said production will remain shut...</td>\n",
       "      <td>NORANDA SETS TEMPORARY MINE SHUTDOWN</td>\n",
       "      <td>[copper, canada]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Canadian government's budget deficit rose ...</td>\n",
       "      <td>CANADA BUDGET DEFICIT RISES IN JANUARY</td>\n",
       "      <td>[canada]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CIS Technologies Inc said it executed a formal...</td>\n",
       "      <td>CIS TECHNOLOGIES&lt;CIH&gt; TO SELL SHARES TO SWISS CO</td>\n",
       "      <td>[acq, usa, switzerland]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Qtly div 42 cts vs 41.5 cts prior Payable APri...</td>\n",
       "      <td>COPLEY PROPERTIES INC &lt;COP&gt; INCREASES DIVIDEND</td>\n",
       "      <td>[earn, usa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Colombia's cost of living index rose 2.71 pct ...</td>\n",
       "      <td>COLOMBIAN INFLATION STABLE AT AROUND 20 PCT</td>\n",
       "      <td>[cpi, colombia]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Federal Home Loan Bank Board said home mor...</td>\n",
       "      <td>FHLBB SAYS MORTGAGE RATES CONTINUE DECLINE</td>\n",
       "      <td>[interest, usa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The New York Stock Exchange said a seat on the...</td>\n",
       "      <td>NYFE SEAT SELLS FOR 1,500 DLRS</td>\n",
       "      <td>[usa, nyse]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td></td>\n",
       "      <td>CANADIAN MONEY SUPPLY M-1 FALLS 291 MLN DLRS I...</td>\n",
       "      <td>[money-supply, canada]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Beneficial Corp said the sale of its American ...</td>\n",
       "      <td>BENEFICIAL &lt;BNL&gt; UNIT SALE APPROVED</td>\n",
       "      <td>[acq, usa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>European options exchanges will see spectacula...</td>\n",
       "      <td>LARGER VOLUME SEEN ON EUROPEAN OPTIONS EXCHANGES</td>\n",
       "      <td>[netherlands, ase, cboe]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Tierco Group INc said it sold at par to the Ku...</td>\n",
       "      <td>TIERCO &lt;TIER&gt; SELLS NOTE</td>\n",
       "      <td>[usa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Shr 5.56 dlrs vs 3.88 dlrs Net 47.5 mln vs 33....</td>\n",
       "      <td>&lt;ITT CANADA LTD&gt; YEAR NET</td>\n",
       "      <td>[earn, canada]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>California Micro Devices Corp said an addition...</td>\n",
       "      <td>CALIFORNIA MICRO DEVICES &lt;CAMD&gt; IN DEFENSE DEAL</td>\n",
       "      <td>[usa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Stewart INformation Services Corp said it resc...</td>\n",
       "      <td>STEWART INFORMATION RESCHEDULES ANNUAL MEETING</td>\n",
       "      <td>[usa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>FIserve Inc said 14 savings and loans with 1.5...</td>\n",
       "      <td>FISERVE &lt;FISV&gt; GETS BUSINESS WORTH ONE MLN DLRS</td>\n",
       "      <td>[usa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Mony Real Estate Investors Trust said its inve...</td>\n",
       "      <td>MONY REAL &lt;MYM&gt; REPORTS PORTFOLIO</td>\n",
       "      <td>[usa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Canadian narrowly-defined money supply M-1 fel...</td>\n",
       "      <td>CANADIAN MONEY SUPPLY FALLS IN WEEK</td>\n",
       "      <td>[money-supply, canada]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Copley Properties Inc said the company will in...</td>\n",
       "      <td>COPLEY PROPERTIES &lt;COP&gt; TO INVEST IN JOINT PACT</td>\n",
       "      <td>[usa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>The United Food And Commercial Workers said th...</td>\n",
       "      <td>UNION TO PROTEST DART'S SUPERMARKETS &lt;SGL&gt; BID</td>\n",
       "      <td>[acq, usa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Pan Am Corp's Pan American World Airways said ...</td>\n",
       "      <td>PAN AM &lt;PN&gt; MARCH LOAD FACTOR ROSE TO 60.6 PCT</td>\n",
       "      <td>[usa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>FCS Laboratories Inc said merger discussions w...</td>\n",
       "      <td>FCS LABORATORIES &lt;FCSI&gt; TERMINATES DEAL TALKS</td>\n",
       "      <td>[acq, usa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>The White House said the rise in interest rate...</td>\n",
       "      <td>WHITE HOUSE SAYS INTEREST RATES REFLECT MARKET</td>\n",
       "      <td>[interest, usa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Shr loss nine cts Net loss 1.4 mln Revs 630,11...</td>\n",
       "      <td>REGAL PETROLEUM LTD &lt;RPLO&gt; YEAR</td>\n",
       "      <td>[earn, usa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Shr 1.1 cts vs 1.7 cts Net 26,708 vs 35,084 Re...</td>\n",
       "      <td>CANTERBURY PRESS INC YEAR NOV 30</td>\n",
       "      <td>[earn, usa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Shr nine cts vs 19 cts Net 188,000 vs 362,000 ...</td>\n",
       "      <td>GRAPHIC MEDIA INC &lt;GMED&gt; YEAR</td>\n",
       "      <td>[earn, usa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Burlington Northern Inc said its Burlington No...</td>\n",
       "      <td>BURLINGTON &lt;BNI&gt; UNIT SETTLES BONDHOLDER SUIT</td>\n",
       "      <td>[usa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21548</th>\n",
       "      <td>Hundreds of marines were on alert at 11 key Br...</td>\n",
       "      <td>BRAZIL SEAMEN CONTINUE STRIKE AFTER COURT DECI...</td>\n",
       "      <td>[ship, brazil]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21549</th>\n",
       "      <td>Indonesia's Armed Forces Commander General Ben...</td>\n",
       "      <td>INDONESIA SAID TO BE STABLE AHEAD OF ELECTIONS</td>\n",
       "      <td>[indonesia]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21550</th>\n",
       "      <td>The suspension of Ecuador's crude oil shipment...</td>\n",
       "      <td>ECUADOR TO EXPORT NO OIL FOR FOUR MONTHS, OFFI...</td>\n",
       "      <td>[crude, ecuador]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21551</th>\n",
       "      <td>Any European Community decision to liberalise ...</td>\n",
       "      <td>EC FARM LIBERALISATION SEEN HURTING THAI TAPIOCA</td>\n",
       "      <td>[tapioca, meal-feed, thailand, ec]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21552</th>\n",
       "      <td>China has raised the prices it pays farmers fo...</td>\n",
       "      <td>CHINA RAISES CROP PRICES TO INCREASE OUTPUT</td>\n",
       "      <td>[cotton, sugar, veg-oil, grain, china]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21553</th>\n",
       "      <td>Australia sold 180,000 tonnes of raw sugar to ...</td>\n",
       "      <td>AUSTRALIA SELLS 180,000 TONNES OF SUGAR TO USSR</td>\n",
       "      <td>[sugar, ussr, australia]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21554</th>\n",
       "      <td>Thai natural rubber exports rose to 763,331 to...</td>\n",
       "      <td>THAI NATURAL RUBBER EXPORTS RISE IN 1986</td>\n",
       "      <td>[rubber, thailand]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21555</th>\n",
       "      <td>Australian beef output declined to 104,353 ton...</td>\n",
       "      <td>AUSTRALIAN BEEF OUTPUT DECLINES IN JANUARY</td>\n",
       "      <td>[carcass, livestock, australia]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21556</th>\n",
       "      <td>Increased federal government borrowing needs a...</td>\n",
       "      <td>GERMAN GOVERNMENT NEEDS SEEN RAISING BOND YIELDS</td>\n",
       "      <td>[interest, west-germany]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21557</th>\n",
       "      <td>Philippine President Corazon Aquino swore in f...</td>\n",
       "      <td>AQUINO SWEARS IN FOUR CABINET SECRETARIES</td>\n",
       "      <td>[philippines, aquino]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21558</th>\n",
       "      <td>Kenya and Shenzhen Electronics Group (SEG) of ...</td>\n",
       "      <td>CHINA, KENYA IN ELECTRONIC GOODS JOINT VENTURE</td>\n",
       "      <td>[china, kenya]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21559</th>\n",
       "      <td>The Soviet Union has agreed to cut its coking ...</td>\n",
       "      <td>USSR TO CUT COAL PRICE FOR JAPANESE STEELMILLS</td>\n",
       "      <td>[iron-steel, japan, ussr]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21560</th>\n",
       "      <td>&lt;Placer Pacific Ltd&gt; said it hopes the Papua N...</td>\n",
       "      <td>PLACER PACIFIC HOPES FOR MISIMA GOLD APPROVAL ...</td>\n",
       "      <td>[gold, australia, papua-new-guinea]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21561</th>\n",
       "      <td>The special prosecutor in the Iran arms scanda...</td>\n",
       "      <td>PAPER SAYS INDICTMENTS IN IRAN CASE EXPECTED</td>\n",
       "      <td>[usa, iran]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21562</th>\n",
       "      <td>The Bangladesh trade deficit narrowed to 1.91 ...</td>\n",
       "      <td>BANGLADESH TRADE DEFICIT NARROWS IN OCTOBER 1986</td>\n",
       "      <td>[trade, bangladesh]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21563</th>\n",
       "      <td>The outlook for Australian industrial investme...</td>\n",
       "      <td>AUSTRALIAN INVESTMENT OUTLOOK SEEN AS UNCERTAIN</td>\n",
       "      <td>[australia]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21564</th>\n",
       "      <td>The central bank has issued 12 billion Taiwan ...</td>\n",
       "      <td>TAIWAN ISSUES 12 BILLION DLRS OF BONDS</td>\n",
       "      <td>[taiwan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21565</th>\n",
       "      <td>Three Japanese credit rating agencies are ente...</td>\n",
       "      <td>JAPAN RATING AGENCIES IN BATTLE WITH U.S. GIANTS</td>\n",
       "      <td>[usa, japan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21566</th>\n",
       "      <td>Most commodity agreements are close to collaps...</td>\n",
       "      <td>GROUP-77 OFFICIALS SET AGENDA FOR DHAKA MEETING</td>\n",
       "      <td>[bangladesh]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21567</th>\n",
       "      <td>Swiss industrial output rose nine pct in the f...</td>\n",
       "      <td>SWISS INDUSTRIAL OUTPUT RISES IN FOURTH QQUARTER</td>\n",
       "      <td>[ipi, switzerland]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21568</th>\n",
       "      <td>Japanese securities houses are thinking of all...</td>\n",
       "      <td>FOREIGN BROKERS MAY GET MORE ACCESS TO JAPAN B...</td>\n",
       "      <td>[usa, japan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21569</th>\n",
       "      <td>The United States has openly attacked Japan's ...</td>\n",
       "      <td>U.S. SEEKS MAJOR JAPANESE ECONOMIC CHANGE IN A...</td>\n",
       "      <td>[japan, usa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21570</th>\n",
       "      <td>The Asian dollar market continued to expand in...</td>\n",
       "      <td>ASIAN DOLLAR ASSETS EXCEED 200 BILLION DLRS</td>\n",
       "      <td>[money-fx, singapore]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21571</th>\n",
       "      <td>Singapore's M-1 money supply rose 3.7 pct duri...</td>\n",
       "      <td>SINGAPORE M-1 MONEY SUPPLY UP 3.7 PCT IN DECEMBER</td>\n",
       "      <td>[money-supply, singapore]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21572</th>\n",
       "      <td>The value of China's industrial output in Janu...</td>\n",
       "      <td>CHINESE INDUSTRIAL GROWTH RATE UP AFTER WEAK 1986</td>\n",
       "      <td>[ipi, china]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21573</th>\n",
       "      <td>The Dutch central bank said it has accepted bi...</td>\n",
       "      <td>NEW DUTCH ADVANCES TOTAL 6.5 BILLION GUILDERS</td>\n",
       "      <td>[money-fx, interest, netherlands]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21574</th>\n",
       "      <td>Union Bank of Finland is issuing a 10 billion ...</td>\n",
       "      <td>UNION BANK OF FINLAND ISSUES EUROYEN BOND</td>\n",
       "      <td>[uk, finland]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21575</th>\n",
       "      <td>Japanese customers have bought nearly six mln ...</td>\n",
       "      <td>IRAN SELLING DISCOUNTED CRUDE, JAPAN TRADERS SAY</td>\n",
       "      <td>[crude, japan, iran]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21576</th>\n",
       "      <td>Imperial Chemical Industries Plc &lt;ICI.L&gt; said ...</td>\n",
       "      <td>ICI SELLS STAKE IN LISTER AND CO</td>\n",
       "      <td>[acq, uk]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21577</th>\n",
       "      <td>Unilever Australia Ltd is issuing 40 mln Austr...</td>\n",
       "      <td>UNILEVER UNIT ISSUES 40 MLN AUSTRALIAN DLR BOND</td>\n",
       "      <td>[uk]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21578 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    body  \\\n",
       "0      Chrysler Corp said car sales for the March 21-...   \n",
       "1      Compaq Computer Corp, IBM's chief rival in the...   \n",
       "2      <Noranda Inc> said production will remain shut...   \n",
       "3      The Canadian government's budget deficit rose ...   \n",
       "4      CIS Technologies Inc said it executed a formal...   \n",
       "5      Qtly div 42 cts vs 41.5 cts prior Payable APri...   \n",
       "6      Colombia's cost of living index rose 2.71 pct ...   \n",
       "7      The Federal Home Loan Bank Board said home mor...   \n",
       "8      The New York Stock Exchange said a seat on the...   \n",
       "9                                                          \n",
       "10     Beneficial Corp said the sale of its American ...   \n",
       "11     European options exchanges will see spectacula...   \n",
       "12     Tierco Group INc said it sold at par to the Ku...   \n",
       "13     Shr 5.56 dlrs vs 3.88 dlrs Net 47.5 mln vs 33....   \n",
       "14     California Micro Devices Corp said an addition...   \n",
       "15     Stewart INformation Services Corp said it resc...   \n",
       "16     FIserve Inc said 14 savings and loans with 1.5...   \n",
       "17     Mony Real Estate Investors Trust said its inve...   \n",
       "18     Canadian narrowly-defined money supply M-1 fel...   \n",
       "19     Copley Properties Inc said the company will in...   \n",
       "20     The United Food And Commercial Workers said th...   \n",
       "21     Pan Am Corp's Pan American World Airways said ...   \n",
       "22     FCS Laboratories Inc said merger discussions w...   \n",
       "23     The White House said the rise in interest rate...   \n",
       "24     Shr loss nine cts Net loss 1.4 mln Revs 630,11...   \n",
       "25     Shr 1.1 cts vs 1.7 cts Net 26,708 vs 35,084 Re...   \n",
       "26     Shr nine cts vs 19 cts Net 188,000 vs 362,000 ...   \n",
       "27                                                         \n",
       "28                                                         \n",
       "29     Burlington Northern Inc said its Burlington No...   \n",
       "...                                                  ...   \n",
       "21548  Hundreds of marines were on alert at 11 key Br...   \n",
       "21549  Indonesia's Armed Forces Commander General Ben...   \n",
       "21550  The suspension of Ecuador's crude oil shipment...   \n",
       "21551  Any European Community decision to liberalise ...   \n",
       "21552  China has raised the prices it pays farmers fo...   \n",
       "21553  Australia sold 180,000 tonnes of raw sugar to ...   \n",
       "21554  Thai natural rubber exports rose to 763,331 to...   \n",
       "21555  Australian beef output declined to 104,353 ton...   \n",
       "21556  Increased federal government borrowing needs a...   \n",
       "21557  Philippine President Corazon Aquino swore in f...   \n",
       "21558  Kenya and Shenzhen Electronics Group (SEG) of ...   \n",
       "21559  The Soviet Union has agreed to cut its coking ...   \n",
       "21560  <Placer Pacific Ltd> said it hopes the Papua N...   \n",
       "21561  The special prosecutor in the Iran arms scanda...   \n",
       "21562  The Bangladesh trade deficit narrowed to 1.91 ...   \n",
       "21563  The outlook for Australian industrial investme...   \n",
       "21564  The central bank has issued 12 billion Taiwan ...   \n",
       "21565  Three Japanese credit rating agencies are ente...   \n",
       "21566  Most commodity agreements are close to collaps...   \n",
       "21567  Swiss industrial output rose nine pct in the f...   \n",
       "21568  Japanese securities houses are thinking of all...   \n",
       "21569  The United States has openly attacked Japan's ...   \n",
       "21570  The Asian dollar market continued to expand in...   \n",
       "21571  Singapore's M-1 money supply rose 3.7 pct duri...   \n",
       "21572  The value of China's industrial output in Janu...   \n",
       "21573  The Dutch central bank said it has accepted bi...   \n",
       "21574  Union Bank of Finland is issuing a 10 billion ...   \n",
       "21575  Japanese customers have bought nearly six mln ...   \n",
       "21576  Imperial Chemical Industries Plc <ICI.L> said ...   \n",
       "21577  Unilever Australia Ltd is issuing 40 mln Austr...   \n",
       "\n",
       "                                                   title  \\\n",
       "0              CHRYSLER <C> LATE MARCH U.S. CAR SALES UP   \n",
       "1               WALL STREET STOCKS/COMPAQ COMPUTER <CPQ>   \n",
       "2                   NORANDA SETS TEMPORARY MINE SHUTDOWN   \n",
       "3                 CANADA BUDGET DEFICIT RISES IN JANUARY   \n",
       "4       CIS TECHNOLOGIES<CIH> TO SELL SHARES TO SWISS CO   \n",
       "5         COPLEY PROPERTIES INC <COP> INCREASES DIVIDEND   \n",
       "6            COLOMBIAN INFLATION STABLE AT AROUND 20 PCT   \n",
       "7             FHLBB SAYS MORTGAGE RATES CONTINUE DECLINE   \n",
       "8                         NYFE SEAT SELLS FOR 1,500 DLRS   \n",
       "9      CANADIAN MONEY SUPPLY M-1 FALLS 291 MLN DLRS I...   \n",
       "10                   BENEFICIAL <BNL> UNIT SALE APPROVED   \n",
       "11      LARGER VOLUME SEEN ON EUROPEAN OPTIONS EXCHANGES   \n",
       "12                              TIERCO <TIER> SELLS NOTE   \n",
       "13                             <ITT CANADA LTD> YEAR NET   \n",
       "14       CALIFORNIA MICRO DEVICES <CAMD> IN DEFENSE DEAL   \n",
       "15        STEWART INFORMATION RESCHEDULES ANNUAL MEETING   \n",
       "16       FISERVE <FISV> GETS BUSINESS WORTH ONE MLN DLRS   \n",
       "17                     MONY REAL <MYM> REPORTS PORTFOLIO   \n",
       "18                   CANADIAN MONEY SUPPLY FALLS IN WEEK   \n",
       "19       COPLEY PROPERTIES <COP> TO INVEST IN JOINT PACT   \n",
       "20        UNION TO PROTEST DART'S SUPERMARKETS <SGL> BID   \n",
       "21        PAN AM <PN> MARCH LOAD FACTOR ROSE TO 60.6 PCT   \n",
       "22         FCS LABORATORIES <FCSI> TERMINATES DEAL TALKS   \n",
       "23        WHITE HOUSE SAYS INTEREST RATES REFLECT MARKET   \n",
       "24                       REGAL PETROLEUM LTD <RPLO> YEAR   \n",
       "25                      CANTERBURY PRESS INC YEAR NOV 30   \n",
       "26                         GRAPHIC MEDIA INC <GMED> YEAR   \n",
       "27                                                         \n",
       "28                                                         \n",
       "29         BURLINGTON <BNI> UNIT SETTLES BONDHOLDER SUIT   \n",
       "...                                                  ...   \n",
       "21548  BRAZIL SEAMEN CONTINUE STRIKE AFTER COURT DECI...   \n",
       "21549     INDONESIA SAID TO BE STABLE AHEAD OF ELECTIONS   \n",
       "21550  ECUADOR TO EXPORT NO OIL FOR FOUR MONTHS, OFFI...   \n",
       "21551   EC FARM LIBERALISATION SEEN HURTING THAI TAPIOCA   \n",
       "21552        CHINA RAISES CROP PRICES TO INCREASE OUTPUT   \n",
       "21553    AUSTRALIA SELLS 180,000 TONNES OF SUGAR TO USSR   \n",
       "21554           THAI NATURAL RUBBER EXPORTS RISE IN 1986   \n",
       "21555         AUSTRALIAN BEEF OUTPUT DECLINES IN JANUARY   \n",
       "21556   GERMAN GOVERNMENT NEEDS SEEN RAISING BOND YIELDS   \n",
       "21557          AQUINO SWEARS IN FOUR CABINET SECRETARIES   \n",
       "21558     CHINA, KENYA IN ELECTRONIC GOODS JOINT VENTURE   \n",
       "21559     USSR TO CUT COAL PRICE FOR JAPANESE STEELMILLS   \n",
       "21560  PLACER PACIFIC HOPES FOR MISIMA GOLD APPROVAL ...   \n",
       "21561       PAPER SAYS INDICTMENTS IN IRAN CASE EXPECTED   \n",
       "21562   BANGLADESH TRADE DEFICIT NARROWS IN OCTOBER 1986   \n",
       "21563    AUSTRALIAN INVESTMENT OUTLOOK SEEN AS UNCERTAIN   \n",
       "21564             TAIWAN ISSUES 12 BILLION DLRS OF BONDS   \n",
       "21565   JAPAN RATING AGENCIES IN BATTLE WITH U.S. GIANTS   \n",
       "21566    GROUP-77 OFFICIALS SET AGENDA FOR DHAKA MEETING   \n",
       "21567   SWISS INDUSTRIAL OUTPUT RISES IN FOURTH QQUARTER   \n",
       "21568  FOREIGN BROKERS MAY GET MORE ACCESS TO JAPAN B...   \n",
       "21569  U.S. SEEKS MAJOR JAPANESE ECONOMIC CHANGE IN A...   \n",
       "21570        ASIAN DOLLAR ASSETS EXCEED 200 BILLION DLRS   \n",
       "21571  SINGAPORE M-1 MONEY SUPPLY UP 3.7 PCT IN DECEMBER   \n",
       "21572  CHINESE INDUSTRIAL GROWTH RATE UP AFTER WEAK 1986   \n",
       "21573      NEW DUTCH ADVANCES TOTAL 6.5 BILLION GUILDERS   \n",
       "21574          UNION BANK OF FINLAND ISSUES EUROYEN BOND   \n",
       "21575   IRAN SELLING DISCOUNTED CRUDE, JAPAN TRADERS SAY   \n",
       "21576                   ICI SELLS STAKE IN LISTER AND CO   \n",
       "21577    UNILEVER UNIT ISSUES 40 MLN AUSTRALIAN DLR BOND   \n",
       "\n",
       "                                       topics  \n",
       "0                                       [usa]  \n",
       "1                                       [usa]  \n",
       "2                            [copper, canada]  \n",
       "3                                    [canada]  \n",
       "4                     [acq, usa, switzerland]  \n",
       "5                                 [earn, usa]  \n",
       "6                             [cpi, colombia]  \n",
       "7                             [interest, usa]  \n",
       "8                                 [usa, nyse]  \n",
       "9                      [money-supply, canada]  \n",
       "10                                 [acq, usa]  \n",
       "11                   [netherlands, ase, cboe]  \n",
       "12                                      [usa]  \n",
       "13                             [earn, canada]  \n",
       "14                                      [usa]  \n",
       "15                                      [usa]  \n",
       "16                                      [usa]  \n",
       "17                                      [usa]  \n",
       "18                     [money-supply, canada]  \n",
       "19                                      [usa]  \n",
       "20                                 [acq, usa]  \n",
       "21                                      [usa]  \n",
       "22                                 [acq, usa]  \n",
       "23                            [interest, usa]  \n",
       "24                                [earn, usa]  \n",
       "25                                [earn, usa]  \n",
       "26                                [earn, usa]  \n",
       "27                                         []  \n",
       "28                                         []  \n",
       "29                                      [usa]  \n",
       "...                                       ...  \n",
       "21548                          [ship, brazil]  \n",
       "21549                             [indonesia]  \n",
       "21550                        [crude, ecuador]  \n",
       "21551      [tapioca, meal-feed, thailand, ec]  \n",
       "21552  [cotton, sugar, veg-oil, grain, china]  \n",
       "21553                [sugar, ussr, australia]  \n",
       "21554                      [rubber, thailand]  \n",
       "21555         [carcass, livestock, australia]  \n",
       "21556                [interest, west-germany]  \n",
       "21557                   [philippines, aquino]  \n",
       "21558                          [china, kenya]  \n",
       "21559               [iron-steel, japan, ussr]  \n",
       "21560     [gold, australia, papua-new-guinea]  \n",
       "21561                             [usa, iran]  \n",
       "21562                     [trade, bangladesh]  \n",
       "21563                             [australia]  \n",
       "21564                                [taiwan]  \n",
       "21565                            [usa, japan]  \n",
       "21566                            [bangladesh]  \n",
       "21567                      [ipi, switzerland]  \n",
       "21568                            [usa, japan]  \n",
       "21569                            [japan, usa]  \n",
       "21570                   [money-fx, singapore]  \n",
       "21571               [money-supply, singapore]  \n",
       "21572                            [ipi, china]  \n",
       "21573       [money-fx, interest, netherlands]  \n",
       "21574                           [uk, finland]  \n",
       "21575                    [crude, japan, iran]  \n",
       "21576                               [acq, uk]  \n",
       "21577                                    [uk]  \n",
       "\n",
       "[21578 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#From the code, accessing the reuters document data base. \n",
    "data_stream = stream_reuters_documents()\n",
    "#Experimenting with functions given in hint. \n",
    "df = pd.DataFrame(data_stream)\n",
    "print(\"The type of df is: \", type(df))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many documents in the dataset? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21578"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['title'].describe()['count']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Chrysler Corp said car sales for the March 21-...\n",
       "1        Compaq Computer Corp, IBM's chief rival in the...\n",
       "2        <Noranda Inc> said production will remain shut...\n",
       "3        The Canadian government's budget deficit rose ...\n",
       "4        CIS Technologies Inc said it executed a formal...\n",
       "5        Qtly div 42 cts vs 41.5 cts prior Payable APri...\n",
       "6        Colombia's cost of living index rose 2.71 pct ...\n",
       "7        The Federal Home Loan Bank Board said home mor...\n",
       "8        The New York Stock Exchange said a seat on the...\n",
       "9                                                         \n",
       "10       Beneficial Corp said the sale of its American ...\n",
       "11       European options exchanges will see spectacula...\n",
       "12       Tierco Group INc said it sold at par to the Ku...\n",
       "13       Shr 5.56 dlrs vs 3.88 dlrs Net 47.5 mln vs 33....\n",
       "14       California Micro Devices Corp said an addition...\n",
       "15       Stewart INformation Services Corp said it resc...\n",
       "16       FIserve Inc said 14 savings and loans with 1.5...\n",
       "17       Mony Real Estate Investors Trust said its inve...\n",
       "18       Canadian narrowly-defined money supply M-1 fel...\n",
       "19       Copley Properties Inc said the company will in...\n",
       "20       The United Food And Commercial Workers said th...\n",
       "21       Pan Am Corp's Pan American World Airways said ...\n",
       "22       FCS Laboratories Inc said merger discussions w...\n",
       "23       The White House said the rise in interest rate...\n",
       "24       Shr loss nine cts Net loss 1.4 mln Revs 630,11...\n",
       "25       Shr 1.1 cts vs 1.7 cts Net 26,708 vs 35,084 Re...\n",
       "26       Shr nine cts vs 19 cts Net 188,000 vs 362,000 ...\n",
       "27                                                        \n",
       "28                                                        \n",
       "29       Burlington Northern Inc said its Burlington No...\n",
       "                               ...                        \n",
       "21548    Hundreds of marines were on alert at 11 key Br...\n",
       "21549    Indonesia's Armed Forces Commander General Ben...\n",
       "21550    The suspension of Ecuador's crude oil shipment...\n",
       "21551    Any European Community decision to liberalise ...\n",
       "21552    China has raised the prices it pays farmers fo...\n",
       "21553    Australia sold 180,000 tonnes of raw sugar to ...\n",
       "21554    Thai natural rubber exports rose to 763,331 to...\n",
       "21555    Australian beef output declined to 104,353 ton...\n",
       "21556    Increased federal government borrowing needs a...\n",
       "21557    Philippine President Corazon Aquino swore in f...\n",
       "21558    Kenya and Shenzhen Electronics Group (SEG) of ...\n",
       "21559    The Soviet Union has agreed to cut its coking ...\n",
       "21560    <Placer Pacific Ltd> said it hopes the Papua N...\n",
       "21561    The special prosecutor in the Iran arms scanda...\n",
       "21562    The Bangladesh trade deficit narrowed to 1.91 ...\n",
       "21563    The outlook for Australian industrial investme...\n",
       "21564    The central bank has issued 12 billion Taiwan ...\n",
       "21565    Three Japanese credit rating agencies are ente...\n",
       "21566    Most commodity agreements are close to collaps...\n",
       "21567    Swiss industrial output rose nine pct in the f...\n",
       "21568    Japanese securities houses are thinking of all...\n",
       "21569    The United States has openly attacked Japan's ...\n",
       "21570    The Asian dollar market continued to expand in...\n",
       "21571    Singapore's M-1 money supply rose 3.7 pct duri...\n",
       "21572    The value of China's industrial output in Janu...\n",
       "21573    The Dutch central bank said it has accepted bi...\n",
       "21574    Union Bank of Finland is issuing a 10 billion ...\n",
       "21575    Japanese customers have bought nearly six mln ...\n",
       "21576    Imperial Chemical Industries Plc <ICI.L> said ...\n",
       "21577    Unilever Australia Ltd is issuing 40 mln Austr...\n",
       "Name: body, dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['body']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 21,578 as seen below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['body', 'title', 'topics']\n",
      "count     21578\n",
      "unique    18763\n",
      "top            \n",
      "freq       2535\n",
      "Name: body, dtype: object\n",
      "0                                         [usa]\n",
      "1                                         [usa]\n",
      "2                              [copper, canada]\n",
      "3                                      [canada]\n",
      "4                       [acq, usa, switzerland]\n",
      "5                                   [earn, usa]\n",
      "6                               [cpi, colombia]\n",
      "7                               [interest, usa]\n",
      "8                                   [usa, nyse]\n",
      "9                        [money-supply, canada]\n",
      "10                                   [acq, usa]\n",
      "11                     [netherlands, ase, cboe]\n",
      "12                                        [usa]\n",
      "13                               [earn, canada]\n",
      "14                                        [usa]\n",
      "15                                        [usa]\n",
      "16                                        [usa]\n",
      "17                                        [usa]\n",
      "18                       [money-supply, canada]\n",
      "19                                        [usa]\n",
      "20                                   [acq, usa]\n",
      "21                                        [usa]\n",
      "22                                   [acq, usa]\n",
      "23                              [interest, usa]\n",
      "24                                  [earn, usa]\n",
      "25                                  [earn, usa]\n",
      "26                                  [earn, usa]\n",
      "27                                           []\n",
      "28                                           []\n",
      "29                                        [usa]\n",
      "                          ...                  \n",
      "21548                            [ship, brazil]\n",
      "21549                               [indonesia]\n",
      "21550                          [crude, ecuador]\n",
      "21551        [tapioca, meal-feed, thailand, ec]\n",
      "21552    [cotton, sugar, veg-oil, grain, china]\n",
      "21553                  [sugar, ussr, australia]\n",
      "21554                        [rubber, thailand]\n",
      "21555           [carcass, livestock, australia]\n",
      "21556                  [interest, west-germany]\n",
      "21557                     [philippines, aquino]\n",
      "21558                            [china, kenya]\n",
      "21559                 [iron-steel, japan, ussr]\n",
      "21560       [gold, australia, papua-new-guinea]\n",
      "21561                               [usa, iran]\n",
      "21562                       [trade, bangladesh]\n",
      "21563                               [australia]\n",
      "21564                                  [taiwan]\n",
      "21565                              [usa, japan]\n",
      "21566                              [bangladesh]\n",
      "21567                        [ipi, switzerland]\n",
      "21568                              [usa, japan]\n",
      "21569                              [japan, usa]\n",
      "21570                     [money-fx, singapore]\n",
      "21571                 [money-supply, singapore]\n",
      "21572                              [ipi, china]\n",
      "21573         [money-fx, interest, netherlands]\n",
      "21574                             [uk, finland]\n",
      "21575                      [crude, japan, iran]\n",
      "21576                                 [acq, uk]\n",
      "21577                                      [uk]\n",
      "Name: topics, dtype: object\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "#Some exploring of dataset: \n",
    "if q1Verbose:\n",
    "    print(list(df))\n",
    "    print(df['body'].describe())\n",
    "    # Since df['topics'].descibe() does not work, let's just print. \n",
    "    print(df['topics'])\n",
    "    print(type(df['topics']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Each document can belong to a few topics (we interperted topics for categories). We will feed all information into a FreqDist and receive statistics.\n",
    "\n",
    "As implied, this means a document can appear in several documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a list of all occurences of all topics and feed to FreqDist. \n",
    "freq_dist = nltk.FreqDist(sum(list(df['topics']), []))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many categories: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of categories is:  445\n"
     ]
    }
   ],
   "source": [
    "category_set=set(sum(list(df['topics']), []))\n",
    "num_of_categories = len(category_set)\n",
    "print(\"The number of categories is: \",num_of_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are using a Frequency distribution, the number of categories can also be retrieved by: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "445"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(freq_dist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many documents per category: \n",
    "Since we are using a Frequency distribution, the number of documents per category \n",
    "is the value in freq_dist[category]. \n",
    "\n",
    "We can print a list of all categories and the number of documents in them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category:  sourrouille has  4 Docs\n",
      "Category:  lin-oil has  2 Docs\n",
      "Category:  takeshita has  4 Docs\n",
      "Category:  petricioli has  5 Docs\n",
      "Category:  haiti has  8 Docs\n",
      "Category:  ipe has  2 Docs\n",
      "Category:  housing has  21 Docs\n",
      "Category:  oilseed has  192 Docs\n",
      "Category:  money-supply has  190 Docs\n",
      "Category:  ongpin has  25 Docs\n"
     ]
    }
   ],
   "source": [
    "cat_numOfDocs = [(category, freq_dist[category]) for category in category_set]\n",
    "for pair in cat_numOfDocs[:10]: \n",
    "    print('Category: ', pair[0], 'has ', pair[1], 'Docs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Provide mean and standard deviation, min and max. \n",
    "Mean: Mean number of documents per categorie. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#[num_of_docs for _, num_of_docs in cat_numOfDocs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Mean (Average) number of documents per category is:  89.87191011235954\n",
      "The category with maximum documents is: \" usa \"which has  12542  documents.\n",
      "The category with minimum documents are: ['lin-meal', 'mitterrand', 'bfr'] who have 1 documents each. \n",
      "The standard deviation in number of documents per category is: 643.9321684195971\n"
     ]
    }
   ],
   "source": [
    "#Mean: \n",
    "#Sum of number of documents per each category. \n",
    "sum_docs_cat = sum(num_of_docs for (cat, num_of_docs) in cat_numOfDocs)\n",
    "#\n",
    "#Mean expected number of documents per categorie. \n",
    "#mean_exp=sum(freq_dist.freq(cat)*num_of_docs for (cat, num_of_docs) in cat_numOfDocs)\n",
    "#mean2 = np.mean([num_of_docs for _, num_of_docs in cat_numOfDocs])\n",
    "#print('The Mean number of documents per categorie is: ', mean_exp)\n",
    "#print('The Mean2 number of documents per categorie is: ', mean2)\n",
    "mean = sum_docs_cat/len(cat_numOfDocs)\n",
    "print('The Mean (Average) number of documents per category is: ', mean)\n",
    "\n",
    "#Max:\n",
    "print('The category with maximum documents is: \"',freq_dist.max(), '\"which has ', freq_dist[freq_dist.max()], ' documents.')\n",
    "\n",
    "#Min:\n",
    "min_num_of_docs = sorted(cat_numOfDocs ,key=lambda x: x[1])[0][1]\n",
    "cats_w_min_num_of_docs = [cat for (cat, num_of_docs) in cat_numOfDocs if num_of_docs==min_num_of_docs]\n",
    "display = 3 #Display only part of categories, not all. \n",
    "print('The category with minimum documents are:',cats_w_min_num_of_docs[:display], 'who have', min_num_of_docs, 'documents each. ' )\n",
    "\n",
    "#Standard deviation: \n",
    "std_dev = math.sqrt(sum( (math.pow(num_of_docs-mean, 2) for (_, num_of_docs) in cat_numOfDocs))/len(cat_numOfDocs))\n",
    "print('The standard deviation in number of documents per category is:', std_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1.1.3 Explore how many characters and words are present in the documents of the dataset.\n",
    "\n",
    "first we consider all diffferent word tokens and characters in code, as in a set of elements. We then calculate \n",
    "the number of all tokens and characters all together, which is more relevent to our issues. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create sets of words and characters. \n",
    "#Takes a while to run, use with care :)\n",
    "if q113_verbose: \n",
    "    word_set=set()\n",
    "    word_list=[]\n",
    "    for i in range(len(df['body'])): \n",
    "        word_set.update(word_tokenize(df['body'][i]))\n",
    "        word_list += word_tokenize(df['body'][i])\n",
    "\n",
    "    char_set=set()\n",
    "    char_list=[]\n",
    "    for word in word_set: \n",
    "        for letter in word: \n",
    "            char_set.update(letter)\n",
    "            char_list += letter\n",
    "    print('There are %d different words in all documents. ' %len(word_set))\n",
    "    print('There are %d word tokens in all documents. ' %len(word_list))\n",
    "    print('There are %d different characters in all documents. ' %len(char_set))\n",
    "    print('There are %d characters in all documents. ' %len(char_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "Since runtime is long for the two above boxes, Output given here, no need to run. \n",
    "\n",
    "Output: \n",
    "There are 76886 different words in all documents. \n",
    "There are 2854622 word tokens in all documents. \n",
    "There are 89 different characters in all documents. \n",
    "There are 568599 characters in all documents. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now construct a dictionary, That maps from article index to {num_of_words: , num_of_chars: }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "article_2words_chars = {}\n",
    "for i in range(len(df['body'])): \n",
    "    article_2words_chars[i] = (len(word_tokenize(df['body'][i])), len(df['body'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def explore_doc(i):\n",
    "    print('Document with index %d has %d words and %d letters' % (2, article_2words_chars[x][0], article_2words_chars[x][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1.1.4 Explain informally what are the classifiers that support the \"partial-fit\" method discussed in the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Informally, the classifiers that support \"partial-fit\", are classifiers who do not need to \"hold\" all the\n",
    "information they are given, at every given moment. If we attempt a slightly more formal explanation, We \n",
    "can say that the state of the classifier is changed as it learns from more inputs, yet this input is not\n",
    "a state variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1.1.5 Explain what is the hashing vectorizer used in this tutorial.\n",
    "####            Why is it important to use this vectorizer to achieve \"streaming classification\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As We have seen, We are dealing with a large amount of data. In order to make our data easier to process, \n",
    "We turn it into a sparse matrix that improves our memory usage by changing words into corresponding integers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Q1.2 Spam Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy\n",
    "from pandas import DataFrame\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "NEWLINE = '\\n'\n",
    "\n",
    "HAM = 'ham'\n",
    "SPAM = 'spam'\n",
    "\n",
    "SOURCES = [\n",
    "    ('data/spam',        SPAM),\n",
    "    ('data/easy_ham',    HAM),\n",
    "    ('data/hard_ham',    HAM),\n",
    "    ('data/beck-s',      HAM),\n",
    "    ('data/farmer-d',    HAM),\n",
    "    ('data/kaminski-v',  HAM),\n",
    "    ('data/kitchen-l',   HAM),\n",
    "    ('data/lokay-m',     HAM),\n",
    "    ('data/williams-w3', HAM),\n",
    "    ('data/BG',          SPAM),\n",
    "    ('data/GP',          SPAM),\n",
    "    ('data/SH',          SPAM)\n",
    "]\n",
    "\n",
    "SKIP_FILES = {'cmds'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def read_files(path):\n",
    "    for root, dir_names, file_names in os.walk(path):\n",
    "        for path in dir_names:\n",
    "            read_files(os.path.join(root, path))\n",
    "        for file_name in file_names:\n",
    "            if file_name not in SKIP_FILES:\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                if os.path.isfile(file_path):\n",
    "                    past_header, lines = False, []\n",
    "                    f = open(file_path, encoding=\"latin-1\")\n",
    "                    for line in f:\n",
    "                        if past_header:\n",
    "                            lines.append(line)\n",
    "                        elif line == NEWLINE:\n",
    "                            past_header = True\n",
    "                    f.close()\n",
    "                    content = NEWLINE.join(lines)\n",
    "                    yield file_path, content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_data_frame(path, classification):\n",
    "    rows = []\n",
    "    index = []\n",
    "    for file_name, text in read_files(path):\n",
    "        rows.append({'text': text, 'class': classification})\n",
    "        index.append(file_name)\n",
    "\n",
    "    data_frame = DataFrame(rows, index=index)\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = DataFrame({'text': [], 'class': []})\n",
    "for path, classification in SOURCES:\n",
    "    data = data.append(build_data_frame(path, classification))\n",
    "\n",
    "data = data.reindex(numpy.random.permutation(data.index))\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('count_vectorizer',   CountVectorizer(ngram_range=(1, 2))),\n",
    "    ('classifier',         MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k_fold = KFold(n=len(data), n_folds=6)\n",
    "scores = []\n",
    "confusion = numpy.array([[0, 0], [0, 0]])\n",
    "for train_indices, test_indices in k_fold:\n",
    "    train_text = data.iloc[train_indices]['text'].values\n",
    "    train_y = data.iloc[train_indices]['class'].values.astype(str)\n",
    "\n",
    "    test_text = data.iloc[test_indices]['text'].values\n",
    "    test_y = data.iloc[test_indices]['class'].values.astype(str)\n",
    "\n",
    "    pipeline.fit(train_text, train_y)\n",
    "    predictions = pipeline.predict(test_text)\n",
    "\n",
    "    confusion += confusion_matrix(test_y, predictions)\n",
    "    score = f1_score(test_y, predictions, pos_label=SPAM)\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total emails classified: 32389\n",
      "Score: 0.970473398143\n",
      "Confusion matrix:\n",
      "[[21829    60]\n",
      " [  546  9954]]\n"
     ]
    }
   ],
   "source": [
    "print('Total emails classified:', len(data))\n",
    "print('Score:', sum(scores)/len(scores))\n",
    "print('Confusion matrix:')\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1.2.1 The vectorizer used in Zac Stewart's code is a CountVectorizer with unigrams and bigrams. Report the number of unigrams and bigrams used in this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1984848 unigrams and bigrams, used in this model. \n"
     ]
    }
   ],
   "source": [
    "# Retreive the count vectorizer used in the model. \n",
    "p=pipeline.get_params()\n",
    "CountV=p['count_vectorizer']\n",
    "#Access features: \n",
    "uni_bi_grams = CountV.get_feature_names()\n",
    "print(\"There are %d unigrams and bigrams, used in this model. \" %len(uni_bi_grams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Q1.2.2 What are the 50 most frequent unigrams and bigrams in the dataset?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the', 274312.0)\n",
      "('to', 190011.0)\n",
      "('and', 140757.0)\n"
     ]
    }
   ],
   "source": [
    "def most_freq_feat(classifier, count_vector, n=50):\n",
    "    index = 0\n",
    "    features_c1_c2_count = []\n",
    "\n",
    "    for feat, c1, c2 in zip(count_vector.get_feature_names(), classifier.feature_count_[0], classifier.feature_count_[1]):\n",
    "        features_c1_c2_count.append((feat, c1 + c2))\n",
    "        index+=1\n",
    "\n",
    "    for i in sorted(features_c1_c2_count, key = lambda x: x[1], reverse=True)[:n]:     \n",
    "        print(i)\n",
    "    \n",
    "\n",
    "    \n",
    "most_freq_feat(p['classifier'], p['count_vectorizer'], n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are the 50 most frequent unigrams and bigrams per class (ham and spam)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 most occurring features in class spam: \n",
      "('the', 205629.0, 68683.0)\n",
      "1 most occurring features in class ham: \n",
      "('font', 11189.0, 89667.0)\n"
     ]
    }
   ],
   "source": [
    "#Create a list of feature name and amount of occurrences in each class. \n",
    "#Sort according to different class counter to get occurrences per class. \n",
    "def most_occurring_feat_per_class(classifier, count_vector, n=50):\n",
    "    index = 0\n",
    "    features_c1_c2_count = []\n",
    "\n",
    "    for feat, c1, c2 in zip(count_vector.get_feature_names(), classifier.feature_count_[0], classifier.feature_count_[1]):\n",
    "        features_c1_c2_count.append((feat, c1, c2))\n",
    "        index+=1\n",
    "\n",
    "    print(\"%d most occurring features in class spam: \" %n )    \n",
    "    for i in sorted(features_c1_c2_count, key = lambda x: x[1], reverse=True)[:n]:     \n",
    "        print(i)\n",
    "\n",
    "    print(\"%d most occurring features in class ham: \" %n )    \n",
    "    for i in sorted(features_c1_c2_count, key = lambda x: x[2], reverse=True)[:n]:     \n",
    "        print(i)\n",
    "\n",
    "\n",
    "    \n",
    "most_occurring_feat_per_class(p['classifier'], p['count_vectorizer'], n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Q1.2.4 List the 20 most useful features in the Naive Bayes classifier to distinguish between spam and ham (20 features for each class). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham -16.0769551682 00 005\n",
      "ham -16.0769551682 00 00am\n",
      "\n",
      "spam -4.67308592853 font\n",
      "spam -4.80892052145 br\n"
     ]
    }
   ],
   "source": [
    "#Since each features coefficient links it to it's class, and smaller coefficients classify spam and larger ham, \n",
    "#we sort according to coefficient, once normaly and once reversed, to get most informative features. \n",
    "def most_informative_feature_for_binary_classification(vectorizer, classifier, n=20):\n",
    "    class_labels = classifier.classes_\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    topn_class1 = sorted(zip(classifier.coef_[0], feature_names))[:n]\n",
    "    topn_class2 = sorted(zip(classifier.coef_[0], feature_names))[-n:]\n",
    "    \n",
    "    counter=0\n",
    "    for coef, feat in topn_class1:\n",
    "        print(class_labels[0], coef, feat)\n",
    "        if counter==20: \n",
    "            break\n",
    "    print()\n",
    "    \n",
    "    counter=0\n",
    "    for coef, feat in reversed(topn_class2):\n",
    "        print(class_labels[1], coef, feat)\n",
    "        counter+=1\n",
    "        if counter==20: \n",
    "            break\n",
    "\n",
    "most_informative_feature_for_binary_classification(p['count_vectorizer'], p['classifier'], n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1.2.5 There seems to be an imbalance in the length of spam and ham messages (see the plot in the attached notebook). We want to add a feature based on the number of words in the message in the text representation. Should the length attribute be normalized before fitting the Naive Bayes classifier? (See Sklearn pre-processing for examples.) Do you expect Logistic Regression to perform better with the new feature? Explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_data_frame(path, classification):\n",
    "    rows = []\n",
    "    index = []\n",
    "    for file_name, text in read_files(path):\n",
    "        rows.append({'text': text,'len': len(nltk.tokenize.word_tokenize(text)), 'class': classification})\n",
    "        index.append(file_name)\n",
    "\n",
    "    data_frame = DataFrame(rows, index=index)\n",
    "    return data_frame\n",
    "\n",
    "data1 = DataFrame({'text': [], 'len': [], 'class': []})\n",
    "for path, classification in SOURCES:\n",
    "    data1 = data1.append(build_data_frame(path, classification))\n",
    "\n",
    "data1 = data1.reindex(numpy.random.permutation(data.index))\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('count_vectorizer',   CountVectorizer(ngram_range=(1, 2))),\n",
    "    ('classifier',         MultinomialNB())\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data1 = DataFrame({'text': [], 'len': [], 'class': []})\n",
    "for path, classification in SOURCES:\n",
    "    data1 = data1.append(build_data_frame(path, classification))\n",
    "\n",
    "data1 = data1.reindex(numpy.random.permutation(data.index))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "pipeline = Pipeline([\n",
    "  ('features', FeatureUnion([\n",
    "        ('body_stats', Pipeline([\n",
    "                ('stats', TextStats()),  # returns a list of dicts\n",
    "                ('vect', DictVectorizer()),  # list of dicts -> feature matrix\n",
    "                                ])),\n",
    "        ('count_vectorizer',   CountVectorizer(ngram_range=(1, 2))),   \n",
    "                            ])),\n",
    "  ('classifier', MultinomialNB())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total emails classified: 32389\n",
      "Score: 0.971156501249\n",
      "Confusion matrix:\n",
      "[[21818    71]\n",
      " [  522  9978]]\n"
     ]
    }
   ],
   "source": [
    "k_fold = KFold(n=len(data), n_folds=6)\n",
    "scores = []\n",
    "confusion = numpy.array([[0, 0], [0, 0]])\n",
    "for train_indices, test_indices in k_fold:\n",
    "    train_text = data.iloc[train_indices]['text'].values\n",
    "    train_y = data.iloc[train_indices]['class'].values.astype(str)\n",
    "\n",
    "    test_text = data.iloc[test_indices]['text'].values\n",
    "    test_y = data.iloc[test_indices]['class'].values.astype(str)\n",
    "\n",
    "    pipeline.fit(train_text, train_y)\n",
    "    predictions = pipeline.predict(test_text)\n",
    "\n",
    "    confusion += confusion_matrix(test_y, predictions)\n",
    "    score = f1_score(test_y, predictions, pos_label=SPAM)\n",
    "    scores.append(score)\n",
    "    \n",
    "\n",
    "print('Total emails classified:', len(data))\n",
    "print('Score:', sum(scores)/len(scores))\n",
    "print('Confusion matrix:')\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def build_pipeline2():\n",
    "    pipeline = Pipeline([\n",
    "        ('features', FeatureUnion([\n",
    "            ('body_stats', Pipeline([\n",
    "                ('stats', TextStats()),  # returns a list of dicts\n",
    "                ('vect', DictVectorizer()),  # list of dicts -> feature matrix\n",
    "                                ])),\n",
    "        ('count_vectorizer',   CountVectorizer(ngram_range=(1, 2))),   \n",
    "                            ])),\n",
    "        ('classifier',         LogisticRegression())\n",
    "    ])\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data1.groupby('class').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CountVectorizer(ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin    \n",
    "class TextStats(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'length': len(text)} for text in posts]\n",
    "\n",
    "#        len(nltk.tokenize.word_tokenize(text))\n",
    "#        return [{'length': len(text)} for text in posts]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Q1.3 SMS Spam Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CountV.get_stop_words()\n",
    "CountV.get_params()['analyzer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1.3.1 Test the classifier trained on email data in 1.2 on the SMS data.\n",
    "    We will first train the classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. Named Entity Recognition\n",
    "    Features:\n",
    "            we are intrested in creating a vectorized obejct from our data set, which will take into consideration the\n",
    "            (1) word-form (2)the POS of the word (3) ORT, (4) perfix1 ,(5) perfix2, (6) perfix3, (7) suffix1 \n",
    "            (8) suffix2, (9) suffix3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "first let's load and split our data set to test and train sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split data set to train and test data sets\n",
    "train_sents = list(nltk.corpus.conll2002.iob_sents('esp.train'))\n",
    "test_sents = list(nltk.corpus.conll2002.iob_sents('esp.testb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " we've noticed that our data is build as a list of sentences, all of which are constructed from a list of tripules in the following foramt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Melbourne', 'NP', 'B-LOC'),\n",
       " ('(', 'Fpa', 'O'),\n",
       " ('Australia', 'NP', 'B-LOC'),\n",
       " (')', 'Fpt', 'O'),\n",
       " (',', 'Fc', 'O'),\n",
       " ('25', 'Z', 'O'),\n",
       " ('may', 'NC', 'O'),\n",
       " ('(', 'Fpa', 'O'),\n",
       " ('EFE', 'NC', 'B-ORG'),\n",
       " (')', 'Fpt', 'O'),\n",
       " ('.', 'Fp', 'O')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='padding: 10px;'><code>[  [(&lt;WORD&gt;, &lt;POS&gt;, &lt;CLASS&gt;),....],<br />&nbsp;....[]&nbsp;...<br/>]</code>\n",
    "</div><br/>\n",
    "We would like to add another features, and will do that in a manner simmilar to the one being done\n",
    "in the <a href=\"http://nbviewer.ipython.org/github/tpeng/python-crfsuite/blob/master/examples/CoNLL%202002.ipynb\" target=\"_blank\">CoNLL Classification</a> we'll build out a dictionary with all the\n",
    "wanted features and use <i><u>DictVectorizer</u></i> to get a vectorized representation of the word\n",
    "according to it's features. \n",
    "In order to do that let's define the following function to extract the features from our data-set, first, let's deine a function to extract ortographic data based on the word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ort(word):\n",
    "    if (re.match(\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\", word) != None):\n",
    "        return \"URL\"\n",
    "    if (re.match(\"^-?\\+?[0-9]+(.[0-9]+)?$\", word) != None):\n",
    "        return \"number\"\n",
    "    if (re.search(\".*[0-9]+.*\", word) != None):\n",
    "        return \"contains-digit\"\n",
    "    if (word.find(\"-\") != -1):\n",
    "        return \"contains-hyphen\"\n",
    "    if word.isupper():\n",
    "        return \"all-capitals\"\n",
    "    if (re.match(\"^[A-Z].*\", word) != None):\n",
    "        return \"capitalized\"\n",
    "    if (re.match(\"^[,;.-/!/?/*/+]+$\", word) != None):\n",
    "        return \"punctuation\"\n",
    "    return \"regular\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will returne a string value represening one ortographic feature for a word. for semplicity any word has a single orthograpic feature, and if it doesn't contain any we will say the word has a regular stracture. Now based on this function let's define a function to extract the features from a given word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "    features = {\n",
    "        'WORD-FROM': word.lower(),\n",
    "        'POS=': postag,\n",
    "        'ORT=': get_ort(word),\n",
    "        'PREFIX1': word.lower()[:1],\n",
    "        'PREFIX2': word.lower()[:2],\n",
    "        'PREFIX3': word.lower()[:3],\n",
    "        'SUFFIX1': word.lower()[-1:],\n",
    "        'SUFFIX2': word.lower()[-2:],\n",
    "        'SUFFIX3': word.lower()[-3:]}\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This function will extract the wanted features for each index in te sentence array provided to it. The features will be sent as a dictionary, which is the prefered format of the <b><u>DictVectorizer</u></b> which will later convert is to a one-hot vector.\n",
    "<br />Let's also define a function to easly extract the NER tag from a sentence:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word2label(sent, i):\n",
    "    return sent[i][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now construct our model's pipeline based on the <b>DictVectorizer</b> and the <b>Logistic Regression Classifier</b>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_pipeline():\n",
    "    pipeline = Pipeline([\n",
    "        ('vectorize', DictVectorizer(sparse=False)),\n",
    "        ('classify', LogisticRegression())\n",
    "    ])\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first use the Spanish data-set for training and testring, let's define our sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_sents = list(nltk.corpus.conll2002.iob_sents('esp.train'))\n",
    "test_sents = list(nltk.corpus.conll2002.iob_sents('esp.testb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this data set is pretty big, (we acturally have tried training our pipline with simple array - which abviusly fialed badly due to lack of memory) in order to overcome this problem, we'll load the data-set to a numpy arrays, and along the way will already bulild it a format which will be easer for us to extract our features from. <br />\n",
    "First let's re-use the progress function from the SPAM note-book, so we won't be beard will the data is processed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def progress(i, end_val, bar_length=50):\n",
    "    '''\n",
    "    Print a progress bar of the form: Percent: [#####      ]\n",
    "    i is the current progress value expected in a range [0..end_val]\n",
    "    bar_length is the width of the progress bar on the screen.\n",
    "    '''\n",
    "    percent = float(i) / end_val\n",
    "    hashes = '#' * int(round(percent * bar_length))\n",
    "    spaces = ' ' * (bar_length - len(hashes))\n",
    "    sys.stdout.write(\"\\rPercent: [{0}] {1}%\".format(hashes + spaces, int(round(percent * 100))))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will write a function to build our data in a numpy DataFrame while extracting out our features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_data(data, feature_extractor=(lambda sent, i: word2features(sent, i))):\n",
    "    df = DataFrame({'features': [], 'class': []})\n",
    "    print(\"Starting To Build Data.\")\n",
    "    for i, sent in enumerate(data):\n",
    "        data_frame, nrows = build_data_frame(i, len(data), sent, feature_extractor)\n",
    "        df = df.append(data_frame)\n",
    "    print(\"\\nDone!\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will return a data frame with 'features' holding our dictionary of features (based on our feature extractor - the default one will be the one defined earlier) and 'class' will hold the values to be learned by the classifier - in our case the NER tag.\n",
    "and Here is the <i>build_data_frame</i> function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_data_frame(l, len_data, sent, feature_extractor):\n",
    "    rows = []\n",
    "    index = []\n",
    "    for i in range(len(sent)):\n",
    "        rows.append({'features': feature_extractor(sent, i), 'class': word2label(sent, i)})\n",
    "        index.append(sent)\n",
    "    progress(l, len_data)\n",
    "    data_frame = DataFrame(rows, index=index)\n",
    "    return data_frame, len(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are redy for our training function, again we should keep in mind that our data-set it pretty big, so in order to easely work with our data we will split the data set using <b><u>KFold</u></b> simmilar to the way it's being done with the SPAM detector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(data_sents=None, data_frame=None, n_folds=6):\n",
    "    if data_frame is None and data_sents is None:\n",
    "        raise Exception('No data was provided to train!')\n",
    "    elif data_frame is None:\n",
    "        data_frame = build_data(data_sents)\n",
    "\n",
    "    k_fold = KFold(n=len(data_frame), n_folds=n_folds)\n",
    "    pipeline = build_pipeline()\n",
    "\n",
    "    print(\"Training with %d folds\" % n_folds)\n",
    "    for i, (train_indices, test_indices) in enumerate(k_fold):\n",
    "        x_train = data_frame.iloc[train_indices]['features'].values\n",
    "        y_train = data_frame.iloc[train_indices]['class'].values.astype(str)\n",
    "        if (q2Verbose):\n",
    "            print(\"Training for fold %d\" % i)\n",
    "        pipeline.fit(x_train, y_train)\n",
    "    print('Total classified:', len(data_frame))\n",
    "    return pipeline, data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we're already at-it let's also write another feature extractor, one which will also take into consideration the previouse and next words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we can play a bit with the features\n",
    "def word2features2(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "    fetures = {\n",
    "        'WORD-FROM': word.lower(),\n",
    "        'POS=': postag,\n",
    "        'ORT=': get_ort(word),\n",
    "        'PREFIX1': word.lower()[:1],\n",
    "        'PREFIX2': word.lower()[:2],\n",
    "        'PREFIX3': word.lower()[:3],\n",
    "        'SUFFIX1': word.lower()[-1:],\n",
    "        'SUFFIX2': word.lower()[-2:],\n",
    "        'SUFFIX3': word.lower()[-3:]}\n",
    "\n",
    "    if i < (len(sent) - 1):\n",
    "        fetures['NEXT_WORD_FORM'] = sent[i + 1][0]\n",
    "        fetures['NEXT_POS'] = sent[i + 1][1]\n",
    "    else:\n",
    "        fetures['NEXT_WORD_FORM'] = '*'\n",
    "        fetures['NEXT_POS'] = '*'\n",
    "\n",
    "    if i > 0:\n",
    "        fetures['PREV_WORD_FORM'] = sent[i - 1][0]\n",
    "        fetures['PREV_POS'] = sent[i - 1][1]\n",
    "    else:\n",
    "        fetures['PREV_WORD_FORM'] = '*'\n",
    "        fetures['PREV_POS'] = '*'\n",
    "\n",
    "    return fetures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have also added an ending, and beginning padding with '*' to mark words at a beginning of a sentence or words without followers. Now we are ready to load our data, and extract it's features. this might take a while (up to 10 minutes) <br />\n",
    "<b>The first one is the default extractor (without next and prev word features)</b> we'll call it data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting To Build Data.\n",
      "Percent: [##########################################        ] 83%"
     ]
    }
   ],
   "source": [
    "data1 = build_data(train_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The second is the more advanced extractor (with next and prev word features)</b> we'll call it data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data2 = build_data(train_sents, word2features2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train both of our models (based on diffrent features) and see the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model1, data1 = train(None, data1)\n",
    "model2, data2 = train(None, data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the outputs of both of the models, first we'll define those two helper function to extract data from the data-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [word2label(sent, i) for i in range(len(sent))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test = sum([sent2features(s) for s in test_sents], [])\n",
    "y_test = sum([sent2labels(s) for s in test_sents], [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will build us two arrays of features and NER tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"testing model1..\")\n",
    "predictions1 = model1.predict(x_test)\n",
    "score1 = f1_score(y_test, predictions1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's extract featues according to the second feature extractor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test2 = sum([sent2features2(s) for s in test_sents], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"testing model2..\")\n",
    "predictions2 = model2.predict(x_test)\n",
    "score2 = f1_score(y_test, predictions2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have used the f1 (AKA F_Masure) score to check our models, let's compare the outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"First model (without looking on previous and next word tags) scored %f \" %score1 )\n",
    "print(\"After adding to the feature extraction better features we were managed to score  %f\" %score2 )\n",
    "\n",
    "print (\"Here is both confusions matrix of the first one:\\n\", confusion_matrix(y_test, predictions1))\n",
    "print (\"And Here is the second:\\n\", confusion_matrix(y_test, predictions2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now clearly see that adding the context to the feature extraction have added a 7% better f_masure score! now we can see if this is also the case with a diffrent language, let's load the dutch dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dutch_train_sents = list(nltk.corpus.conll2002.iob_sents('ned.train'))\n",
    "dutch_test_sents_a = list(nltk.corpus.conll2002.iob_sents('ned.testa'))\n",
    "dutch_test_sents_a = list(nltk.corpus.conll2002.iob_sents('ned.testb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract our features to numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dutch = build_data(dutch_train_sents, word2features2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_dutch, data_dutch = train(None, data_dutch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test it wit testa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test_dutch = sum([sent2features2(s) for s in dutch_test_sents_a], [])\n",
    "y_test = sum([sent2labels(s) for s in dutch_test_sents_a], [])\n",
    "predictions_dutcha = model_dutch.predict(x_test_dutch)\n",
    "score_dutch1 = f1_score(y_test, predictions_dutcha)\n",
    "print(score_dutch1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add testb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test_dutchb = sum([sent2features2(s) for s in dutch_test_sents_b], [])\n",
    "y_testb = sum([sent2labels(s) for s in dutch_test_sents_b], [])\n",
    "predictions_dutchb = model_dutch.predict(x_test_dutchb)\n",
    "score_dutch2 = f1_score(y_test, predictions_dutchb)\n",
    "print(score_dutch2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the average score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg_dutch_score = (score_dutch2+score_dutch1)/2\n",
    "print(\"Our NER tagger scored %f in dutch\", % avg_dutch_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see that our NER tagger had scored X in dutch, while scoring Y in Spanish. We guess this as a lot to do with spanish articles since they have unique atricles for locations and placed.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
