{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Our import: \n",
    "import nltk\n",
    "from nltk.probability import ConditionalFreqDist\n",
    "import pandas as pd\n",
    "import math\n",
    "from nltk import word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "##Our defines: \n",
    "q1Verbose=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. Document Classification\n",
    "#### Q1.1. Reuters Dataset\n",
    "\n",
    "##### Q1.1.1 Turn the code of the Sklearn tutorial above into a notebook.\n",
    "\n",
    "This code is taken from the out of core classification guide given in the assigmnent. \n",
    "http://scikit-learn.org/dev/auto_examples/applications/plot_out_of_core_classification.html#example-applications-plot-out-of-core-classification-py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Authors: Eustache Diemert <eustache@diemert.fr>\n",
    "#          @FedericoV <https://github.com/FedericoV/>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from glob import glob\n",
    "import itertools\n",
    "import os.path\n",
    "import re\n",
    "import tarfile\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "\n",
    "from sklearn.externals.six.moves import html_parser\n",
    "from sklearn.externals.six.moves import urllib\n",
    "from sklearn.datasets import get_data_home\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _not_in_sphinx():\n",
    "    # Hack to detect whether we are running by the sphinx builder\n",
    "    return '__file__' in globals()\n",
    "\n",
    "%matplotlib inline\n",
    "###############################################################################\n",
    "# Reuters Dataset related routines\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "class ReutersParser(html_parser.HTMLParser):\n",
    "    \"\"\"Utility class to parse a SGML file and yield documents one at a time.\"\"\"\n",
    "\n",
    "    def __init__(self, encoding='latin-1'):\n",
    "        html_parser.HTMLParser.__init__(self)\n",
    "        self._reset()\n",
    "        self.encoding = encoding\n",
    "\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        method = 'start_' + tag\n",
    "        getattr(self, method, lambda x: None)(attrs)\n",
    "\n",
    "    def handle_endtag(self, tag):\n",
    "        method = 'end_' + tag\n",
    "        getattr(self, method, lambda: None)()\n",
    "\n",
    "    def _reset(self):\n",
    "        self.in_title = 0\n",
    "        self.in_body = 0\n",
    "        self.in_topics = 0\n",
    "        self.in_topic_d = 0\n",
    "        self.title = \"\"\n",
    "        self.body = \"\"\n",
    "        self.topics = []\n",
    "        self.topic_d = \"\"\n",
    "\n",
    "    def parse(self, fd):\n",
    "        self.docs = []\n",
    "        for chunk in fd:\n",
    "            self.feed(chunk.decode(self.encoding))\n",
    "            for doc in self.docs:\n",
    "                yield doc\n",
    "            self.docs = []\n",
    "        self.close()\n",
    "\n",
    "    def handle_data(self, data):\n",
    "        if self.in_body:\n",
    "            self.body += data\n",
    "        elif self.in_title:\n",
    "            self.title += data\n",
    "        elif self.in_topic_d:\n",
    "            self.topic_d += data\n",
    "\n",
    "    def start_reuters(self, attributes):\n",
    "        pass\n",
    "\n",
    "    def end_reuters(self):\n",
    "        self.body = re.sub(r'\\s+', r' ', self.body)\n",
    "        self.docs.append({'title': self.title,\n",
    "                          'body': self.body,\n",
    "                          'topics': self.topics})\n",
    "        self._reset()\n",
    "\n",
    "    def start_title(self, attributes):\n",
    "        self.in_title = 1\n",
    "\n",
    "    def end_title(self):\n",
    "        self.in_title = 0\n",
    "\n",
    "    def start_body(self, attributes):\n",
    "        self.in_body = 1\n",
    "\n",
    "    def end_body(self):\n",
    "        self.in_body = 0\n",
    "\n",
    "    def start_topics(self, attributes):\n",
    "        self.in_topics = 1\n",
    "\n",
    "    def end_topics(self):\n",
    "        self.in_topics = 0\n",
    "\n",
    "    def start_d(self, attributes):\n",
    "        self.in_topic_d = 1\n",
    "\n",
    "    def end_d(self):\n",
    "        self.in_topic_d = 0\n",
    "        self.topics.append(self.topic_d)\n",
    "        self.topic_d = \"\"\n",
    "\n",
    "\n",
    "def stream_reuters_documents(data_path=None):\n",
    "    \"\"\"Iterate over documents of the Reuters dataset.\n",
    "\n",
    "    The Reuters archive will automatically be downloaded and uncompressed if\n",
    "    the `data_path` directory does not exist.\n",
    "\n",
    "    Documents are represented as dictionaries with 'body' (str),\n",
    "    'title' (str), 'topics' (list(str)) keys.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    DOWNLOAD_URL = ('http://archive.ics.uci.edu/ml/machine-learning-databases/'\n",
    "                    'reuters21578-mld/reuters21578.tar.gz')\n",
    "    ARCHIVE_FILENAME = 'reuters21578.tar.gz'\n",
    "\n",
    "    if data_path is None:\n",
    "        data_path = os.path.join(get_data_home(), \"reuters\")\n",
    "    if not os.path.exists(data_path):\n",
    "        \"\"\"Download the dataset.\"\"\"\n",
    "        print(\"downloading dataset (once and for all) into %s\" %\n",
    "              data_path)\n",
    "        os.mkdir(data_path)\n",
    "\n",
    "        def progress(blocknum, bs, size):\n",
    "            total_sz_mb = '%.2f MB' % (size / 1e6)\n",
    "            current_sz_mb = '%.2f MB' % ((blocknum * bs) / 1e6)\n",
    "            if _not_in_sphinx():\n",
    "                print('\\rdownloaded %s / %s' % (current_sz_mb, total_sz_mb),\n",
    "                      end='')\n",
    "\n",
    "        archive_path = os.path.join(data_path, ARCHIVE_FILENAME)\n",
    "        urllib.request.urlretrieve(DOWNLOAD_URL, filename=archive_path,\n",
    "                                   reporthook=progress)\n",
    "        if _not_in_sphinx():\n",
    "            print('\\r', end='')\n",
    "        print(\"untarring Reuters dataset...\")\n",
    "        tarfile.open(archive_path, 'r:gz').extractall(data_path)\n",
    "        print(\"done.\")\n",
    "\n",
    "    parser = ReutersParser()\n",
    "    for filename in glob(os.path.join(data_path, \"*.sgm\")):\n",
    "        for doc in parser.parse(open(filename, 'rb')):\n",
    "            yield doc\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Main\n",
    "###############################################################################\n",
    "# Create the vectorizer and limit the number of features to a reasonable\n",
    "# maximum\n",
    "vectorizer = HashingVectorizer(decode_error='ignore', n_features=2 ** 18,\n",
    "                               non_negative=True)\n",
    "\n",
    "\n",
    "# Iterator over parsed Reuters SGML files.\n",
    "data_stream = stream_reuters_documents()\n",
    "\n",
    "# We learn a binary classification between the \"acq\" class and all the others.\n",
    "# \"acq\" was chosen as it is more or less evenly distributed in the Reuters\n",
    "# files. For other datasets, one should take care of creating a test set with\n",
    "# a realistic portion of positive instances.\n",
    "all_classes = np.array([0, 1])\n",
    "positive_class = 'acq'\n",
    "\n",
    "# Here are some classifiers that support the `partial_fit` method\n",
    "partial_fit_classifiers = {\n",
    "    'SGD': SGDClassifier(),\n",
    "    'Perceptron': Perceptron(),\n",
    "    'NB Multinomial': MultinomialNB(alpha=0.01),\n",
    "    'Passive-Aggressive': PassiveAggressiveClassifier(),\n",
    "}\n",
    "\n",
    "\n",
    "def get_minibatch(doc_iter, size, pos_class=positive_class):\n",
    "    \"\"\"Extract a minibatch of examples, return a tuple X_text, y.\n",
    "\n",
    "    Note: size is before excluding invalid docs with no topics assigned.\n",
    "\n",
    "    \"\"\"\n",
    "    data = [(u'{title}\\n\\n{body}'.format(**doc), pos_class in doc['topics'])\n",
    "            for doc in itertools.islice(doc_iter, size)\n",
    "            if doc['topics']]\n",
    "    if not len(data):\n",
    "        return np.asarray([], dtype=int), np.asarray([], dtype=int)\n",
    "    X_text, y = zip(*data)\n",
    "    return X_text, np.asarray(y, dtype=int)\n",
    "\n",
    "\n",
    "def iter_minibatches(doc_iter, minibatch_size):\n",
    "    \"\"\"Generator of minibatches.\"\"\"\n",
    "    X_text, y = get_minibatch(doc_iter, minibatch_size)\n",
    "    while len(X_text):\n",
    "        yield X_text, y\n",
    "        X_text, y = get_minibatch(doc_iter, minibatch_size)\n",
    "\n",
    "\n",
    "# test data statistics\n",
    "test_stats = {'n_test': 0, 'n_test_pos': 0}\n",
    "\n",
    "# First we hold out a number of examples to estimate accuracy\n",
    "n_test_documents = 1000\n",
    "tick = time.time()\n",
    "X_test_text, y_test = get_minibatch(data_stream, 1000)\n",
    "parsing_time = time.time() - tick\n",
    "tick = time.time()\n",
    "X_test = vectorizer.transform(X_test_text)\n",
    "vectorizing_time = time.time() - tick\n",
    "test_stats['n_test'] += len(y_test)\n",
    "test_stats['n_test_pos'] += sum(y_test)\n",
    "print(\"Test set is %d documents (%d positive)\" % (len(y_test), sum(y_test)))\n",
    "\n",
    "\n",
    "def progress(cls_name, stats):\n",
    "    \"\"\"Report progress information, return a string.\"\"\"\n",
    "    duration = time.time() - stats['t0']\n",
    "    s = \"%20s classifier : \\t\" % cls_name\n",
    "    s += \"%(n_train)6d train docs (%(n_train_pos)6d positive) \" % stats\n",
    "    s += \"%(n_test)6d test docs (%(n_test_pos)6d positive) \" % test_stats\n",
    "    s += \"accuracy: %(accuracy).3f \" % stats\n",
    "    s += \"in %.2fs (%5d docs/s)\" % (duration, stats['n_train'] / duration)\n",
    "    return s\n",
    "\n",
    "\n",
    "cls_stats = {}\n",
    "\n",
    "for cls_name in partial_fit_classifiers:\n",
    "    stats = {'n_train': 0, 'n_train_pos': 0,\n",
    "             'accuracy': 0.0, 'accuracy_history': [(0, 0)], 't0': time.time(),\n",
    "             'runtime_history': [(0, 0)], 'total_fit_time': 0.0}\n",
    "    cls_stats[cls_name] = stats\n",
    "\n",
    "get_minibatch(data_stream, n_test_documents)\n",
    "# Discard test set\n",
    "\n",
    "# We will feed the classifier with mini-batches of 1000 documents; this means\n",
    "# we have at most 1000 docs in memory at any time.  The smaller the document\n",
    "# batch, the bigger the relative overhead of the partial fit methods.\n",
    "minibatch_size = 1000\n",
    "\n",
    "# Create the data_stream that parses Reuters SGML files and iterates on\n",
    "# documents as a stream.\n",
    "minibatch_iterators = iter_minibatches(data_stream, minibatch_size)\n",
    "total_vect_time = 0.0\n",
    "\n",
    "# Main loop : iterate on mini-batchs of examples\n",
    "for i, (X_train_text, y_train) in enumerate(minibatch_iterators):\n",
    "\n",
    "    tick = time.time()\n",
    "    X_train = vectorizer.transform(X_train_text)\n",
    "    total_vect_time += time.time() - tick\n",
    "\n",
    "    for cls_name, cls in partial_fit_classifiers.items():\n",
    "        tick = time.time()\n",
    "        # update estimator with examples in the current mini-batch\n",
    "        cls.partial_fit(X_train, y_train, classes=all_classes)\n",
    "\n",
    "        # accumulate test accuracy stats\n",
    "        cls_stats[cls_name]['total_fit_time'] += time.time() - tick\n",
    "        cls_stats[cls_name]['n_train'] += X_train.shape[0]\n",
    "        cls_stats[cls_name]['n_train_pos'] += sum(y_train)\n",
    "        tick = time.time()\n",
    "        cls_stats[cls_name]['accuracy'] = cls.score(X_test, y_test)\n",
    "        cls_stats[cls_name]['prediction_time'] = time.time() - tick\n",
    "        acc_history = (cls_stats[cls_name]['accuracy'],\n",
    "                       cls_stats[cls_name]['n_train'])\n",
    "        cls_stats[cls_name]['accuracy_history'].append(acc_history)\n",
    "        run_history = (cls_stats[cls_name]['accuracy'],\n",
    "                       total_vect_time + cls_stats[cls_name]['total_fit_time'])\n",
    "        cls_stats[cls_name]['runtime_history'].append(run_history)\n",
    "\n",
    "        if i % 3 == 0:\n",
    "            print(progress(cls_name, cls_stats[cls_name]))\n",
    "    if i % 3 == 0:\n",
    "        print('\\n')\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Plot results\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "def plot_accuracy(x, y, x_legend):\n",
    "    \"\"\"Plot accuracy as a function of x.\"\"\"\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    plt.title('Classification accuracy as a function of %s' % x_legend)\n",
    "    plt.xlabel('%s' % x_legend)\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.grid(True)\n",
    "    plt.plot(x, y)\n",
    "\n",
    "rcParams['legend.fontsize'] = 10\n",
    "cls_names = list(sorted(cls_stats.keys()))\n",
    "\n",
    "# Plot accuracy evolution\n",
    "plt.figure()\n",
    "for _, stats in sorted(cls_stats.items()):\n",
    "    # Plot accuracy evolution with #examples\n",
    "    accuracy, n_examples = zip(*stats['accuracy_history'])\n",
    "    plot_accuracy(n_examples, accuracy, \"training examples (#)\")\n",
    "    ax = plt.gca()\n",
    "    ax.set_ylim((0.8, 1))\n",
    "plt.legend(cls_names, loc='best')\n",
    "\n",
    "plt.figure()\n",
    "for _, stats in sorted(cls_stats.items()):\n",
    "    # Plot accuracy evolution with runtime\n",
    "    accuracy, runtime = zip(*stats['runtime_history'])\n",
    "    plot_accuracy(runtime, accuracy, 'runtime (s)')\n",
    "    ax = plt.gca()\n",
    "    ax.set_ylim((0.8, 1))\n",
    "plt.legend(cls_names, loc='best')\n",
    "\n",
    "# Plot fitting times\n",
    "plt.figure()\n",
    "fig = plt.gcf()\n",
    "cls_runtime = []\n",
    "for cls_name, stats in sorted(cls_stats.items()):\n",
    "    cls_runtime.append(stats['total_fit_time'])\n",
    "\n",
    "cls_runtime.append(total_vect_time)\n",
    "cls_names.append('Vectorization')\n",
    "bar_colors = rcParams['axes.color_cycle'][:len(cls_names)]\n",
    "\n",
    "ax = plt.subplot(111)\n",
    "rectangles = plt.bar(range(len(cls_names)), cls_runtime, width=0.5,\n",
    "                     color=bar_colors)\n",
    "\n",
    "ax.set_xticks(np.linspace(0.25, len(cls_names) - 0.75, len(cls_names)))\n",
    "ax.set_xticklabels(cls_names, fontsize=10)\n",
    "ymax = max(cls_runtime) * 1.2\n",
    "ax.set_ylim((0, ymax))\n",
    "ax.set_ylabel('runtime (s)')\n",
    "ax.set_title('Training Times')\n",
    "\n",
    "\n",
    "def autolabel(rectangles):\n",
    "    \"\"\"attach some text vi autolabel on rectangles.\"\"\"\n",
    "    for rect in rectangles:\n",
    "        height = rect.get_height()\n",
    "        ax.text(rect.get_x() + rect.get_width() / 2.,\n",
    "                1.05 * height, '%.4f' % height,\n",
    "                ha='center', va='bottom')\n",
    "\n",
    "autolabel(rectangles)\n",
    "plt.show()\n",
    "\n",
    "# Plot prediction times\n",
    "plt.figure()\n",
    "#fig = plt.gcf()\n",
    "cls_runtime = []\n",
    "cls_names = list(sorted(cls_stats.keys()))\n",
    "for cls_name, stats in sorted(cls_stats.items()):\n",
    "    cls_runtime.append(stats['prediction_time'])\n",
    "cls_runtime.append(parsing_time)\n",
    "cls_names.append('Read/Parse\\n+Feat.Extr.')\n",
    "cls_runtime.append(vectorizing_time)\n",
    "cls_names.append('Hashing\\n+Vect.')\n",
    "bar_colors = rcParams['axes.color_cycle'][:len(cls_names)]\n",
    "\n",
    "ax = plt.subplot(111)\n",
    "rectangles = plt.bar(range(len(cls_names)), cls_runtime, width=0.5,\n",
    "                     color=bar_colors)\n",
    "\n",
    "ax.set_xticks(np.linspace(0.25, len(cls_names) - 0.75, len(cls_names)))\n",
    "ax.set_xticklabels(cls_names, fontsize=8)\n",
    "plt.setp(plt.xticks()[1], rotation=30)\n",
    "ymax = max(cls_runtime) * 1.2\n",
    "ax.set_ylim((0, ymax))\n",
    "ax.set_ylabel('runtime (s)')\n",
    "ax.set_title('Prediction Times (%d instances)' % n_test_documents)\n",
    "autolabel(rectangles)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q1.1.2 Explore how many documents are in the dataset, how many categories, how many documents per categories, provide mean and standard deviation, min and max. (Hint: use the pandas library to explore the dataset, use the dataframe.describe() method.)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Note: installed modules: panda, request, dict, public, self, get, query_string, post\n",
    "~~Tried to import panda instead of pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#From the code, accessing the reuters document data base. \n",
    "data_stream = stream_reuters_documents()\n",
    "#Experimenting with functions given in hint. \n",
    "df = pd.DataFrame(data_stream)\n",
    "print(\"The type of df is: \", type(df))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many documents in the dataset? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['title'].describe()['count']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Chrysler Corp said car sales for the March 21-...\n",
       "1        Compaq Computer Corp, IBM's chief rival in the...\n",
       "2        <Noranda Inc> said production will remain shut...\n",
       "3        The Canadian government's budget deficit rose ...\n",
       "4        CIS Technologies Inc said it executed a formal...\n",
       "5        Qtly div 42 cts vs 41.5 cts prior Payable APri...\n",
       "6        Colombia's cost of living index rose 2.71 pct ...\n",
       "7        The Federal Home Loan Bank Board said home mor...\n",
       "8        The New York Stock Exchange said a seat on the...\n",
       "9                                                         \n",
       "10       Beneficial Corp said the sale of its American ...\n",
       "11       European options exchanges will see spectacula...\n",
       "12       Tierco Group INc said it sold at par to the Ku...\n",
       "13       Shr 5.56 dlrs vs 3.88 dlrs Net 47.5 mln vs 33....\n",
       "14       California Micro Devices Corp said an addition...\n",
       "15       Stewart INformation Services Corp said it resc...\n",
       "16       FIserve Inc said 14 savings and loans with 1.5...\n",
       "17       Mony Real Estate Investors Trust said its inve...\n",
       "18       Canadian narrowly-defined money supply M-1 fel...\n",
       "19       Copley Properties Inc said the company will in...\n",
       "20       The United Food And Commercial Workers said th...\n",
       "21       Pan Am Corp's Pan American World Airways said ...\n",
       "22       FCS Laboratories Inc said merger discussions w...\n",
       "23       The White House said the rise in interest rate...\n",
       "24       Shr loss nine cts Net loss 1.4 mln Revs 630,11...\n",
       "25       Shr 1.1 cts vs 1.7 cts Net 26,708 vs 35,084 Re...\n",
       "26       Shr nine cts vs 19 cts Net 188,000 vs 362,000 ...\n",
       "27                                                        \n",
       "28                                                        \n",
       "29       Burlington Northern Inc said its Burlington No...\n",
       "                               ...                        \n",
       "21548    Hundreds of marines were on alert at 11 key Br...\n",
       "21549    Indonesia's Armed Forces Commander General Ben...\n",
       "21550    The suspension of Ecuador's crude oil shipment...\n",
       "21551    Any European Community decision to liberalise ...\n",
       "21552    China has raised the prices it pays farmers fo...\n",
       "21553    Australia sold 180,000 tonnes of raw sugar to ...\n",
       "21554    Thai natural rubber exports rose to 763,331 to...\n",
       "21555    Australian beef output declined to 104,353 ton...\n",
       "21556    Increased federal government borrowing needs a...\n",
       "21557    Philippine President Corazon Aquino swore in f...\n",
       "21558    Kenya and Shenzhen Electronics Group (SEG) of ...\n",
       "21559    The Soviet Union has agreed to cut its coking ...\n",
       "21560    <Placer Pacific Ltd> said it hopes the Papua N...\n",
       "21561    The special prosecutor in the Iran arms scanda...\n",
       "21562    The Bangladesh trade deficit narrowed to 1.91 ...\n",
       "21563    The outlook for Australian industrial investme...\n",
       "21564    The central bank has issued 12 billion Taiwan ...\n",
       "21565    Three Japanese credit rating agencies are ente...\n",
       "21566    Most commodity agreements are close to collaps...\n",
       "21567    Swiss industrial output rose nine pct in the f...\n",
       "21568    Japanese securities houses are thinking of all...\n",
       "21569    The United States has openly attacked Japan's ...\n",
       "21570    The Asian dollar market continued to expand in...\n",
       "21571    Singapore's M-1 money supply rose 3.7 pct duri...\n",
       "21572    The value of China's industrial output in Janu...\n",
       "21573    The Dutch central bank said it has accepted bi...\n",
       "21574    Union Bank of Finland is issuing a 10 billion ...\n",
       "21575    Japanese customers have bought nearly six mln ...\n",
       "21576    Imperial Chemical Industries Plc <ICI.L> said ...\n",
       "21577    Unilever Australia Ltd is issuing 40 mln Austr...\n",
       "Name: body, dtype: object"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['body']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 21,578 as seen below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Some exploring of dataset: \n",
    "if q1Verbose:\n",
    "    print(list(df))\n",
    "    print(df['body'].describe())\n",
    "    # Since df['topics'].descibe() does not work, let's just print. \n",
    "    print(df['topics'])\n",
    "    print(type(df['topics']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Each document can belong to a few topics (we interperted topics for categories). We will feed all information into a FreqDist and receive statistics.\n",
    "\n",
    "As implied, this means a document can appear in several documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create a list of all occurences of all topics and feed to FreqDist. \n",
    "freq_dist = nltk.FreqDist(sum(list(df['topics']), []))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many categories: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "category_set=set(sum(list(df['topics']), []))\n",
    "num_of_categories = len(category_set)\n",
    "print(\"The number of categories is: \",num_of_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are using a Frequency distribution, the number of categories can also be retrieved by: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "445"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(freq_dist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many documents per category: \n",
    "Since we are using a Frequency distribution, the number of documents per category \n",
    "is the value in freq_dist[category]. \n",
    "\n",
    "We can print a list of all categories and the number of documents in them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cat_numOfDocs = [(category, freq_dist[category]) for category in category_set]\n",
    "for pair in cat_numOfDocs[:10]: \n",
    "    print('Category: ', pair[0], 'has ', pair[1], 'Docs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Provide mean and standard deviation, min and max. \n",
    "Mean: Mean number of documents per categorie. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[num_of_docs for _, num_of_docs in cat_numOfDocs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Mean: \n",
    "#Sum of number of documents per each category. \n",
    "sum_docs_cat = sum(num_of_docs for (cat, num_of_docs) in cat_numOfDocs)\n",
    "#\n",
    "#Mean expected number of documents per categorie. \n",
    "#mean_exp=sum(freq_dist.freq(cat)*num_of_docs for (cat, num_of_docs) in cat_numOfDocs)\n",
    "#mean2 = np.mean([num_of_docs for _, num_of_docs in cat_numOfDocs])\n",
    "#print('The Mean number of documents per categorie is: ', mean_exp)\n",
    "#print('The Mean2 number of documents per categorie is: ', mean2)\n",
    "mean = sum_docs_cat/len(cat_numOfDocs)\n",
    "print('The Mean (Average) number of documents per category is: ', mean)\n",
    "\n",
    "#Max:\n",
    "print('The categorie with maximum documents is: \"',freq_dist.max(), '\"which has ', freq_dist[freq_dist.max()], ' documents.')\n",
    "\n",
    "#Min:\n",
    "min_num_of_docs = sorted(cat_numOfDocs ,key=lambda x: x[1])[0][1]\n",
    "cats_w_min_num_of_docs = [cat for (cat, num_of_docs) in cat_numOfDocs if num_of_docs==min_num_of_docs]\n",
    "display = 3 #Display only part of categories, not all. \n",
    "print('The category with minimum documents are:',cats_w_min_num_of_docs[:display], 'who have', min_num_of_docs, 'documents each. ' )\n",
    "\n",
    "#Standard deviation: \n",
    "std_dev = math.sqrt(sum( (math.pow(num_of_docs-mean, 2) for (_, num_of_docs) in cat_numOfDocs))/len(cat_numOfDocs))\n",
    "print('The standard deviation in number of documents per category is:', std_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1.1.2 Explore how many characters and words are present in the documents of the dataset.\n",
    "\n",
    "first we consider all diffferent word tokens and characters in code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Create sets of words and characters. \n",
    "#Takes a while to run, use with care :)\n",
    "word_set=set()\n",
    "for i in range(len(df['body'])): \n",
    "    word_set.update(word_tokenize(df['body'][i]))\n",
    "\n",
    "\n",
    "char_set=set()\n",
    "for word in word_set: \n",
    "    for letter in word: \n",
    "        char_set.update(letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 76886 different words in all documents. \n",
      "There are 89 different characters in all documents. \n"
     ]
    }
   ],
   "source": [
    "print('There are %d different words in all documents. ' %len(word_set))\n",
    "print('There are %d different characters in all documents. ' %len(char_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now construct a dictionary, That maps from article index to {num_of_words: , num_of_chars: }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "article_2words_chars = {}\n",
    "for i in range(len(df['body'])): \n",
    "    article_2words_chars[i] = (len(word_tokenize(df['body'][i])), len(df['body'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def explore_doc(i):\n",
    "    print('Document with index %d has %d words and %d letters' % (x, article_2words_chars[x][0], article_2words_chars[x][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Q1.1.3 Explore how many characters and words are present in the documents of the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total emails classified: 5078\n",
      "Score: 0.0\n",
      "Confusion matrix:\n",
      "[[5078 5078]\n",
      " [5078 5078]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy\n",
    "from pandas import DataFrame\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "NEWLINE = '\\n'\n",
    "\n",
    "HAM = 'ham'\n",
    "SPAM = 'spam'\n",
    "\n",
    "SOURCES = [\n",
    "    ('data/spam',        SPAM),\n",
    "    ('data/easy_ham',    HAM),\n",
    "    ('data/hard_ham',    HAM),\n",
    "    ('data/beck-s',      HAM),\n",
    "    ('data/farmer-d',    HAM),\n",
    "    ('data/kaminski-v',  HAM),\n",
    "    ('data/kitchen-l',   HAM),\n",
    "    ('data/lokay-m',     HAM),\n",
    "    ('data/williams-w3', HAM),\n",
    "    ('data/BG',          SPAM),\n",
    "    ('data/GP',          SPAM),\n",
    "    ('data/SH',          SPAM)\n",
    "]\n",
    "\n",
    "SKIP_FILES = {'cmds'}\n",
    "\n",
    "\n",
    "def read_files(path):\n",
    "    for root, dir_names, file_names in os.walk(path):\n",
    "        for path in dir_names:\n",
    "            read_files(os.path.join(root, path))\n",
    "        for file_name in file_names:\n",
    "            if file_name not in SKIP_FILES:\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                if os.path.isfile(file_path):\n",
    "                    past_header, lines = False, []\n",
    "                    f = open(file_path, encoding=\"latin-1\")\n",
    "                    for line in f:\n",
    "                        if past_header:\n",
    "                            lines.append(line)\n",
    "                        elif line == NEWLINE:\n",
    "                            past_header = True\n",
    "                    f.close()\n",
    "                    content = NEWLINE.join(lines)\n",
    "                    yield file_path, content\n",
    "\n",
    "\n",
    "def build_data_frame(path, classification):\n",
    "    rows = []\n",
    "    index = []\n",
    "    for file_name, text in read_files(path):\n",
    "        rows.append({'text': text, 'class': classification})\n",
    "        index.append(file_name)\n",
    "\n",
    "    data_frame = DataFrame(rows, index=index)\n",
    "    return data_frame\n",
    "\n",
    "data = DataFrame({'text': [], 'class': []})\n",
    "for path, classification in SOURCES:\n",
    "    data = data.append(build_data_frame(path, classification))\n",
    "\n",
    "data = data.reindex(numpy.random.permutation(data.index))\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('count_vectorizer',   CountVectorizer(ngram_range=(1, 2))),\n",
    "    ('classifier',         MultinomialNB())\n",
    "])\n",
    "\n",
    "k_fold = KFold(n=len(data), n_folds=6)\n",
    "scores = []\n",
    "confusion = numpy.array([[0, 0], [0, 0]])\n",
    "for train_indices, test_indices in k_fold:\n",
    "    train_text = data.iloc[train_indices]['text'].values\n",
    "    train_y = data.iloc[train_indices]['class'].values.astype(str)\n",
    "\n",
    "    test_text = data.iloc[test_indices]['text'].values\n",
    "    test_y = data.iloc[test_indices]['class'].values.astype(str)\n",
    "\n",
    "    pipeline.fit(train_text, train_y)\n",
    "    predictions = pipeline.predict(test_text)\n",
    "\n",
    "    confusion += confusion_matrix(test_y, predictions)\n",
    "    score = f1_score(test_y, predictions, pos_label=SPAM)\n",
    "    scores.append(score)\n",
    "\n",
    "print('Total emails classified:', len(data))\n",
    "print('Score:', sum(scores)/len(scores))\n",
    "print('Confusion matrix:')\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p=pipeline.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count_vectorizer \n",
      " CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) <class 'sklearn.feature_extraction.text.CountVectorizer'>\n",
      "count_vectorizer__encoding \n",
      " utf-8 <class 'str'>\n",
      "count_vectorizer__max_features \n",
      " None <class 'NoneType'>\n",
      "classifier__class_prior \n",
      " None <class 'NoneType'>\n",
      "count_vectorizer__lowercase \n",
      " True <class 'bool'>\n",
      "count_vectorizer__strip_accents \n",
      " None <class 'NoneType'>\n",
      "count_vectorizer__min_df \n",
      " 1 <class 'int'>\n",
      "count_vectorizer__dtype \n",
      " <class 'numpy.int64'> <class 'type'>\n",
      "count_vectorizer__analyzer \n",
      " word <class 'str'>\n",
      "count_vectorizer__stop_words \n",
      " None <class 'NoneType'>\n",
      "classifier__fit_prior \n",
      " True <class 'bool'>\n",
      "count_vectorizer__ngram_range \n",
      " (1, 2) <class 'tuple'>\n",
      "count_vectorizer__token_pattern \n",
      " (?u)\\b\\w\\w+\\b <class 'str'>\n",
      "count_vectorizer__tokenizer \n",
      " None <class 'NoneType'>\n",
      "steps \n",
      " [('count_vectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)), ('classifier', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))] <class 'list'>\n",
      "classifier__alpha \n",
      " 1.0 <class 'float'>\n",
      "classifier \n",
      " MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True) <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "count_vectorizer__input \n",
      " content <class 'str'>\n",
      "count_vectorizer__preprocessor \n",
      " None <class 'NoneType'>\n",
      "count_vectorizer__max_df \n",
      " 1.0 <class 'float'>\n",
      "count_vectorizer__vocabulary \n",
      " None <class 'NoneType'>\n",
      "count_vectorizer__binary \n",
      " False <class 'bool'>\n",
      "count_vectorizer__decode_error \n",
      " strict <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "type(p)\n",
    "list(p)\n",
    "for x in list(p):\n",
    "    print(x, '\\n',  p[x], type(p[x]), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(p['count_vectorizer__input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CountV=p['count_vectorizer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '00 00',\n",
       " '00 005',\n",
       " '00 00pm',\n",
       " '00 01',\n",
       " '00 09withdrawal',\n",
       " '00 10',\n",
       " '00 11',\n",
       " '00 1104',\n",
       " '00 117',\n",
       " '00 12',\n",
       " '00 120',\n",
       " '00 1200',\n",
       " '00 13',\n",
       " '00 14',\n",
       " '00 15',\n",
       " '00 16',\n",
       " '00 17',\n",
       " '00 18',\n",
       " '00 19',\n",
       " '00 20',\n",
       " '00 200',\n",
       " '00 21',\n",
       " '00 22',\n",
       " '00 23',\n",
       " '00 24',\n",
       " '00 25',\n",
       " '00 27',\n",
       " '00 30',\n",
       " '00 30p',\n",
       " '00 30pm',\n",
       " '00 36',\n",
       " '00 37',\n",
       " '00 40',\n",
       " '00 400',\n",
       " '00 45',\n",
       " '00 50',\n",
       " '00 54',\n",
       " '00 56',\n",
       " '00 59',\n",
       " '00 69',\n",
       " '00 75',\n",
       " '00 85',\n",
       " '00 855',\n",
       " '00 97',\n",
       " '00 ___________________________________________________________________________',\n",
       " '00 adder',\n",
       " '00 administrative',\n",
       " '00 adult',\n",
       " '00 all',\n",
       " '00 alport',\n",
       " '00 also',\n",
       " '00 am',\n",
       " '00 amoco',\n",
       " '00 amy',\n",
       " '00 and',\n",
       " '00 annuity',\n",
       " '00 antioch',\n",
       " '00 approval',\n",
       " '00 arlington',\n",
       " '00 as',\n",
       " '00 aspen',\n",
       " '00 at',\n",
       " '00 atlanta',\n",
       " '00 atlantic',\n",
       " '00 attached',\n",
       " '00 axford',\n",
       " '00 badeer',\n",
       " '00 baltimore',\n",
       " '00 bangkok',\n",
       " '00 barcelona',\n",
       " '00 base',\n",
       " '00 beaver',\n",
       " '00 belden',\n",
       " '00 bid_price',\n",
       " '00 boston',\n",
       " '00 br',\n",
       " '00 brainerd',\n",
       " '00 bring',\n",
       " '00 bruce',\n",
       " '00 buerkle',\n",
       " '00 buffalo',\n",
       " '00 bus',\n",
       " '00 but',\n",
       " '00 by',\n",
       " '00 cairo',\n",
       " '00 calger',\n",
       " '00 can',\n",
       " '00 capacity',\n",
       " '00 carbondale',\n",
       " '00 central',\n",
       " '00 charge',\n",
       " '00 chatterton',\n",
       " '00 chicago',\n",
       " '00 child',\n",
       " '00 children',\n",
       " '00 chris',\n",
       " '00 cig',\n",
       " '00 cincinnati',\n",
       " '00 cinergy',\n",
       " '00 claiming',\n",
       " '00 clark',\n",
       " '00 click',\n",
       " '00 clock',\n",
       " '00 cob',\n",
       " '00 coffee',\n",
       " '00 columbia',\n",
       " '00 come',\n",
       " '00 computer',\n",
       " '00 continental',\n",
       " '00 cost',\n",
       " '00 craig',\n",
       " '00 creating',\n",
       " '00 cst',\n",
       " '00 daily',\n",
       " '00 dallas',\n",
       " '00 davidson',\n",
       " '00 deal',\n",
       " '00 debra',\n",
       " '00 deficient',\n",
       " '00 denver',\n",
       " '00 depending',\n",
       " '00 deposit',\n",
       " '00 dinner',\n",
       " '00 downtown',\n",
       " '00 driscoll',\n",
       " '00 dubai',\n",
       " '00 duration',\n",
       " '00 ea',\n",
       " '00 each',\n",
       " '00 east',\n",
       " '00 eastern',\n",
       " '00 effective',\n",
       " '00 elafandi',\n",
       " '00 enron',\n",
       " '00 espeak',\n",
       " '00 ets',\n",
       " '00 ews',\n",
       " '00 expect',\n",
       " '00 external',\n",
       " '00 facilities',\n",
       " '00 features',\n",
       " '00 featuring',\n",
       " '00 filming',\n",
       " '00 final',\n",
       " '00 financial',\n",
       " '00 flat',\n",
       " '00 flower',\n",
       " '00 for',\n",
       " '00 forwarded',\n",
       " '00 frankfurt',\n",
       " '00 friday',\n",
       " '00 from',\n",
       " '00 fuller',\n",
       " '00 gas',\n",
       " '00 gate',\n",
       " '00 gets',\n",
       " '00 gilbert',\n",
       " '00 grand',\n",
       " '00 guillaume',\n",
       " '00 had',\n",
       " '00 have',\n",
       " '00 having',\n",
       " '00 he',\n",
       " '00 hebron',\n",
       " '00 highland',\n",
       " '00 holiday',\n",
       " '00 hopland',\n",
       " '00 houston',\n",
       " '00 html',\n",
       " '00 human',\n",
       " '00 if',\n",
       " '00 in',\n",
       " '00 istanbul',\n",
       " '00 it',\n",
       " '00 jacksonville',\n",
       " '00 john',\n",
       " '00 just',\n",
       " '00 kassel',\n",
       " '00 ken',\n",
       " '00 kern',\n",
       " '00 lake',\n",
       " '00 lansing',\n",
       " '00 las',\n",
       " '00 ld',\n",
       " '00 little',\n",
       " '00 location',\n",
       " '00 london',\n",
       " '00 look',\n",
       " '00 lorna',\n",
       " '00 los',\n",
       " '00 madison',\n",
       " '00 make',\n",
       " '00 malvern',\n",
       " '00 march',\n",
       " '00 margaret',\n",
       " '00 marriott',\n",
       " '00 may',\n",
       " '00 melbourne',\n",
       " '00 member',\n",
       " '00 members',\n",
       " '00 miami',\n",
       " '00 mid',\n",
       " '00 milwaukee',\n",
       " '00 minneapolis',\n",
       " '00 mmbtu',\n",
       " '00 mojave',\n",
       " '00 monda',\n",
       " '00 monday',\n",
       " '00 montreal',\n",
       " '00 mst',\n",
       " '00 mwh',\n",
       " '00 na',\n",
       " '00 name',\n",
       " '00 near',\n",
       " '00 nesa',\n",
       " '00 network',\n",
       " '00 new',\n",
       " '00 newark',\n",
       " '00 no',\n",
       " '00 non',\n",
       " '00 noon',\n",
       " '00 not',\n",
       " '00 note',\n",
       " '00 np',\n",
       " '00 omaha',\n",
       " '00 on',\n",
       " '00 or',\n",
       " '00 organizations',\n",
       " '00 orlando',\n",
       " '00 ottawa',\n",
       " '00 our',\n",
       " '00 over',\n",
       " '00 pacific',\n",
       " '00 palo',\n",
       " '00 pattaya',\n",
       " '00 pdt',\n",
       " '00 people',\n",
       " '00 per',\n",
       " '00 pg',\n",
       " '00 philadelphia',\n",
       " '00 phoenix',\n",
       " '00 pittsburgh',\n",
       " '00 place',\n",
       " '00 please',\n",
       " '00 pm',\n",
       " '00 pnm',\n",
       " '00 portland',\n",
       " '00 premium',\n",
       " '00 price_units',\n",
       " '00 providence',\n",
       " '00 pst',\n",
       " '00 pwx',\n",
       " '00 quebec',\n",
       " '00 rasmussen',\n",
       " '00 recent',\n",
       " '00 register',\n",
       " '00 regular',\n",
       " '00 reyes',\n",
       " '00 rsvps',\n",
       " '00 salt',\n",
       " '00 san',\n",
       " '00 santa',\n",
       " '00 scholtes',\n",
       " '00 seattle',\n",
       " '00 semperger',\n",
       " '00 shanghai',\n",
       " '00 share',\n",
       " '00 sheppard',\n",
       " '00 shields',\n",
       " '00 short',\n",
       " '00 singles',\n",
       " '00 slaughter',\n",
       " '00 snowmass',\n",
       " '00 so',\n",
       " '00 soderquist',\n",
       " '00 sp',\n",
       " '00 span',\n",
       " '00 special',\n",
       " '00 sponsor',\n",
       " '00 st',\n",
       " '00 steam',\n",
       " '00 steiner',\n",
       " '00 stop',\n",
       " '00 strength',\n",
       " '00 tampa',\n",
       " '00 teams',\n",
       " '00 telephony',\n",
       " '00 test',\n",
       " '00 thanks',\n",
       " '00 the',\n",
       " '00 their',\n",
       " '00 then',\n",
       " '00 therefore',\n",
       " '00 these',\n",
       " '00 they',\n",
       " '00 this',\n",
       " '00 three',\n",
       " '00 through',\n",
       " '00 thursday',\n",
       " '00 ticket',\n",
       " '00 tickets',\n",
       " '00 tim',\n",
       " '00 timeslot',\n",
       " '00 to',\n",
       " '00 toby',\n",
       " '00 today',\n",
       " '00 tonight',\n",
       " '00 total',\n",
       " '00 totals',\n",
       " '00 trans_type',\n",
       " '00 tully',\n",
       " '00 tw',\n",
       " '00 two',\n",
       " '00 unl',\n",
       " '00 until',\n",
       " '00 us',\n",
       " '00 usgt',\n",
       " '00 vail',\n",
       " '00 van',\n",
       " '00 variable',\n",
       " '00 villeggiante',\n",
       " '00 virginia',\n",
       " '00 virgpwr',\n",
       " '00 warner',\n",
       " '00 warsaw',\n",
       " '00 washington',\n",
       " '00 washngton',\n",
       " '00 we',\n",
       " '00 wednesday',\n",
       " '00 wente',\n",
       " '00 what',\n",
       " '00 which',\n",
       " '00 will',\n",
       " '00 williams',\n",
       " '00 with',\n",
       " '00 yoder',\n",
       " '00 your',\n",
       " '000',\n",
       " '000 00',\n",
       " '000 000',\n",
       " '000 02',\n",
       " '000 0200',\n",
       " '000 03',\n",
       " '000 0300',\n",
       " '000 0800',\n",
       " '000 09',\n",
       " '000 0900',\n",
       " '000 10',\n",
       " '000 1000',\n",
       " '000 12',\n",
       " '000 124',\n",
       " '000 14',\n",
       " '000 15',\n",
       " '000 16',\n",
       " '000 18',\n",
       " '000 19',\n",
       " '000 20',\n",
       " '000 200',\n",
       " '000 21',\n",
       " '000 21days',\n",
       " '000 22',\n",
       " '000 23',\n",
       " '000 238',\n",
       " '000 24',\n",
       " '000 25',\n",
       " '000 26',\n",
       " '000 27',\n",
       " '000 27813',\n",
       " '000 27819',\n",
       " '000 27821',\n",
       " '000 28',\n",
       " '000 28days',\n",
       " '000 29',\n",
       " '000 30',\n",
       " '000 31',\n",
       " '000 310',\n",
       " '000 32',\n",
       " '000 33',\n",
       " '000 34',\n",
       " '000 35',\n",
       " '000 36',\n",
       " '000 363',\n",
       " '000 37',\n",
       " '000 370',\n",
       " '000 38',\n",
       " '000 3883',\n",
       " '000 39',\n",
       " '000 40',\n",
       " '000 400',\n",
       " '000 41',\n",
       " '000 416',\n",
       " '000 463',\n",
       " '000 50',\n",
       " '000 500',\n",
       " '000 51',\n",
       " '000 55',\n",
       " '000 60',\n",
       " '000 600',\n",
       " '000 62',\n",
       " '000 642',\n",
       " '000 659',\n",
       " '000 674',\n",
       " '000 68',\n",
       " '000 686',\n",
       " '000 70',\n",
       " '000 71',\n",
       " '000 74',\n",
       " '000 76',\n",
       " '000 768',\n",
       " '000 80',\n",
       " '000 800',\n",
       " '000 83',\n",
       " '000 85',\n",
       " '000 87',\n",
       " '000 961',\n",
       " '000 acres',\n",
       " '000 adults',\n",
       " '000 along',\n",
       " '000 amw',\n",
       " '000 and',\n",
       " '000 annual',\n",
       " '000 annually',\n",
       " '000 apr',\n",
       " '000 aquila',\n",
       " '000 are',\n",
       " '000 as',\n",
       " '000 assists',\n",
       " '000 at',\n",
       " '000 atoka',\n",
       " '000 barrels',\n",
       " '000 bcf',\n",
       " '000 because',\n",
       " '000 believe',\n",
       " '000 bert',\n",
       " '000 bob',\n",
       " '000 bp',\n",
       " '000 but',\n",
       " '000 by',\n",
       " '000 california',\n",
       " '000 calories',\n",
       " '000 capacity',\n",
       " '000 career',\n",
       " '000 ceg',\n",
       " '000 change',\n",
       " '000 check',\n",
       " '000 citing',\n",
       " '000 consisted',\n",
       " '000 construction',\n",
       " '000 consumers',\n",
       " '000 contract',\n",
       " '000 contribute',\n",
       " '000 could',\n",
       " '000 credit',\n",
       " '000 cubic',\n",
       " '000 customers',\n",
       " '000 daily',\n",
       " '000 day',\n",
       " '000 deals',\n",
       " '000 dec',\n",
       " '000 did',\n",
       " '000 difference',\n",
       " '000 down',\n",
       " '000 dt',\n",
       " '000 dth',\n",
       " '000 due',\n",
       " '000 duke',\n",
       " '000 during',\n",
       " '000 dynegy',\n",
       " '000 each',\n",
       " '000 east',\n",
       " '000 effective',\n",
       " '000 efm',\n",
       " '000 el',\n",
       " '000 electric',\n",
       " '000 employees',\n",
       " '000 ena',\n",
       " '000 engi',\n",
       " '000 enron',\n",
       " '000 epmi',\n",
       " '000 feb',\n",
       " '000 feet',\n",
       " '000 flow',\n",
       " '000 for',\n",
       " '000 from',\n",
       " '000 gas',\n",
       " '000 grand',\n",
       " '000 h2o',\n",
       " '000 h2s',\n",
       " '000 has',\n",
       " '000 hit',\n",
       " '000 horsepower',\n",
       " '000 hp',\n",
       " '000 if',\n",
       " '000 ignacio',\n",
       " '000 imbalance',\n",
       " '000 in',\n",
       " '000 including',\n",
       " '000 interconnect',\n",
       " '000 is',\n",
       " '000 jan',\n",
       " '000 kilometers',\n",
       " '000 last',\n",
       " '000 leaf',\n",
       " '000 life',\n",
       " '000 listings',\n",
       " '000 mainline',\n",
       " '000 mar',\n",
       " '000 matching',\n",
       " '000 maxdtq',\n",
       " '000 mbtu',\n",
       " '000 mcf',\n",
       " '000 mcfd',\n",
       " '000 megawatts',\n",
       " '000 mgi',\n",
       " '000 mile',\n",
       " '000 miles',\n",
       " '000 minimum',\n",
       " '000 misc',\n",
       " '000 mmbtu',\n",
       " '000 mmbtus',\n",
       " '000 mmbut',\n",
       " '000 mmcf',\n",
       " '000 month',\n",
       " '000 most',\n",
       " '000 moving',\n",
       " '000 must',\n",
       " '000 mw',\n",
       " '000 mwh',\n",
       " '000 ne',\n",
       " '000 new',\n",
       " '000 no',\n",
       " '000 northwest',\n",
       " '000 nov',\n",
       " '000 obviously',\n",
       " '000 oct',\n",
       " '000 oemt',\n",
       " '000 of',\n",
       " '000 on',\n",
       " '000 or',\n",
       " '000 our',\n",
       " '000 oxy',\n",
       " '000 patients',\n",
       " '000 payable',\n",
       " '000 people',\n",
       " '000 per',\n",
       " '000 pg',\n",
       " '000 please',\n",
       " '000 pma',\n",
       " '000 pnm',\n",
       " '000 population',\n",
       " '000 positive',\n",
       " '000 range',\n",
       " '000 real',\n",
       " '000 records',\n",
       " '000 red',\n",
       " '000 reliant',\n",
       " '000 residential',\n",
       " '000 san',\n",
       " '000 sempra',\n",
       " '000 sj',\n",
       " '000 smud',\n",
       " '000 so',\n",
       " '000 south',\n",
       " '000 southwest',\n",
       " '000 sq',\n",
       " '000 square',\n",
       " '000 subscribed',\n",
       " '000 subtotal',\n",
       " '000 tax',\n",
       " '000 texaco',\n",
       " '000 thank',\n",
       " '000 thanks',\n",
       " '000 that',\n",
       " '000 the',\n",
       " '000 then',\n",
       " '000 these',\n",
       " '000 this',\n",
       " '000 threshold',\n",
       " '000 through',\n",
       " '000 times',\n",
       " '000 to',\n",
       " '000 tons',\n",
       " '000 topock',\n",
       " '000 total',\n",
       " '000 towards',\n",
       " '000 transactions',\n",
       " '000 two',\n",
       " '000 up',\n",
       " '000 usgt',\n",
       " '000 visitors',\n",
       " '000 volumetric',\n",
       " '000 volunteers',\n",
       " '000 was',\n",
       " '000 we',\n",
       " '000 week',\n",
       " '000 west',\n",
       " '000 when',\n",
       " '000 which',\n",
       " '000 will',\n",
       " '000 with',\n",
       " '000 workstations',\n",
       " '000 worth',\n",
       " '000 year',\n",
       " '000 young',\n",
       " '0000',\n",
       " '0000 aep',\n",
       " '0000 cell',\n",
       " '0000 hours',\n",
       " '0000 htm',\n",
       " '0000 mmbtu',\n",
       " '0000 the',\n",
       " '0000 transalta',\n",
       " '000000',\n",
       " '000000 analysis',\n",
       " '000000 font',\n",
       " '000000 history',\n",
       " '000000 span',\n",
       " '000000 update',\n",
       " '00000000',\n",
       " '00000000 message',\n",
       " '00000000 we',\n",
       " '000000000014314',\n",
       " '000000000014314 page',\n",
       " '000000000042084',\n",
       " '000000000042084 page',\n",
       " '000000000048227',\n",
       " '000000000048227 page',\n",
       " '000000000051388',\n",
       " '000000000051388 page',\n",
       " '000000000051388 request',\n",
       " '000000000052681',\n",
       " '000000000052681 page',\n",
       " '000000000053089',\n",
       " '000000000053089 page',\n",
       " '000000000053239',\n",
       " '000000000053239 page',\n",
       " '000000000053239 request',\n",
       " '000000000053327',\n",
       " '000000000053327 approver',\n",
       " '000000000053327 page',\n",
       " '000000000053327 request',\n",
       " '000000000053327 this',\n",
       " '000000000053352',\n",
       " '000000000053352 page',\n",
       " '000000000054493',\n",
       " '000000000054493 page',\n",
       " '000000000054493 request',\n",
       " '000000000055952',\n",
       " '000000000055952 page',\n",
       " '000000000055952 request',\n",
       " '000000000058321',\n",
       " '000000000058321 page',\n",
       " '000000000072188',\n",
       " '000000000072188 page',\n",
       " '000006405005ef831',\n",
       " '000006405005ef831 you',\n",
       " '000006406005ef831',\n",
       " '000006406005ef831 qscr',\n",
       " '000006408005ef831',\n",
       " '000006408005ef831 qscr',\n",
       " '000006409005ef831',\n",
       " '000006409005ef831 goto',\n",
       " '00000640a005ef831',\n",
       " '00000640a005ef831 if',\n",
       " '00000640b005ef831',\n",
       " '00000640b005ef831 qscr',\n",
       " '00000640c005ef831',\n",
       " '00000640c005ef831 thank',\n",
       " '00000640d005ef831',\n",
       " '00000640d005ef831 travel',\n",
       " '00000640e005ef831',\n",
       " '00000640e005ef831 travel',\n",
       " '00000640f005ef831',\n",
       " '00000640f005ef831 golden',\n",
       " '000006812005ef831',\n",
       " '000006812005ef831 your',\n",
       " '000006823005ef831',\n",
       " '000006823005ef831 expedia',\n",
       " '000006823005ef831 looking',\n",
       " '000006823005ef831 travelscape',\n",
       " '000006840005ef831',\n",
       " '000006840005ef831 learn',\n",
       " '000006848005ef831',\n",
       " '000006848005ef831 qscr',\n",
       " '000006884005ef831',\n",
       " '000006884005ef831 see',\n",
       " '000068288',\n",
       " '000068288 for',\n",
       " '0000ff',\n",
       " '0000ff content',\n",
       " '0000ff derivatives',\n",
       " '0000ff early',\n",
       " '0000ff energy',\n",
       " '0000ff in',\n",
       " '0000ff span',\n",
       " '000269',\n",
       " '000269 01',\n",
       " '000369',\n",
       " '000369 02',\n",
       " '0004',\n",
       " '0004 or',\n",
       " '0006509',\n",
       " '0006509 but',\n",
       " '0006510',\n",
       " '0006510 do',\n",
       " '0009214',\n",
       " '0009214 01',\n",
       " '000mmbtu',\n",
       " '000mmbtu down',\n",
       " '001',\n",
       " '001 08',\n",
       " '001 epmi',\n",
       " '001 ht3',\n",
       " '001 ht4',\n",
       " '001 ht5',\n",
       " '001 mmcf',\n",
       " '001 not',\n",
       " '001 pdf',\n",
       " '0011',\n",
       " '0011 the',\n",
       " '0012',\n",
       " '0012 or',\n",
       " '002',\n",
       " '002 15',\n",
       " '002 700',\n",
       " '002 epmi',\n",
       " '002 ht8',\n",
       " '0021',\n",
       " '0021 instead',\n",
       " '0022',\n",
       " '0022 you',\n",
       " '0024',\n",
       " '0024 the',\n",
       " '0025',\n",
       " '0025 25',\n",
       " '0025726',\n",
       " '0025726 tag',\n",
       " '0025777',\n",
       " '0025777 mscgo1',\n",
       " '0028',\n",
       " '0028 mobile',\n",
       " '002_00',\n",
       " '002_00 jpg',\n",
       " '002_bull_001',\n",
       " '002_bull_001 html',\n",
       " '003',\n",
       " '003 below',\n",
       " '003955',\n",
       " '003955 01',\n",
       " '004',\n",
       " '004 epmi',\n",
       " '004 pdf',\n",
       " '004_1a',\n",
       " '004_1a jpg',\n",
       " '004ed7e3',\n",
       " '004ed7e3 in',\n",
       " '005',\n",
       " '005 00',\n",
       " '005 005',\n",
       " '005 015',\n",
       " '005 03',\n",
       " '005 04',\n",
       " '005 07',\n",
       " '005 13',\n",
       " '005 friday',\n",
       " '005 perm',\n",
       " '005 pg',\n",
       " '0055',\n",
       " '0055 high',\n",
       " '005_2a',\n",
       " '005_2a jpg',\n",
       " '006',\n",
       " '006 15',\n",
       " '006 epmi',\n",
       " '0060',\n",
       " '0060 and',\n",
       " '0060 if',\n",
       " '00639c',\n",
       " '00639c img',\n",
       " '006902704',\n",
       " '006902704 customer_code',\n",
       " '007',\n",
       " '007 mwh',\n",
       " '007 usgt',\n",
       " '0070',\n",
       " '0070 to',\n",
       " '0075',\n",
       " '0075 thanks',\n",
       " '0075 to',\n",
       " '008',\n",
       " '008 32',\n",
       " '0088',\n",
       " '0088 marketers',\n",
       " '009',\n",
       " '009 325',\n",
       " '009 alj',\n",
       " '009 please',\n",
       " '0092',\n",
       " '00a',\n",
       " '00a 10',\n",
       " '00a in',\n",
       " '00a this',\n",
       " '00am',\n",
       " '00am 00pm',\n",
       " '00am 02',\n",
       " '00am 11',\n",
       " '00am 12',\n",
       " '00am 2pm',\n",
       " '00am 30pm',\n",
       " '00am amy',\n",
       " '00am and',\n",
       " '00am at',\n",
       " '00am bill',\n",
       " '00am central',\n",
       " '00am chris',\n",
       " '00am cst',\n",
       " '00am deadline',\n",
       " '00am eastern',\n",
       " '00am flowing',\n",
       " '00am for',\n",
       " '00am fyi',\n",
       " '00am geir',\n",
       " '00am greg',\n",
       " '00am houston',\n",
       " '00am if',\n",
       " '00am in',\n",
       " '00am justin',\n",
       " '00am lunch',\n",
       " '00am on',\n",
       " '00am once',\n",
       " '00am please',\n",
       " '00am ryan',\n",
       " '00am to',\n",
       " '00am today',\n",
       " '00am until',\n",
       " '00am we',\n",
       " '00busine',\n",
       " '00busine ssusairwaysmiles',\n",
       " '00dy',\n",
       " '00dy unl',\n",
       " '00family',\n",
       " '00family bostontrail',\n",
       " '00family californiamagic',\n",
       " '00news',\n",
       " '00news pdf',\n",
       " '00p',\n",
       " '00p 00p',\n",
       " '00p eb',\n",
       " '00p in',\n",
       " '00p rsvp',\n",
       " '00p today',\n",
       " '00pdt',\n",
       " '00pdt tag',\n",
       " '00pm',\n",
       " '00pm 00pm',\n",
       " '00pm 12',\n",
       " '00pm 30pm',\n",
       " '00pm 45pm',\n",
       " '00pm amy',\n",
       " '00pm and',\n",
       " '00pm available',\n",
       " '00pm bill',\n",
       " '00pm breakfast',\n",
       " '00pm cara',\n",
       " '00pm central',\n",
       " '00pm chris',\n",
       " '00pm cst',\n",
       " '00pm for',\n",
       " '00pm greg',\n",
       " '00pm https',\n",
       " '00pm hyatt',\n",
       " '00pm if',\n",
       " '00pm in',\n",
       " '00pm jan',\n",
       " '00pm les',\n",
       " '00pm let',\n",
       " '00pm light',\n",
       " '00pm location',\n",
       " '00pm lunch',\n",
       " '00pm mdt',\n",
       " '00pm mt',\n",
       " '00pm on',\n",
       " '00pm or',\n",
       " '00pm please',\n",
       " '00pm pst',\n",
       " '00pm receipt',\n",
       " '00pm richard',\n",
       " '00pm stan',\n",
       " '00pm stewart',\n",
       " '00pm theresa',\n",
       " '00pm thursday',\n",
       " '00pm tim',\n",
       " '00pm to',\n",
       " '00pm today',\n",
       " '00pm todd',\n",
       " '00pm tom',\n",
       " '00s',\n",
       " '00s averaging',\n",
       " '00s per',\n",
       " '00s range',\n",
       " '00usd',\n",
       " '00usd cf',\n",
       " '01',\n",
       " '01 00',\n",
       " '01 000',\n",
       " '01 001',\n",
       " '01 005',\n",
       " '01 00am',\n",
       " '01 01',\n",
       " '01 011',\n",
       " '01 02',\n",
       " '01 025',\n",
       " '01 03',\n",
       " '01 035',\n",
       " '01 04',\n",
       " '01 05',\n",
       " '01 06',\n",
       " '01 07',\n",
       " '01 08',\n",
       " '01 09',\n",
       " '01 10',\n",
       " '01 100',\n",
       " '01 108',\n",
       " '01 11',\n",
       " '01 110',\n",
       " '01 115',\n",
       " '01 12',\n",
       " '01 120',\n",
       " '01 123',\n",
       " '01 135',\n",
       " '01 14',\n",
       " '01 143',\n",
       " '01 15',\n",
       " '01 156',\n",
       " '01 159',\n",
       " '01 15th',\n",
       " '01 16',\n",
       " '01 162',\n",
       " '01 166',\n",
       " '01 168',\n",
       " '01 17',\n",
       " '01 172',\n",
       " '01 18',\n",
       " '01 1821gmt',\n",
       " '01 19',\n",
       " '01 192',\n",
       " '01 199',\n",
       " '01 20',\n",
       " '01 2000',\n",
       " '01 2000060109560313721',\n",
       " '01 2001',\n",
       " '01 2002',\n",
       " '01 2003',\n",
       " '01 204',\n",
       " '01 21',\n",
       " '01 217',\n",
       " '01 22',\n",
       " '01 2220',\n",
       " '01 23',\n",
       " '01 234',\n",
       " '01 238',\n",
       " '01 24',\n",
       " '01 243',\n",
       " '01 25',\n",
       " '01 252',\n",
       " '01 26',\n",
       " '01 268',\n",
       " '01 27',\n",
       " '01 270',\n",
       " '01 27725',\n",
       " '01 27726',\n",
       " '01 27727',\n",
       " '01 27728',\n",
       " '01 27730',\n",
       " '01 27731',\n",
       " '01 27732',\n",
       " '01 27733',\n",
       " '01 27741',\n",
       " '01 288',\n",
       " '01 29',\n",
       " '01 30',\n",
       " '01 31',\n",
       " '01 319',\n",
       " '01 32',\n",
       " '01 336',\n",
       " '01 36',\n",
       " '01 360',\n",
       " '01 365',\n",
       " '01 37',\n",
       " '01 378',\n",
       " '01 39',\n",
       " '01 40',\n",
       " '01 400',\n",
       " '01 41',\n",
       " '01 419',\n",
       " '01 42',\n",
       " '01 43',\n",
       " '01 440',\n",
       " '01 46',\n",
       " ...]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CountV.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#CountV.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('zz http', 271487), ('zz from', 271486), ('zz 3d', 271485), ('zz', 271484), ('zy2qmr lokay', 271483), ('zy2qmr', 271482), ('zum5763n dynegy', 271481), ('zum5763n', 271480), ('zufferli john', 271479), ('zufferli ena', 271478), ('zufferli click', 271477), ('zufferli', 271476), ('zuela steele', 271475), ('zuela eldridge', 271474), ('zuela dasilva', 271473), ('zuela carter', 271472), ('zuela', 271471), ('zrmygmfpvsogfhxglkwr this', 271470), ('zrmygmfpvsogfhxglkwr', 271469), ('zqxc5o southwest', 271468), ('zqxc5o', 271467), ('zqhmteeh experiencing', 271466), ('zqhmteeh', 271465), ('zp26 we', 271464), ('zp26 so', 271463), ('zp26 sale', 271462), ('zp26 np15', 271461), ('zp26', 271460), ('zou am', 271459), ('zou', 271458), ('zottermj hotmail', 271457), ('zottermj', 271456), ('zotter mail', 271455), ('zotter', 271454), ('zoomerang com', 271453), ('zoomerang', 271452), ('zoo is', 271451), ('zoo don', 271450), ('zoo', 271449), ('zoning ordinances', 271448), ('zoning laws', 271447), ('zoning lastly', 271446), ('zoning houston', 271445), ('zoning approval', 271444), ('zoning also', 271443), ('zoning', 271442), ('zones unlimited', 271441), ('zones in', 271440), ('zones and', 271439), ('zones', 271438)]\n"
     ]
    }
   ],
   "source": [
    "two_grams = [(key, CountV.vocabulary_[key]) for key in CountV.vocabulary_]\n",
    "print(sorted(two_grams,reverse=True, key=lambda x: x[1])[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Q2. Named Entity Recognition\n",
    "    Features:\n",
    "            we are intrested in creating a vectorized obejct from our data set, which will take into consideration the\n",
    "            (1) word form the (2)POS of the word (3) ORT, (4) perfix1 ,(5) perfix2, (6) perfix3, (7) suffix1 \n",
    "            (8) suffix2, (9) suffix3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'word'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CountV.get_stop_words()\n",
    "CountV.get_params()['analyzer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 2.1 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
