{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Our import: \n",
    "import nltk\n",
    "from nltk.probability import ConditionalFreqDist\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import math\n",
    "from nltk import word_tokenize\n",
    "import numpy as np\n",
    "from random import randint\n",
    "import csv\n",
    "import numpy\n",
    "import re\n",
    "\n",
    "import sys\n",
    "\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from textblob import TextBlob\n",
    "import pandas\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import StratifiedKFold, cross_val_score, train_test_split \n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.learning_curve import learning_curve\n",
    "\n",
    "\n",
    "##Our defines: \n",
    "q1Verbose=1\n",
    "q113_verbose=0\n",
    "q2Verbose=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. Document Classification\n",
    "#### Q1.1. Reuters Dataset\n",
    "\n",
    "##### Q1.1.1 Turn the code of the Sklearn tutorial above into a notebook.\n",
    "\n",
    "This code is taken from the out of core classification guide given in the assigmnent. \n",
    "http://scikit-learn.org/dev/auto_examples/applications/plot_out_of_core_classification.html#example-applications-plot-out-of-core-classification-py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Notebook attached in folder, in order to avoid clattering. \n",
    "# Taken from import example-application-plot-out-of-core-classification.pt so we could call it's functions\n",
    "from plot_out_of_core_classification import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q1.1.2 Explore how many documents are in the dataset, how many categories, how many documents per categories, provide mean and standard deviation, min and max. (Hint: use the pandas library to explore the dataset, use the dataframe.describe() method.)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Had some trouble with the imports, installed lots of modules, found the problem to be a spelling mistake. \n",
    "Note: installed modules: pandas, request, dict, public, self, get, query_string, post\n",
    "~~Tried to import panda instead of pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of df is:  <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "#From the code, accessing the reuters document data base. \n",
    "data_stream = stream_reuters_documents()\n",
    "\n",
    "#Experimenting with functions given in hint. \n",
    "df = pd.DataFrame(data_stream)\n",
    "print(\"The type of df is: \", type(df))\n",
    "#df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many documents in the dataset? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21578"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['title'].describe()['count']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Output: 21578"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 21,578 as seen below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Some exploring of dataset: \n",
    "if q1Verbose:\n",
    "    print(list(df))\n",
    "    print(df['body'].describe())\n",
    "    # Since df['topics'].descibe() does not work, let's just print. \n",
    "    print(df['topics'])\n",
    "    print(type(df['topics']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Each document can belong to a few topics (we interperted topics for categories). We will feed all information into a FreqDist and receive statistics.\n",
    "\n",
    "As implied, this means a document can appear in several documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create a list of all occurences of all topics and feed to FreqDist. \n",
    "freq_dist = nltk.FreqDist(sum(list(df['topics']), []))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many categories: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of categories is:  445\n"
     ]
    }
   ],
   "source": [
    "category_set=set(sum(list(df['topics']), []))\n",
    "num_of_categories = len(category_set)\n",
    "print(\"The number of categories is: \",num_of_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output: \"The number of categories is:  445\"\n",
    "Since we are using a Frequency distribution, the number of categories can also be retrieved by: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "445"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(freq_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many documents per category: \n",
    "Since we are using a Frequency distribution, the number of documents per category \n",
    "is the value in freq_dist[category]. \n",
    "\n",
    "We can print a list of all categories and the number of documents in them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category:  naphtha has  7 Docs\n",
      "Category:  uganda has  17 Docs\n",
      "Category:  brunei has  6 Docs\n",
      "Category:  coconut has  6 Docs\n",
      "Category:  oapec has  2 Docs\n",
      "Category:  austdlr has  4 Docs\n",
      "Category:  antigua has  1 Docs\n",
      "Category:  gnp has  163 Docs\n",
      "Category:  palm-oil has  43 Docs\n",
      "Category:  lit has  3 Docs\n"
     ]
    }
   ],
   "source": [
    "#Create a list of (Category, Number of doc's)\n",
    "cat_numOfDocs = [(category, freq_dist[category]) for category in category_set]\n",
    "for pair in cat_numOfDocs[:10]: \n",
    "    print('Category: ', pair[0], 'has ', pair[1], 'Docs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:\n",
    "Category:  naphtha has  7 Docs\n",
    "Category:  uganda has  17 Docs\n",
    "Category:  brunei has  6 Docs\n",
    "Category:  coconut has  6 Docs\n",
    "Category:  oapec has  2 Docs\n",
    "Category:  austdlr has  4 Docs\n",
    "Category:  antigua has  1 Docs\n",
    "Category:  gnp has  163 Docs\n",
    "Category:  palm-oil has  43 Docs\n",
    "Category:  lit has  3 Docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Provide mean and standard deviation, min and max. \n",
    "Mean: Mean number of documents per categorie. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not for the last time during this assignment, We didn't find the \"leads\" in the attached notebooks, \n",
    "ended up working hard, and at a later stage, noticing the lead when researching a different problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    21578.000000\n",
       "mean       718.527621\n",
       "std        819.414178\n",
       "min          0.000000\n",
       "25%        235.000000\n",
       "50%        491.000000\n",
       "75%        904.000000\n",
       "max       9875.000000\n",
       "Name: length, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Obviously We first went the wrong way and worked very hard. We will first present the elegant way. \n",
    "#Simply add a length column, and describe it. \n",
    "df['length'] = df['body'].map(lambda text: len(text))\n",
    "df['length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Mean (Average) number of documents per category is:  89.87191011235954\n",
      "The category with maximum documents is: \" usa \"which has  12542  documents.\n",
      "The category with minimum documents are: ['antigua', 'bhutan', 'corn-oil'] who have 1 documents each. \n",
      "The standard deviation in number of documents per category is: 643.9321684195975\n"
     ]
    }
   ],
   "source": [
    "#Some insight into the actual process: \n",
    "\n",
    "#Mean: \n",
    "#Sum of number of documents per each category. \n",
    "sum_docs_cat = sum(num_of_docs for (cat, num_of_docs) in cat_numOfDocs)\n",
    "#\n",
    "#Mean expected number of documents per categorie. \n",
    "#mean_exp=sum(freq_dist.freq(cat)*num_of_docs for (cat, num_of_docs) in cat_numOfDocs)\n",
    "#mean2 = np.mean([num_of_docs for _, num_of_docs in cat_numOfDocs])\n",
    "#print('The Mean number of documents per categorie is: ', mean_exp)\n",
    "#print('The Mean2 number of documents per categorie is: ', mean2)\n",
    "mean = sum_docs_cat/len(cat_numOfDocs)\n",
    "print('The Mean (Average) number of documents per category is: ', mean)\n",
    "\n",
    "#Max:\n",
    "print('The category with maximum documents is: \"',freq_dist.max(), '\"which has ', freq_dist[freq_dist.max()], ' documents.')\n",
    "\n",
    "#Min:\n",
    "min_num_of_docs = sorted(cat_numOfDocs ,key=lambda x: x[1])[0][1]\n",
    "cats_w_min_num_of_docs = [cat for (cat, num_of_docs) in cat_numOfDocs if num_of_docs==min_num_of_docs]\n",
    "display = 3 #Display only part of categories, not all. \n",
    "print('The category with minimum documents are:',cats_w_min_num_of_docs[:display], 'who have', min_num_of_docs, 'documents each. ' )\n",
    "\n",
    "#Standard deviation: \n",
    "std_dev = math.sqrt(sum( (math.pow(num_of_docs-mean, 2) for (_, num_of_docs) in cat_numOfDocs))/len(cat_numOfDocs))\n",
    "print('The standard deviation in number of documents per category is:', std_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Mean (Average) number of documents per category is:  89.87191011235954\n",
    "The category with maximum documents is: \" usa \"which has  12542  documents.\n",
    "The category with minimum documents are: ['antigua', 'bhutan', 'corn-oil'] who have 1 documents each. \n",
    "The standard deviation in number of documents per category is: 643.9321684195975"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1.1.3 Explore how many characters and words are present in the documents of the dataset.\n",
    "\n",
    "First we consider all diffferent word tokens and characters in code, as in a set of elements. We then calculate \n",
    "the number of all tokens and characters all together, which is more relevent to our issues. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create sets of words and characters. \n",
    "#Takes a while to run, use with care :)\n",
    "if q113_verbose: \n",
    "    word_set=set()\n",
    "    word_list=[]\n",
    "    for i in range(len(df['body'])): \n",
    "        word_set.update(word_tokenize(df['body'][i]))\n",
    "        word_list += word_tokenize(df['body'][i])\n",
    "\n",
    "    char_set=set()\n",
    "    char_list=[]\n",
    "    for word in word_set: \n",
    "        for letter in word: \n",
    "            char_set.update(letter)\n",
    "            char_list += letter\n",
    "    print('There are %d different words in all documents. ' %len(word_set))\n",
    "    print('There are %d word tokens in all documents. ' %len(word_list))\n",
    "    print('There are %d different characters in all documents. ' %len(char_set))\n",
    "    print('There are %d characters in all documents. ' %len(char_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "Since runtime is long for the two above boxes, Output given here, no need to run. \n",
    "\n",
    "Output: \n",
    "There are 76886 different words in all documents. \n",
    "There are 2854622 word tokens in all documents. \n",
    "There are 89 different characters in all documents. \n",
    "There are 568599 characters in all documents. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now construct a dictionary, That maps from article index to {num_of_words: , num_of_chars: }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "article_2words_chars = {}\n",
    "for i in range(len(df['body'])): \n",
    "    article_2words_chars[i] = (len(word_tokenize(df['body'][i])), len(df['body'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def explore_doc(x):\n",
    "    print('Document with index %d has %d words and %d letters' % (x, article_2words_chars[x][0], article_2words_chars[x][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document with index 1051 has 111 words and 572 letters\n",
      "Document with index 7507 has 444 words and 2593 letters\n",
      "Document with index 18162 has 299 words and 1667 letters\n",
      "Document with index 8530 has 102 words and 533 letters\n",
      "Document with index 12794 has 25 words and 135 letters\n"
     ]
    }
   ],
   "source": [
    "#Do some exploring: (We'll print statistic's for some random doc's.)\n",
    "i=0\n",
    "while i<5: \n",
    "    x = randint(0,len(df['body']))\n",
    "    explore_doc(x)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Document with index 1051 has 111 words and 572 letters\n",
    "Document with index 7507 has 444 words and 2593 letters\n",
    "Document with index 18162 has 299 words and 1667 letters\n",
    "Document with index 8530 has 102 words and 533 letters\n",
    "Document with index 12794 has 25 words and 135 letters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1.1.4 Explain informally what are the classifiers that support the \"partial-fit\" method discussed in the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Informally, the classifiers that support \"partial-fit\", are classifiers who do not need to \"hold\" all the\n",
    "information they are given, at every given moment. If we attempt a slightly more formal explanation, We \n",
    "can say that the state of the classifier is changed as it learns from more inputs, yet this input is not\n",
    "a state variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1.1.5 Explain what is the hashing vectorizer used in this tutorial.\n",
    "####            Why is it important to use this vectorizer to achieve \"streaming classification\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hashing Vectorizer used in this tutorial is a tool for counting words as features, with a few tweaks \n",
    "from the \"usual\" count vectors. It is first given a size (in our case, 2^18, ~num of words), and later, \n",
    "while meeting words, it will add them to the appropriate cell, using a hash function. \n",
    "\n",
    "By using the hashing vectorizer, We achieve two main advantages. \n",
    "1) Hashing strings into integers results in imporved memory usage. \n",
    "2) Our feature space remains the same over different batches. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Q1.2 Spam Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy\n",
    "from pandas import DataFrame\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "NEWLINE = '\\n'\n",
    "\n",
    "HAM = 'ham'\n",
    "SPAM = 'spam'\n",
    "\n",
    "SOURCES = [\n",
    "    ('data/spam',        SPAM),\n",
    "    ('data/easy_ham',    HAM),\n",
    "    ('data/hard_ham',    HAM),\n",
    "    ('data/beck-s',      HAM),\n",
    "    ('data/farmer-d',    HAM),\n",
    "    ('data/kaminski-v',  HAM),\n",
    "    ('data/kitchen-l',   HAM),\n",
    "    ('data/lokay-m',     HAM),\n",
    "    ('data/williams-w3', HAM),\n",
    "    ('data/BG',          SPAM),\n",
    "    ('data/GP',          SPAM),\n",
    "    ('data/SH',          SPAM)\n",
    "]\n",
    "\n",
    "SKIP_FILES = {'cmds'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def read_files(path):\n",
    "    for root, dir_names, file_names in os.walk(path):\n",
    "        for path in dir_names:\n",
    "            read_files(os.path.join(root, path))\n",
    "        for file_name in file_names:\n",
    "            if file_name not in SKIP_FILES:\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                if os.path.isfile(file_path):\n",
    "                    past_header, lines = False, []\n",
    "                    f = open(file_path, encoding=\"latin-1\")\n",
    "                    for line in f:\n",
    "                        if past_header:\n",
    "                            lines.append(line)\n",
    "                        elif line == NEWLINE:\n",
    "                            past_header = True\n",
    "                    f.close()\n",
    "                    content = NEWLINE.join(lines)\n",
    "                    yield file_path, content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_data_frame(path, classification):\n",
    "    rows = []\n",
    "    index = []\n",
    "    for file_name, text in read_files(path):\n",
    "        rows.append({'text': text, 'class': classification})\n",
    "        index.append(file_name)\n",
    "\n",
    "    data_frame = DataFrame(rows, index=index)\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = DataFrame({'text': [], 'class': []})\n",
    "for path, classification in SOURCES:\n",
    "    data = data.append(build_data_frame(path, classification))\n",
    "\n",
    "data = data.reindex(numpy.random.permutation(data.index))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('count_vectorizer',   CountVectorizer(ngram_range=(1, 2))),\n",
    "    ('classifier',         MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k_fold = KFold(n=len(data), n_folds=6)\n",
    "scores = []\n",
    "confusion = numpy.array([[0, 0], [0, 0]])\n",
    "for train_indices, test_indices in k_fold:\n",
    "    train_text = data.iloc[train_indices]['text'].values\n",
    "    train_y = data.iloc[train_indices]['class'].values.astype(str)\n",
    "\n",
    "    test_text = data.iloc[test_indices]['text'].values\n",
    "    test_y = data.iloc[test_indices]['class'].values.astype(str)\n",
    "\n",
    "    pipeline.fit(train_text, train_y)\n",
    "    predictions = pipeline.predict(test_text)\n",
    "\n",
    "    confusion += confusion_matrix(test_y, predictions)\n",
    "    score = f1_score(test_y, predictions, pos_label=SPAM)\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total emails classified: 58910\n",
      "Score: 0.978251240858\n",
      "Confusion matrix:\n",
      "[[23459    80]\n",
      " [ 1429 33942]]\n"
     ]
    }
   ],
   "source": [
    "print('Total emails classified:', len(data))\n",
    "print('Score:', sum(scores)/len(scores))\n",
    "print('Confusion matrix:')\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total emails classified: 58910\n",
    "Score: 0.978251240858\n",
    "Confusion matrix:\n",
    "[[23459    80]\n",
    " [ 1429 33942]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Some experiments from our first attempts. \n",
    "# data\n",
    "# print(len(data['text']))\n",
    "# data.groupby('class').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1.2.1 The vectorizer used in Zac Stewart's code is a CountVectorizer with unigrams and bigrams. Report the number of unigrams and bigrams used in this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3571898 unigrams and bigrams, used in this model. \n"
     ]
    }
   ],
   "source": [
    "# Retreive the count vectorizer used in the model. \n",
    "p=pipeline.get_params()\n",
    "CountV=p['count_vectorizer']\n",
    "#Access features: \n",
    "uni_bi_grams = CountV.get_feature_names()\n",
    "print(\"There are %d unigrams and bigrams, used in this model. \" %len(uni_bi_grams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Q1.2.2 What are the 50 most frequent unigrams and bigrams in the dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After long research, trial and error, and lots of stack overflow reading, We have this. \n",
    "Following the sturcture of the pipeline, as well as it's components, proved uncomfortable and \n",
    "counter intuitive, possibly on account of the amount of new information and functions. \n",
    "The structure follows a logic, and items are sorted in the same order, but can have no \n",
    "corresponding fields. For example, as seen below, we get the feature names from the count vector, \n",
    "but the feature count from the classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('font', 496308.0)\n",
      "('3d', 477648.0)\n",
      "('the', 440039.0)\n",
      "('br', 422254.0)\n",
      "('td', 345149.0)\n",
      "('to', 322600.0)\n",
      "('and', 256449.0)\n",
      "('of', 231818.0)\n",
      "('nbsp', 228765.0)\n",
      "('http', 203133.0)\n"
     ]
    }
   ],
   "source": [
    "#We get the feature names from from the count vector as a list, and lists for feature count's, \n",
    "#with the index (0/1) specifing class. Zipping them together gives us a list of (feature, count in spam, count in ham)\n",
    "def most_freq_feat(classifier, count_vector, n=50):\n",
    "    index = 0\n",
    "    features_c1_c2_count = []\n",
    "\n",
    "    for feat, c1, c2 in zip(count_vector.get_feature_names(), classifier.feature_count_[0], classifier.feature_count_[1]):\n",
    "        features_c1_c2_count.append((feat, c1 + c2))\n",
    "        index+=1\n",
    "\n",
    "    for i in sorted(features_c1_c2_count, key = lambda x: x[1], reverse=True)[:n]:     \n",
    "        print(i)\n",
    "    \n",
    "\n",
    "    \n",
    "most_freq_feat(p['classifier'], p['count_vectorizer'], n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are the 50 most frequent unigrams and bigrams per class (ham and spam)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 most occurring features in class spam: \n",
      "('the', 228461.0, 211578.0)\n",
      "('to', 155374.0, 167226.0)\n",
      "('and', 104175.0, 152274.0)\n",
      "('of', 97814.0, 134004.0)\n",
      "('in', 76007.0, 97121.0)\n",
      "('com', 59545.0, 102710.0)\n",
      "('for', 58849.0, 68774.0)\n",
      "('enron', 57904.0, 3.0)\n",
      "('is', 49909.0, 73549.0)\n",
      "('on', 49315.0, 35525.0)\n",
      "('http', 49090.0, 154043.0)\n",
      "('that', 46778.0, 40383.0)\n",
      "('you', 42453.0, 90704.0)\n",
      "('td', 37809.0, 307340.0)\n",
      "('this', 36165.0, 70785.0)\n",
      "('it', 34585.0, 32132.0)\n",
      "('with', 33303.0, 38826.0)\n",
      "('be', 32529.0, 38583.0)\n",
      "('ect', 31955.0, 117.0)\n",
      "('20', 30188.0, 158033.0)\n",
      "('width', 29023.0, 134549.0)\n",
      "('have', 28261.0, 27348.0)\n",
      "('from', 27854.0, 35797.0)\n",
      "('www', 27705.0, 71213.0)\n",
      "('will', 27625.0, 25286.0)\n",
      "('as', 27477.0, 32776.0)\n",
      "('we', 27189.0, 35266.0)\n",
      "('at', 26653.0, 21809.0)\n",
      "('http www', 26154.0, 63559.0)\n",
      "('by', 24478.0, 22986.0)\n",
      "('are', 24394.0, 33775.0)\n",
      "('3d', 24097.0, 453551.0)\n",
      "('of the', 22285.0, 23869.0)\n",
      "('font', 21956.0, 474352.0)\n",
      "('or', 20949.0, 41556.0)\n",
      "('tr', 19165.0, 171177.0)\n",
      "('if', 18846.0, 20276.0)\n",
      "('not', 18503.0, 28425.0)\n",
      "('in the', 18200.0, 19452.0)\n",
      "('your', 18147.0, 62630.0)\n",
      "('br', 17243.0, 405011.0)\n",
      "('09', 16493.0, 8327.0)\n",
      "('height', 16276.0, 84761.0)\n",
      "('src', 15728.0, 42217.0)\n",
      "('gif', 15573.0, 31730.0)\n",
      "('an', 15558.0, 20137.0)\n",
      "('img', 15513.0, 44434.0)\n",
      "('hou', 15114.0, 1859.0)\n",
      "('has', 14987.0, 14681.0)\n",
      "('10', 14958.0, 17121.0)\n",
      "50 most occurring features in class ham: \n",
      "('font', 21956.0, 474352.0)\n",
      "('3d', 24097.0, 453551.0)\n",
      "('br', 17243.0, 405011.0)\n",
      "('td', 37809.0, 307340.0)\n",
      "('nbsp', 8072.0, 220693.0)\n",
      "('the', 228461.0, 211578.0)\n",
      "('size', 10499.0, 179557.0)\n",
      "('tr', 19165.0, 171177.0)\n",
      "('to', 155374.0, 167226.0)\n",
      "('20', 30188.0, 158033.0)\n",
      "('http', 49090.0, 154043.0)\n",
      "('and', 104175.0, 152274.0)\n",
      "('width', 29023.0, 134549.0)\n",
      "('of', 97814.0, 134004.0)\n",
      "('br br', 1173.0, 129623.0)\n",
      "('nbsp nbsp', 1944.0, 123481.0)\n",
      "('face', 7433.0, 121165.0)\n",
      "('style', 2919.0, 109149.0)\n",
      "('span', 968.0, 105849.0)\n",
      "('align', 3430.0, 103625.0)\n",
      "('border', 12265.0, 103592.0)\n",
      "('com', 59545.0, 102710.0)\n",
      "('in', 76007.0, 97121.0)\n",
      "('color', 4859.0, 96178.0)\n",
      "('html', 6217.0, 94434.0)\n",
      "('font size', 3318.0, 91970.0)\n",
      "('you', 42453.0, 90704.0)\n",
      "('href', 13826.0, 89630.0)\n",
      "('height', 16276.0, 84761.0)\n",
      "('td tr', 8623.0, 82513.0)\n",
      "('font face', 5871.0, 80826.0)\n",
      "('content', 2404.0, 74778.0)\n",
      "('td td', 8285.0, 74165.0)\n",
      "('is', 49909.0, 73549.0)\n",
      "('www', 27705.0, 71213.0)\n",
      "('this', 36165.0, 70785.0)\n",
      "('div', 1543.0, 70121.0)\n",
      "('tr td', 6459.0, 69915.0)\n",
      "('for', 58849.0, 68774.0)\n",
      "('3d http', 4550.0, 65769.0)\n",
      "('http www', 26154.0, 63559.0)\n",
      "('style 3d', 360.0, 63308.0)\n",
      "('arial', 5552.0, 62961.0)\n",
      "('your', 18147.0, 62630.0)\n",
      "('table', 11058.0, 62371.0)\n",
      "('center', 3200.0, 58640.0)\n",
      "('text', 3000.0, 53398.0)\n",
      "('font td', 3791.0, 53000.0)\n",
      "('tr tr', 3578.0, 52250.0)\n",
      "('href 3d', 1499.0, 50821.0)\n"
     ]
    }
   ],
   "source": [
    "#Create a list of feature name and amount of occurrences in each class. \n",
    "#Sort according to different class counter to get occurrences per class. \n",
    "def most_occurring_feat_per_class(classifier, count_vector, n=50):\n",
    "    index = 0\n",
    "    features_c1_c2_count = []\n",
    "\n",
    "    for feat, c1, c2 in zip(count_vector.get_feature_names(), classifier.feature_count_[0], classifier.feature_count_[1]):\n",
    "        features_c1_c2_count.append((feat, c1, c2))\n",
    "        index+=1\n",
    "\n",
    "    print(\"%d most occurring features in class spam: \" %n )    \n",
    "    for i in sorted(features_c1_c2_count, key = lambda x: x[1], reverse=True)[:n]:     \n",
    "        print(i)\n",
    "\n",
    "    print(\"%d most occurring features in class ham: \" %n )    \n",
    "    for i in sorted(features_c1_c2_count, key = lambda x: x[2], reverse=True)[:n]:     \n",
    "        print(i)\n",
    "\n",
    "\n",
    "    \n",
    "most_occurring_feat_per_class(p['classifier'], p['count_vectorizer'], n=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output: \n",
    "    \n",
    "    \n",
    "50 most occurring features in class spam: \n",
    "('the', 228461.0, 211578.0)\n",
    "('to', 155374.0, 167226.0)\n",
    "('and', 104175.0, 152274.0)\n",
    "('of', 97814.0, 134004.0)\n",
    "('in', 76007.0, 97121.0)\n",
    "('com', 59545.0, 102710.0)\n",
    "('for', 58849.0, 68774.0)\n",
    "('enron', 57904.0, 3.0)\n",
    "('is', 49909.0, 73549.0)\n",
    "('on', 49315.0, 35525.0)\n",
    "('http', 49090.0, 154043.0)\n",
    "('that', 46778.0, 40383.0)\n",
    "('you', 42453.0, 90704.0)\n",
    "('td', 37809.0, 307340.0)\n",
    "('this', 36165.0, 70785.0)\n",
    "('it', 34585.0, 32132.0)\n",
    "('with', 33303.0, 38826.0)\n",
    "('be', 32529.0, 38583.0)\n",
    "('ect', 31955.0, 117.0)\n",
    "('20', 30188.0, 158033.0)\n",
    "('width', 29023.0, 134549.0)\n",
    "('have', 28261.0, 27348.0)\n",
    "('from', 27854.0, 35797.0)\n",
    "('www', 27705.0, 71213.0)\n",
    "('will', 27625.0, 25286.0)\n",
    "('as', 27477.0, 32776.0)\n",
    "('we', 27189.0, 35266.0)\n",
    "('at', 26653.0, 21809.0)\n",
    "('http www', 26154.0, 63559.0)\n",
    "('by', 24478.0, 22986.0)\n",
    "('are', 24394.0, 33775.0)\n",
    "('3d', 24097.0, 453551.0)\n",
    "('of the', 22285.0, 23869.0)\n",
    "('font', 21956.0, 474352.0)\n",
    "('or', 20949.0, 41556.0)\n",
    "('tr', 19165.0, 171177.0)\n",
    "('if', 18846.0, 20276.0)\n",
    "('not', 18503.0, 28425.0)\n",
    "('in the', 18200.0, 19452.0)\n",
    "('your', 18147.0, 62630.0)\n",
    "('br', 17243.0, 405011.0)\n",
    "('09', 16493.0, 8327.0)\n",
    "('height', 16276.0, 84761.0)\n",
    "('src', 15728.0, 42217.0)\n",
    "('gif', 15573.0, 31730.0)\n",
    "('an', 15558.0, 20137.0)\n",
    "('img', 15513.0, 44434.0)\n",
    "('hou', 15114.0, 1859.0)\n",
    "('has', 14987.0, 14681.0)\n",
    "('10', 14958.0, 17121.0)\n",
    "50 most occurring features in class ham: \n",
    "('font', 21956.0, 474352.0)\n",
    "('3d', 24097.0, 453551.0)\n",
    "('br', 17243.0, 405011.0)\n",
    "('td', 37809.0, 307340.0)\n",
    "('nbsp', 8072.0, 220693.0)\n",
    "('the', 228461.0, 211578.0)\n",
    "('size', 10499.0, 179557.0)\n",
    "('tr', 19165.0, 171177.0)\n",
    "('to', 155374.0, 167226.0)\n",
    "('20', 30188.0, 158033.0)\n",
    "('http', 49090.0, 154043.0)\n",
    "('and', 104175.0, 152274.0)\n",
    "('width', 29023.0, 134549.0)\n",
    "('of', 97814.0, 134004.0)\n",
    "('br br', 1173.0, 129623.0)\n",
    "('nbsp nbsp', 1944.0, 123481.0)\n",
    "('face', 7433.0, 121165.0)\n",
    "('style', 2919.0, 109149.0)\n",
    "('span', 968.0, 105849.0)\n",
    "('align', 3430.0, 103625.0)\n",
    "('border', 12265.0, 103592.0)\n",
    "('com', 59545.0, 102710.0)\n",
    "('in', 76007.0, 97121.0)\n",
    "('color', 4859.0, 96178.0)\n",
    "('html', 6217.0, 94434.0)\n",
    "('font size', 3318.0, 91970.0)\n",
    "('you', 42453.0, 90704.0)\n",
    "('href', 13826.0, 89630.0)\n",
    "('height', 16276.0, 84761.0)\n",
    "('td tr', 8623.0, 82513.0)\n",
    "('font face', 5871.0, 80826.0)\n",
    "('content', 2404.0, 74778.0)\n",
    "('td td', 8285.0, 74165.0)\n",
    "('is', 49909.0, 73549.0)\n",
    "('www', 27705.0, 71213.0)\n",
    "('this', 36165.0, 70785.0)\n",
    "('div', 1543.0, 70121.0)\n",
    "('tr td', 6459.0, 69915.0)\n",
    "('for', 58849.0, 68774.0)\n",
    "('3d http', 4550.0, 65769.0)\n",
    "('http www', 26154.0, 63559.0)\n",
    "('style 3d', 360.0, 63308.0)\n",
    "('arial', 5552.0, 62961.0)\n",
    "('your', 18147.0, 62630.0)\n",
    "('table', 11058.0, 62371.0)\n",
    "('center', 3200.0, 58640.0)\n",
    "('text', 3000.0, 53398.0)\n",
    "('font td', 3791.0, 53000.0)\n",
    "('tr tr', 3578.0, 52250.0)\n",
    "('href 3d', 1499.0, 50821.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Q1.2.4 List the 20 most useful features in the Naive Bayes classifier to distinguish between spam and ham (20 features for each class). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham -17.3273410308 00 005\n",
      "ham -17.3273410308 00 00am\n",
      "ham -17.3273410308 00 00pm\n",
      "ham -17.3273410308 00 016\n",
      "ham -17.3273410308 00 025\n",
      "ham -17.3273410308 00 02pm\n",
      "ham -17.3273410308 00 0392a0\n",
      "ham -17.3273410308 00 048\n",
      "ham -17.3273410308 00 04pm\n",
      "ham -17.3273410308 00 0700\n",
      "ham -17.3273410308 00 076\n",
      "ham -17.3273410308 00 079\n",
      "ham -17.3273410308 00 09may\n",
      "ham -17.3273410308 00 09withdrawal\n",
      "ham -17.3273410308 00 104\n",
      "ham -17.3273410308 00 1104\n",
      "ham -17.3273410308 00 113\n",
      "ham -17.3273410308 00 118\n",
      "ham -17.3273410308 00 1200\n",
      "ham -17.3273410308 00 1206\n",
      "\n",
      "spam -4.25763398155 font\n",
      "spam -4.30247582511 3d\n",
      "spam -4.41566905552 br\n",
      "spam -4.69162787147 td\n",
      "spam -5.02280862515 nbsp\n",
      "spam -5.06498730061 the\n",
      "spam -5.2290874763 size\n",
      "spam -5.27688180104 tr\n",
      "spam -5.30023358095 to\n",
      "spam -5.35677555206 20\n",
      "spam -5.38234747592 http\n",
      "spam -5.39389765509 and\n",
      "spam -5.51764987464 width\n",
      "spam -5.52170863913 of\n",
      "spam -5.55494779986 br br\n",
      "spam -5.60349035536 nbsp nbsp\n",
      "spam -5.62242424559 face\n",
      "spam -5.72686266882 style\n",
      "spam -5.75756275423 span\n",
      "spam -5.77879748822 align\n"
     ]
    }
   ],
   "source": [
    "#Since each features coefficient links it to it's class, and smaller coefficients classify spam and larger ham, \n",
    "#we sort according to coefficient, once normaly and once reversed, to get most informative features. \n",
    "def most_informative_feature_for_binary_classification(vectorizer, classifier, n=20):\n",
    "    class_labels = classifier.classes_\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    topn_class1 = sorted(zip(classifier.coef_[0], feature_names))[:n]\n",
    "    topn_class2 = sorted(zip(classifier.coef_[0], feature_names))[-n:]\n",
    "    \n",
    "    counter=0\n",
    "    for coef, feat in topn_class1:\n",
    "        print(class_labels[0], coef, feat)\n",
    "        if counter==20: \n",
    "            break\n",
    "    print()\n",
    "    \n",
    "    counter=0\n",
    "    for coef, feat in reversed(topn_class2):\n",
    "        print(class_labels[1], coef, feat)\n",
    "        counter+=1\n",
    "        if counter==n: \n",
    "            break\n",
    "\n",
    "most_informative_feature_for_binary_classification(p['count_vectorizer'], p['classifier'], n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output: \n",
    "    \n",
    "ham -17.3273410308 00 005\n",
    "ham -17.3273410308 00 00am\n",
    "ham -17.3273410308 00 00pm\n",
    "ham -17.3273410308 00 016\n",
    "ham -17.3273410308 00 025\n",
    "ham -17.3273410308 00 02pm\n",
    "ham -17.3273410308 00 0392a0\n",
    "ham -17.3273410308 00 048\n",
    "ham -17.3273410308 00 04pm\n",
    "ham -17.3273410308 00 0700\n",
    "ham -17.3273410308 00 076\n",
    "ham -17.3273410308 00 079\n",
    "ham -17.3273410308 00 09may\n",
    "ham -17.3273410308 00 09withdrawal\n",
    "ham -17.3273410308 00 104\n",
    "ham -17.3273410308 00 1104\n",
    "ham -17.3273410308 00 113\n",
    "ham -17.3273410308 00 118\n",
    "ham -17.3273410308 00 1200\n",
    "ham -17.3273410308 00 1206\n",
    "\n",
    "spam -4.25763398155 font\n",
    "spam -4.30247582511 3d\n",
    "spam -4.41566905552 br\n",
    "spam -4.69162787147 td\n",
    "spam -5.02280862515 nbsp\n",
    "spam -5.06498730061 the\n",
    "spam -5.2290874763 size\n",
    "spam -5.27688180104 tr\n",
    "spam -5.30023358095 to\n",
    "spam -5.35677555206 20\n",
    "spam -5.38234747592 http\n",
    "spam -5.39389765509 and\n",
    "spam -5.51764987464 width\n",
    "spam -5.52170863913 of\n",
    "spam -5.55494779986 br br\n",
    "spam -5.60349035536 nbsp nbsp\n",
    "spam -5.62242424559 face\n",
    "spam -5.72686266882 style\n",
    "spam -5.75756275423 span\n",
    "spam -5.77879748822 align"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1.2.5 There seems to be an imbalance in the length of spam and ham messages (see the plot in the attached notebook). We want to add a feature based on the number of words in the message in the text representation. Should the length attribute be normalized before fitting the Naive Bayes classifier? (See Sklearn pre-processing for examples.) Do you expect Logistic Regression to perform better with the new feature? Explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we've created a length feature extractor with the fit and transform functions.  \n",
    "from sklearn.base import BaseEstimator, TransformerMixin    \n",
    "class TextStats(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'length': len(text)} for text in posts]\n",
    "\n",
    "#        len(nltk.tokenize.word_tokenize(text))\n",
    "#        return [{'length': len(text)} for text in posts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#A new pipeline, adding theh length as a feature with normalization. \n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn import preprocessing\n",
    "pipeline_w_l = Pipeline([\n",
    "  ('features', FeatureUnion([\n",
    "        ('body_stats', Pipeline([\n",
    "                ('stats', TextStats()),  # returns a list of dicts\n",
    "                ('vect', DictVectorizer()),  # list of dicts -> feature matrix\n",
    "                ('scale', preprocessing.Normalizer()),              \n",
    "\n",
    "                                ])),\n",
    "        ('count_vectorizer',   CountVectorizer(ngram_range=(1, 2))),   \n",
    "                            ])),\n",
    "  ('classifier', MultinomialNB())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total emails classified: 58910\n",
      "Score: 0.978131441426\n",
      "Confusion matrix:\n",
      "[[23462    77]\n",
      " [ 1440 33931]]\n"
     ]
    }
   ],
   "source": [
    "k_fold = KFold(n=len(data), n_folds=6)\n",
    "scores = []\n",
    "confusion = numpy.array([[0, 0], [0, 0]])\n",
    "for train_indices, test_indices in k_fold:\n",
    "    train_text = data.iloc[train_indices]['text'].values\n",
    "    train_y = data.iloc[train_indices]['class'].values.astype(str)\n",
    "\n",
    "    test_text = data.iloc[test_indices]['text'].values\n",
    "    test_y = data.iloc[test_indices]['class'].values.astype(str)\n",
    "\n",
    "    pipeline_w_l.fit(train_text, train_y)\n",
    "    predictions = pipeline_w_l.predict(test_text)\n",
    "\n",
    "    confusion += confusion_matrix(test_y, predictions)\n",
    "    score = f1_score(test_y, predictions, pos_label=SPAM)\n",
    "    scores.append(score)\n",
    "    \n",
    "\n",
    "print('Total emails classified:', len(data))\n",
    "print('Score:', sum(scores)/len(scores))\n",
    "print('Confusion matrix:')\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since each run takes ~Iternity (give or take), the results of the above run, with length added as a feature and\n",
    "normalizing using the preproccessing unit, and the multinomialNB, can be found here: \n",
    "\n",
    "Total emails classified: 58910\n",
    "Score: 0.978131441426\n",
    "Confusion matrix:\n",
    "[[23462    77]\n",
    " [ 1440 33931]]\n",
    " \n",
    "We can see that even though the overall score decreased slightly, the number of false spams decreased substantialy. \n",
    "Obviously, this feature does improve our ability to recognize ham (though confuses us more with spam).  \n",
    "\n",
    "Compared to the original classifier from zac's notebook:\n",
    "\n",
    "Total emails classified: 58910\n",
    "Score: 0.978284726388\n",
    "Confusion matrix:\n",
    "[[23450    89]\n",
    " [ 1418 33953]]\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "pipeline_w_l_lr = Pipeline([\n",
    "        ('features', FeatureUnion([\n",
    "            ('body_stats', Pipeline([\n",
    "                ('stats', TextStats()),  # returns a list of dicts\n",
    "                ('vect', DictVectorizer()),  # list of dicts -> feature matrix\n",
    "                ('scale', preprocessing.Normalizer()),              \n",
    "                                ])),\n",
    "        ('count_vectorizer',   CountVectorizer(ngram_range=(1, 2))),   \n",
    "                            ])),\n",
    "        ('classifier',         LogisticRegression())\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 4 folds\n",
      "Training for fold 0\n",
      "Testing for fold 0\n",
      "Score for 0: 0.99\n",
      "Confusion matrix for 0: \n",
      "[[5849   57]\n",
      " [ 200 8622]]\n",
      "Training for fold 1\n",
      "Testing for fold 1\n",
      "Score for 1: 0.99\n",
      "Confusion matrix for 1: \n",
      "[[11671    96]\n",
      " [  397 17292]]\n",
      "Training for fold 2\n",
      "Testing for fold 2\n",
      "Score for 2: 0.98\n",
      "Confusion matrix for 2: \n",
      "[[17417   133]\n",
      " [  643 25990]]\n",
      "Training for fold 3\n",
      "Testing for fold 3\n",
      "Score for 3: 0.99\n",
      "Confusion matrix for 3: \n",
      "[[23373   166]\n",
      " [  850 34521]]\n",
      "Total emails classified: 58910\n",
      "Score: 0.985502200587\n",
      "Confusion matrix:\n",
      "[[23373   166]\n",
      " [  850 34521]]\n"
     ]
    }
   ],
   "source": [
    "#Lets try running the same scheme (almost) with the logisticRegression classifier, \n",
    "#and hope it finishes running before midnight monday. \n",
    "n_folds = 4\n",
    "k_fold = KFold(n=len(data), n_folds = n_folds)\n",
    "scores = []\n",
    "confusion = numpy.array([[0, 0], [0, 0]])\n",
    "print(\"Training with %d folds\" % n_folds)\n",
    "for i, (train_indices, test_indices) in enumerate(k_fold):\n",
    "    train_text = data.iloc[train_indices]['text'].values\n",
    "    train_y = data.iloc[train_indices]['class'].values.astype(str)\n",
    "    test_text = data.iloc[test_indices]['text'].values\n",
    "    test_y = data.iloc[test_indices]['class'].values.astype(str)\n",
    "\n",
    "    print(\"Training for fold %d\" % i)\n",
    "    pipeline_w_l_lr.fit(train_text, train_y)\n",
    "    print(\"Testing for fold %d\" % i)\n",
    "    predictions = pipeline_w_l_lr.predict(test_text)\n",
    "\n",
    "    confusion += confusion_matrix(test_y, predictions)\n",
    "    score = f1_score(test_y, predictions, pos_label=SPAM)\n",
    "    scores.append(score)\n",
    "\n",
    "    print(\"Score for %d: %2.2f\" % (i, score))\n",
    "    print(\"Confusion matrix for %d: \" % i)\n",
    "    print(confusion)\n",
    "\n",
    "print('Total emails classified:', len(data))\n",
    "print('Score:', sum(scores)/len(scores))\n",
    "print('Confusion matrix:')\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([<matplotlib.axes._subplots.AxesSubplot object at 0x7f2769d5df60>,\n",
       "       <matplotlib.axes._subplots.AxesSubplot object at 0x7f277418c3c8>], dtype=object)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAEYCAYAAACk+XocAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAHqBJREFUeJzt3X+0nVV95/H3BwIIQsFIGyJEwqqhGsUG0UTHcXH9hXGN\nC2hXJ0BHxZrpuCZOocyPGuysEmamCFRBWC7oWBEDFkwAoVIBidbrSC1EMWDgQg1KJLmSCwJGsEIT\n+M4fe19ycjw39+x7nnPvc875vNZ6Vp6zn/PsvZ+cu8/37OfZz7MVEZiZmZXYa6YrYGZmvcfBw8zM\nijl4mJlZMQcPMzMr5uBhZmbFHDzMzKyYg0fNSdos6Z0zXQ8zs0YOHvUXeTEzqw0HDzMzK+bg0RuO\nlXSvpJ9L+pKk/SS9TNLfS3pM0pOSbpZ0+PgOkoYl/W9J/yjpaUlfkfRySX8rabuk9ZKOnMmDMmuH\npI9J2irpF5IelPQOSaskXZ/bwy8k3S3p9Q37rJT0UN52v6STG7Z9KLeLiyQ9JelHkt6S0x+RNCbp\ngzNztL3DwaP+BPx74D3AUcDrgQ/l9CuAV+blV8BnmvY9BXg/cDjw28B38j6zgQeAc7pee7MOSPod\n4KPAGyPiN4ATgM1584nAWuBlwDXATZL2ztseAv5t3udc4IuS5jRkvRi4l9QWrgHWAMeR2sn7gc9I\nOqCLh9bzHDzqL4BLI2JbRDwF3AwsiognI+LGiHg2Ip4BzgOOb9rvyoh4OCJ+AdwKPBQR/xARzwPX\nAcdO87GYlXoe2A94raR9IuKRiPhx3va9iPhy/nu+CHgJ8BaAiLg+Irbl9bXAJmBJQ74PR8TqSA/3\nWwscAfyviNgREeuAfwVeNR0H2KscPHrDtob1XwEHStpf0v/No7G2A98CDpakhveONaw/CzzW9PrA\nrtXYrAIR8RDwp8AqYEzStZLm5s1bG94X+fVcAEkflLQhn5Z6Cngd8PKGrBvbxq9yHo83pbl97IGD\nR+8ZH3n134GjgcURcTCp16G87Gk/s54SEddGxNuAI0l/xxfkf+eNv0fSXqTew0/ztbzPkk53zY6I\nlwH3MXHbsClw8Og94w3gQNKvo+2SZtP6+oUmWDfrCZKOzhfI9wOeI/WYn8+bj5P0e5JmkXonzwJ3\nAi8lBZefAXtJ+iNSz8Mq5ODRe8bv+/g0sD+pgXyHdE2juXcRLfabaLtZHe0HfAJ4HHgUOBT4eN72\nd6RBIU8C/wH4/Yh4PiJGgE8B/0Q65fs64I6GPN0WKqA9TQYlaR5wFfBbpP/cz0bEpfmX7hpSN3Iz\nsCwifp73ORv4MOnXwRkRcXtOPw74Aumi1i0RcWZO3y+X8QbgCeCUiPhJ5Udq1iWSXkK65rQfMAu4\nPiJWVdlObHeSzgFeFREfmOm6DKrJeh47gLMi4rXAm4GPSnoNsBJYFxFHA9/Ir5G0kPRLYCGwFLis\n4QLu5cDyiFgALJC0NKcvB57I6ReTzmea9YyIeBZ4e0QsAhYBSyUtodp2YrvzadgZtsfgkYeH3pPX\nnyHdG3A4aXz16vy21cD4DTgnAdfm4W6bSWOtl+TREQdFxPr8vqsa9mnM6wbAz3GynhMR/5JX9wX2\nIfXUq2wntjs/tmeGzWr3jZLmk+4LuAuYExHjQ93GgPGbb15BumA1bisp2OygYVgdMJrTyf9uAYiI\nnfnu59kR8WTRkZjNoDza5/ukm8w+ExHrJVXZTqxBRJw703UYdG1dMJd0IKlXcGZEPN24LY+v9i8A\nG2gR8UI+bXUEqRfxuqbtbifWVybteUjahxQ4ro6Im3LymKTDImJb7mqP33w2SsPYa1JD2prTj2iR\nPr7PK0njs2cBB7fqdUhyw7OuiIjKzp9HxHZJ3yQ9TqaKdjLaXIbbgnVLSVvYY88jX8S7AhiJiE83\nbPoKcHpePx24qSH9VEn7SjoKWACsz48J+IWkJTnPD5CG2TXn9QekC4sTHVhXl3POOcdlDFgZVZB0\nqKRD8vr+wLtJ1weraCc30cLOnTtn9P+2is+mDnnUoQ51OY5Sk/U83kp6SNgPJG3IaWcD5wNrJS0n\nD0EEiIgRSWuBEWAnsCJ21WoFaQji/qQhiLfl9CuAqyVtIg3VPbX4KMxm1lxgdX4o317Amoi4RdKd\nVNdOdvPqV/8umzbd18VDMtuzPQaPiLiDiXsn75pgn/NID+lrTr8bOKZF+nPkRmXWiyJiI+k+peb0\nJ6monTR77rnyeppVyXeYNxgaGnIZA1bGoOr0/7aKz6YOedShDlXkMRNtZY93mNeJpOiVulrvkERU\neMF8OkiKefNeyyOP+LSVVae0LbjnYWZmxRw8zMysmIOHmZkVc/AwM7NiPRU8tm/fPtNVMDMzeix4\nfO1rX5vpKpiZGT0WPMzMrB4cPMzMrJiDh5mZFXPwMDOzYg4eZmZWzMHDzMyKOXiYmVkxBw8zMyvm\n4GFmZsUcPMzMrJiDh5mZFXPwMDOzYg4eZmZWzMHDzMyKOXiYmVkxBw8zMyvm4GFmZsUcPMzMrJiD\nh5mZFXPwMOuQpHmSvinpfkn3STojp6+StFXShry8t2GfsyVtkvSgpBMa0o+TtDFvu2QmjsesHbNm\nugJmfWAHcFZE3CPpQOBuSeuAAC6KiIsa3yxpIXAKsBA4HPi6pAUREcDlwPKIWC/pFklLI+K26T0c\ns8m552HWoYjYFhH35PVngAdIQQFALXY5Cbg2InZExGbgIWCJpLnAQRGxPr/vKuDkrlbebIocPMwq\nJGk+cCxwZ076E0n3SrpC0iE57RXA1obdtpKCTXP6KLuCkFmtOHiYVSSfsroeODP3QC4HjgIWAY8C\nn5rB6plVytc8zCogaR/gBuCLEXETQEQ81rD9c8DN+eUoMK9h9yNIPY7RvN6YPtqqvO3bH2PVqlUA\nDA0NMTQ0VMVh2AAZHh5meHh4yvsrXaOrP0mxZs0ali1bNtNVsT4iiYhodV2iJA8Bq4EnIuKshvS5\nEfFoXj8LeFNE/GG+YH4NsJh8wRx4VUSEpLuAM4D1wFeBS5svmEuKefNeyyOP3NdJtc12U9oW3PMw\n69xbgfcDP5C0Iad9HDhN0iLSqKuHgY8ARMSIpLXACLATWBG7fsWtAL4A7A/c4pFWVlcOHmYdiog7\naH398NY97HMecF6L9LuBY6qrnVl3+IK5mZkVc/AwM7NiDh5mZlbMwcOsB23Zcj+SSAO9zKafg4dZ\nz+qNYfbWnxw8zMysmIOHmZkVc/AwM7NiDh5mZlZs0uAh6fOSxiRtbEirbIY0SftJWpPT75R0ZJUH\naGZm1Wun53ElsLQpbXyGtGPzciv82gxpS4HLtGss4fgMaQuABZLG81xOeqDcAuBi4IKOjsjMzLpu\n0uAREd8GnmqxqaoZ0k4kPZEU0iOt39l+9c3MbCZ0cs2jqhnSDge2AETETmC7pNkd1MvMzLpsqsHD\nM6SZmQ2wKT2SvaIZ0rY27PNK4KeSZgEHR8STrcq97rrrGBkZATx7mk1Np7OnmVnS1kyCkuYDN0fE\nMfl1ZTOkSVoBHBMR/1nSqcDJEXFqizp4JkGrXBUzCU43SbnRBiB6ZTZQq7fKZxKUdC1wPHCopC3A\nOcBQhTOkXQFcLWkT8ATwa4HDzMzqxXOY20Bzz8MsKW0LvsPczMyKOXiYmVkxBw8zMyvm4GFmZsUc\nPMzMrJiDh5mZFXPwMDOzYg4eZmZWzMHDzMyKOXiYdUjSPEnflHS/pPsknZHTZ0taJ+mHkm5vmLqg\neMZNs7px8DDr3A7grIh4LfBm4KOSXgOsBNZFxNHAN/Lrqc64aVYrDh5mHYqIbRFxT15/BniA9FTp\nxlkyV7Nr9sypzLhpVisOHmYVytMXHAvcBcyJiLG8aQyYk9enMuOmWa04eJhVRNKBwA3AmRHxdOO2\nPDWBH39rfWNKMwma2e4k7UMKHFdHxE05eUzSYRGxLZ+SGp+Bs2TGzdGJS10FpNkRPaumlep0Vk3P\n52EDrYr5PPLF7tXAExFxVkP6hTntAkkrgUMiYuVUZtxsKs/zeVjlKp9J0Mwm9Vbg/cAPJG3IaWcD\n5wNrJS0HNgPLYMozbprVioOHWYci4g4mvn74rgn2OQ84r0X63cAx1dXOrDt8wdzMzIo5eJiZWTEH\nDzMzK+bgYWZmxRw8zMysmIOHmZkVc/AwM7NiDh5mZlbMwcPMzIo5eJiZWTEHDzMzK+bgYWZmxRw8\nzMysmIOHmZkVc/AwM7NiDh5mZlbMwcPMzIo5eJiZWTEHDzMzK+bgYWZmxRw8zMysmIOHmZkVc/Aw\nM7NiDh5mZlbMwcPMzIo5eJiZWTEHDzMzKzZp8JD0eUljkjY2pM2WtE7SDyXdLumQhm1nS9ok6UFJ\nJzSkHydpY952SUP6fpLW5PQ7JR1Z5QGaddsEbWSVpK2SNuTlvQ3bitqIWR210/O4EljalLYSWBcR\nRwPfyK+RtBA4BViY97lMkvI+lwPLI2IBsEDSeJ7LgSdy+sXABR0cj9lMaNVGArgoIo7Ny60w5TZi\nVjuTBo+I+DbwVFPyicDqvL4aODmvnwRcGxE7ImIz8BCwRNJc4KCIWJ/fd1XDPo153QC8cwrHYTZj\nJmgjAGqRNpU2YlY7U73mMScixvL6GDAnr78C2Nrwvq3A4S3SR3M6+d8tABGxE9guafYU62VWJ38i\n6V5JVzSc2p1KGzGrnY4vmEdEkLroZrbL5cBRwCLgUeBTM1sds2rNmuJ+Y5IOi4htubv9WE4fBeY1\nvO8I0q+p0bzenD6+zyuBn0qaBRwcEU+2KvS6665jZGQEgKGhIYaGhqZYfRtUw8PDDA8Pd72ciBhv\nE0j6HHBzflnSRkb3XMoqIB2T24KV6rgtRMSkCzAf2Njw+kLgY3l9JXB+Xl8I3APsS/rV9SNAedtd\nwBLSeeBbgKU5fQVweV4/FfjSBHWINWvWhFmVyJ3nTpcWbWRuw/pZwDUxxTbSoqzc23+x/mYdK20L\nk/Y8JF0LHA8cKmkL8BfA+cBaScuBzcCyHIhGJK0FRoCdwIpcqfEg8QVgf+CWiLgtp18BXC1pE/BE\nDiBmPaNFGzkHGJK0iPRF/zDwEZhyGzGrHe36u603SbFmzRqWLVs201WxPiKJiGg1Kqq2JOVGG4Do\nlTZs9VbaFnyHuZmZFXPwMDOzYg4eZmZWzMHDrMfterqJ2fRx8DAzs2IOHmZmVszBw8zMijl4mJlZ\nMQcPMzMr5uBhZmbFHDzMzKyYg4eZmRVz8DAzs2IOHmZmVszBw8zMijl4mJlZMQcPMzMr5uBhZmbF\nHDzMzKyYg4eZmRVz8DDrA5I8KZRNKwcPs74QM10BGzAOHmZmVszBw8zMijl4mJlZMQcPMzMr5uBh\nZmbFHDzMOiTp85LGJG1sSJstaZ2kH0q6XdIhDdvOlrRJ0oOSTmhIP07Sxrztkuk+DrMSDh5mnbsS\nWNqUthJYFxFHA9/Ir5G0EDgFWJj3uUy7btC4HFgeEQuABZKa8zSrDQcPsw5FxLeBp5qSTwRW5/XV\nwMl5/STg2ojYERGbgYeAJZLmAgdFxPr8vqsa9jGrHQcPs+6YExFjeX0MmJPXXwFsbXjfVuDwFumj\nOd2slhw8zLosIgLfAm59ZtZMV8CsT41JOiwituVTUo/l9FFgXsP7jiD1OEbzemP66J6LWFVVXW0A\nDQ8PMzw8POX9lX4U1Z+kWLNmDcuWLZvpqlgfkUREdPxEQUnzgZsj4pj8+kLgiYi4QNJK4JCIWJkv\nmF8DLCadlvo68KqICEl3AWcA64GvApdGxG0tysqNNoDxqqf1XmnPVj+lbcE9D7MOSboWOB44VNIW\n4C+A84G1kpYDm4FlABExImktMALsBFbErm/8FcAXgP2BW1oFDrO6cPAw61BEnDbBpndN8P7zgPNa\npN8NHFNh1cy6xhfMzcysmIOHmZkVc/AwM7NiDh5mZlbMwcPMzIo5eJiZWTEHDzMzK+bgYWZmxRw8\nzMysWEfBQ9JmST+QtEHS+pzmGdTMzPpcpz2PAIYi4tiIWJzTPIOamVmfq+K0VfNTGD2DmplZn6ui\n5/F1Sd+T9Mc5zTOomZn1uU6fqvvWiHhU0m8C6yQ92Lgxz1HgCQbMzPpMR8EjIh7N/z4u6UbSBDdd\nm0HtuuuuY2RkBIChoSGGhoY6qb4NoE5nTzOzZMozCUo6ANg7Ip6W9FLgduBc0hwGlc+g5pkErRuq\nmklwOnkmQeuG6ZxJcA5wYx4wNQv424i4XdL38AxqZmZ9bcrBIyIeBha1SH8Sz6BmZtbXfIe5mZkV\nc/AwM7NiDh5mZlbMwcPMzIo5eJiZWTEHDzMzK+bgYWZmxRw8zMysmIOHmZkVc/Aw66KqZts0qxsH\nD7Pu6nS2TbdRqyX/YZp1XyezbS7GrIYcPMy6q4rZNs1qp9OZBM1szzqdbdMTdFgtOXiYdVEFs222\nnFUzWdWNKtuA6HRWzSnPJDjdPJOgdUM3ZxKscrbNpnwnnEkQ8GyCNiXTOZOgme1ZlbNtmtWKg4dZ\nl1Q526ZZ3Xi0lZmZFXPwMDOzYj5tZdZn8jUWwBfPrXvc8zDrSw4a1l0OHmZmVszBw8zMijl4mJlZ\nMQcPMzMr5uBhZmbFHDzMzKyYg4eZmRXrqafqjq/3Sp2t/rr5VN1umeypuo2v3VasXaVtwT0PMzMr\n5uBhZmbFHDzMzKyYg4eZmRXzU3XN+pifsGvd4p6HWd9z0LDqOXiYmVkxBw8zMyvm4GFmZsUcPMzM\nrJiDh5mZFXPwMDOzYg4eZgOi8Z4Ps045eJiZWTEHDzNrmyT3YAyoUfCQtFTSg5I2SfrYJO+drmqZ\nTbuStjCFvCfd7vZl7ahF8JC0N/AZYCmwEDhN0mumux7Dw8MuY8DKqJvpaAvjAaKTILGnz6bdvKv4\nfDvNow51qCKPmWgrtQgewGLgoYjYHBE7gC8BJ013JfrlC9Fl9LRpaAu//qyr5i/8yQJMHb4wq8ij\nDnWoIo9BDh6HA1saXm/NaWaDZlrbwu4BojmoxG7v8eksa1SX4FH82E//QVufmuZH4LZb3O7vO/fc\nc6fU9txm+4fq8Ix/SW8GVkXE0vz6bOCFiLig4T0zX1HrSxFRm28ztwWbSSVtoS7BYxbwz8A7gZ8C\n64HTIuKBGa2Y2TRzW7BeUYuZBCNip6T/AnwN2Bu4wo3FBpHbgvWKWvQ8zMyst9Si59Esj2s/iV2j\nTLYCX/EvMBs0bgtWV7XreeQ7ak8jjW/fmpPnAacAayLiExWXNxsgIp6sMl+XMdhlVKGqtiBpKXAy\nuwLQKHBTRNxWbY27W4eqjqOKz78Oecx4HSKiVguwCdinRfq+pJunqijjSFKDfBx4KC+P57T5LsNl\n1GGpoi0AlwC3AKcCb8vLaTnt0jbzWAr8NXBzXv4aWFpwHFXUoaM8qvj865BHHerwYj4z3UBaHNiD\nrQ4AmA/8c0Vl3En69TarIW1W/sO802W4jDosVbQFYNME6aKNANTpl3YVdajoODr+/OuQRx3q8OI+\nJX/M07GQfuU8BNwG/E1ebgN+BLy3ojJa/iFOts1luIzpXKpoC8BGYHGL9CXAxqn+vxV+8XdUh24e\nR8nnX4c86lCH8aV2F8wj4jZJv0N6xs/hpFtbR4HvRcTOior5vqTLgNXsehTEK4HTgQ0uw2XUQUVt\n4UPA5ZIOYtd1kyOAX+Rtk3lW0uKIWN+Uvhj41TTVoYo8qvj865BHHeoA1PCC+XSQtB+wHDiR3S++\nfYU0rv45l+Ey+omkucAr8svRiNjW5n7HAZcDrb60V0TE3d2uQxV5VPH51yGPOtThxXwGMXiYDRKl\nh0ktYffhvuujoPF3+sVfUR06zsOqU7vTVtOlxbC/rcDfRYXDF13G4JVRN5JOAC4jXTtp7DkskLQi\nIr7WRh4ijdAZ/3+bJWms3S/tiupQRR4df/51yKMOdYAB7XlIugRYAFxF6q5B+kP8AOki4Bkuw2X0\nA0kPkobVbm5KPwq4NSJePcn+E35pk05btfOl3VEdKjqOjj//OuRRhzq8qN0r6/20UMEIEpfhMnph\nocN7RZh4uPBRwIPTUYeKjmPGhwtXkUcd6jC+DOppqypGkLgMl9ELPg98V9K17H6X+ql522T2Ztev\n00ajtH/au9M6VJFHFZ9/HfKoQx2AwT1tVdkIEpfhMupO0kLS87FevOBNej7WSBv7nk26oazVl/ba\niDiv23Wo6Dg6/vzrkEcd6vBiPoMYPMZVMXTQZbiMflfFF39dzORw4SrzqEMdBvW0VccjSFyGy+gF\nkg4BVpJG1swh3Wj4GHATcH5E/HyyPHKQmHKgqKIOFeXR8edfhzzqUAcY0J5HFSNIXIbL6AWSbge+\nQbqbeCwiIv/iPB14R0ScMMn+VXxpd1SHio6jilFjM55HHerwonavrPfTQgUjSFyGy+iFBfjhVLY1\nvOd24GPAYez6sTmXFFBun446VHQcVYwam/E86lCH8WWvtiJM/6liBInLcBm94CeS/kzSnPEESYcp\nzRXySBv7z4+ICyJiW+RvmIh4NCLOJz3ddzrqUEUeVXz+dcijDnWAkjf2mSqGDroMl9ELTiH1Er7V\n8MU7RnqO0bI29v+JpD8DVkfEGKQvbdLpona/+FvVYRtpbpB26lBFHnUYLlxFHnWoAzCg1zxgekaQ\nuIzBK6OOlKayPRy4KyKebkhfGpM8jkJpprmVpIfoNQef86ONGegkLSGdDtku6aU5vzcA9wN/GRHb\n28hjP9KX208jYp2k9wNvIV3I/2xE7GgjjxkdLlxVHnWoAwxw8DAbBJLOAD4KPAAcC5wZETflbRsi\n4tgp5Hl1RHyg4P0jwOsjYqekvwF+CVwPvCun/34beVxDOt1yAPBz4EDgyzkPIuL00uOoC0lzxnt1\nU9z/0Ij4WZV1aku7F0f6aQEOAc4nXTh6Cngyr58PHOIyXEa/LMB9wIF5fT5wN/Cn+fWGNva/mdTL\nuLlh+eV4ept1eKBh/ftN2+5tM4+N+d9ZpNFes/Jr0d5kUB1//sDB+f1fBP6wadtlbeYxu2l5ObB5\n/HUb+18A/GZefyPwY9KoqUeAoTbr8Cbgm/k45gHrgO3Ad4Fj2/3bGtQL5mtJf0BDpA9sNvB20i+a\ntS7DZfQRRcQzAJEeKng88F5JF5O+eCdzBPA0cBHwSeBTpDuRx9fbcb+kD+f1eyW9CUDS0cC/tnsc\n+dTVQcD+pC9ygJfQ3rXbKj7/K/O/NwCnSbpB0kty2lvazONnpAA+vnyPdEpxfH0y/y4iHs/rnwRO\niYhXkXpg7X4elwEXAl8F/gn4LCm4rszb2jMdv37qtlDB0EGX4TJ6YSH9wlzUlLYP6YmqL7Sx/97A\nfwW+Tv5VCjxcWIdDSPdn/Bi4C9gBPAz8P+B328zjrLz/I8CZpHs+PkfqWa2ajs+fpl4S8OfAPwKH\n0kYvLu/z30hTCb++Ia3t/0/S6cd98vqdTdvandJ3Q8P6I03b7mm3LoM62qqKESQuw2X0gg+Svqxf\nFBE7JJ1O+sW5RxHxPHCRpLXAxZIeo3CUZqQbCU+XdDDpXoJZwNYoeBxGRFyc60BEjEq6ivRr+7Px\n6w/4a6WKz39fSXtFxAu5Hn8paRT4FukaTDvH8al8HBdJ2gqc02bZ4y4DbpH0CeA2pcerfxl4B3BP\nm3k8J+k9pN5bSPq9iLhR0vFA+1N9l/yC6JeFdH7xQnad/3wqr19IG+cdXUbflPFAlWUMwgK8Dzhv\nputR0edf9DcG/BXw7hbpS5ngMeeT5HcSqSc2Vrjf20mn2jYAG4FbgY/Q4pH1E+y/iHTz523Aq4FL\nSafvRoC3tlsPj7ZqIumPIuLKyd/ZVl5THiJZUMYSICJivaTXAe8hXaC8pYr8JyizaLTNFPJ/G+nx\n0Bsj4vaK8ux4uKj1pyravKQPR0Tx/UKSDgB+OyI2dlqPio6j7TwcPJpI2hIR8yrIp/Ihki3KWEX6\n1bMP6ZfEEtI57neTHh3xfyoo42bS84waL66+A/gHUtA6sYIy1kfE4rz+x6T/txuBE4C/j4hPVFBG\nx8NFrT9V0ebrkMd012Egr3lI2riHzXP2sK3EfwKOi4hnJM0HbpA0PyI+XVH+AH9A6oLuS7px64j8\ny/qTwHqg4+BBGm0zQro4+QIpiLyRNNKjKvs0rH+EdGrg8XwcdwEdBw/SD6Xx87nHRcQb8vodku6t\nIH+rsSra/DTk8Vvd3r+NPNr+/hvI4EH6T15KOu/Z7DsVlbHbEMl8MeoGSUfS3hDJduzMX4g7Jf1o\n/NRLRPxK0gsVlfFG0uiWPwf+R0RskPRsRHyrovwB9s53MgvYO/JQxIj4paT2L+Dt2f0NpxbulfSm\niPhu4XBR611VtPk65FGHOgCDGzy+SrpxakPzBklVfSk+JmlRRNwDkHsg7wOuAF5fURnPSTogIv6F\ndP4eePEx2s9XUUBUMNqmDb9BGucOafTH3Ih4VNJBFZbxH4FLJP1P4HHgO3m0y5a8zfpbFW2+DnnU\noQ7pvb7m0R2S5gE7omk4oiSRRjTcUUEZL4mIZ1ukHwrMjYg9dU+nWub7gH8TER+vOu8WZR0AzImI\nhyvMc8rDRc1sFwcPMzMrNqiPJzEzsw44eJiZWTEHDzMzK+bgYWZmxRw8zMys2P8HXYVzCRw/iccA\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2760aa8c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">ham</th>\n",
       "      <th>count</th>\n",
       "      <td>21889.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>363.062589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1253.975002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>59.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>149.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>318.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>50295.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">spam</th>\n",
       "      <th>count</th>\n",
       "      <td>10500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>625.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>914.769248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>151.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>327.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>666.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>16955.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      len\n",
       "class                    \n",
       "ham   count  21889.000000\n",
       "      mean     363.062589\n",
       "      std     1253.975002\n",
       "      min        1.000000\n",
       "      25%       59.000000\n",
       "      50%      149.000000\n",
       "      75%      318.000000\n",
       "      max    50295.000000\n",
       "spam  count  10500.000000\n",
       "      mean     625.010000\n",
       "      std      914.769248\n",
       "      min        0.000000\n",
       "      25%      151.000000\n",
       "      50%      327.500000\n",
       "      75%      666.000000\n",
       "      max    16955.000000"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.groupby('class').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Q1.3 SMS Spam Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1.3.1 Test the classifier trained on email data in 1.2 on the SMS data.\n",
    "We are running the original pipeline trained by Zac in Q1.2.1, on the sms data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total smss classified: 5574\n",
      "Score: 0.87226181865\n",
      "Confusion matrix:\n",
      "[[4274  553]\n",
      " [ 572  175]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "messages1 = pd.read_csv('./data/SMSSpamCollection', sep='\\t', quoting=csv.QUOTE_NONE,\n",
    "                           names=[\"class\", \"text\"])\n",
    "\n",
    "test_text = messages1['text'].values\n",
    "test_y = messages1['class'].values.astype(str)\n",
    "\n",
    "predictions = pipeline.predict(test_text)\n",
    "\n",
    "confusion = numpy.array([[0, 0], [0, 0]])\n",
    "confusion += confusion_matrix(test_y, predictions)\n",
    "score = f1_score(test_y, predictions, pos_label=SPAM)\n",
    "scores.append(score)\n",
    "\n",
    "print('Total smss classified:', len(messages1))\n",
    "print('Score:', sum(scores)/len(scores))\n",
    "print('Confusion matrix:')\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the original classifier from Zac's tutorial, we can see that althogh it does not do as well\n",
    "on the new database, it does pretty well. These results fit our expectations. Spam does carry similar features \n",
    "over sms, but still varies from the original data set. \n",
    "\n",
    "Total smss classified: 5574\n",
    "Score: 0.87226181865\n",
    "Confusion matrix:\n",
    "[[4274  553]\n",
    " [ 572  175]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1.3.2 Test the SMS classifier trained on the email data\n",
    "\n",
    "We'll take the SMS classifier from Radim's notebook, and run in with the same code Zac used to run his own.\n",
    "Hopefully, this will assure as much symmetry as possible, and render our results as informative as possible. \n",
    "For this purpose we transformed Rare's (short for RNDr. Radim Řehůřek, Ph.D.) code into a pipeline, and the\n",
    "ran Zac's code on the same data, with a different pipeline. \n",
    "\n",
    "For reference, this code: \n",
    "\n",
    " bow_transformer = CountVectorizer(analyzer=split_into_lemmas).fit(data['text'])\n",
    " messages_bow = bow_transformer.transform(data['text'])\n",
    " tfidf_transformer = TfidfTransformer().fit(messages_bow)\n",
    " messages_tfidf = tfidf_transformer.transform(messages_bow)\n",
    " spam_detector = MultinomialNB().fit(messages_tfidf, data['class'])\n",
    " all_predictions = spam_detector.predict(messages_tfidf)\n",
    "\n",
    "\n",
    "Was basically reduced to this code: \n",
    "\n",
    "rare_pipe=Pipeline([\n",
    "            ('CV', CountVectorizer(analyzer=split_into_lemmas)),\n",
    "            ('TFIDF',  TfidfTransformer()),\n",
    "            ('Classifier', MultinomialNB())    \n",
    "            \n",
    "rare_pipe.fit(train_text, train_y)\n",
    "predictions = rare_pipe.predict(test_text)\n",
    "\n",
    "Neat!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# messages = pandas.read_csv('/home/noam/Dropbox/pythonWorkspase/NLP_HW2/data_science_python/data/SMSSpamCollection', sep='\\t', quoting=csv.QUOTE_NONE,\n",
    "#                            names=[\"label\", \"message\"])\n",
    "\n",
    "def split_into_tokens(message):\n",
    "    return TextBlob(message).words\n",
    "\n",
    "\n",
    "def split_into_lemmas(message):\n",
    "    message = message.lower()\n",
    "    words = TextBlob(message).words\n",
    "    # for each word, take its \"base form\" = lemma \n",
    "    return [word.lemma for word in words]\n",
    "\n",
    "rare_pipe=Pipeline([\n",
    "            ('CV', CountVectorizer(analyzer=split_into_lemmas)),\n",
    "            ('TFIDF',  TfidfTransformer()),\n",
    "            ('Classifier', MultinomialNB())    \n",
    "                   ])\n",
    "k_fold = KFold(n=len(data), n_folds=6)\n",
    "scores = []\n",
    "confusion = numpy.array([[0, 0], [0, 0]])\n",
    "for train_indices, test_indices in k_fold:\n",
    "    train_text = data.iloc[train_indices]['text'].values\n",
    "    train_y = data.iloc[train_indices]['class'].values.astype(str)\n",
    "\n",
    "    test_text = data.iloc[test_indices]['text'].values\n",
    "    test_y = data.iloc[test_indices]['class'].values.astype(str)\n",
    "\n",
    "    rare_pipe.fit(train_text, train_y)\n",
    "    predictions = rare_pipe.predict(test_text)\n",
    "\n",
    "    confusion += confusion_matrix(test_y, predictions)\n",
    "    score = f1_score(test_y, predictions, pos_label=SPAM)\n",
    "    scores.append(score)\n",
    "    \n",
    "\n",
    "# print('accuracy', accuracy_score(messages['label'], all_predictions))\n",
    "# print('confusion matrix\\n', confusion_matrix(data['class'], all_predictions))\n",
    "# print('(row=expected, col=predicted)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Total emails classified:', len(data))\n",
    "print('Score:', sum(scores)/len(scores))\n",
    "print('Confusion matrix:')\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CountV.get_stop_words()\n",
    "CountV.get_params()['analyzer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sms classifier: \n",
    "\n",
    "accuracy 0.969501255831\n",
    "confusion matrix\n",
    " [[4827    0]\n",
    " [ 170  577]]\n",
    "(row=expected, col=predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.967747411305\n",
      "confusion matrix\n",
      " [[22495  1044]\n",
      " [  856 34515]]\n",
      "(row=expected, col=predicted)\n"
     ]
    }
   ],
   "source": [
    "#ow_transformer = CountVectorizer(analyzer=split_into_lemmas).fit(data['text'])\n",
    "#messages_bow = bow_transformer.transform(data['text'])\n",
    "#tfidf_transformer = TfidfTransformer().fit(messages_bow)\n",
    "#messages_tfidf = tfidf_transformer.transform(messages_bow)\n",
    "#spam_detector = MultinomialNB().fit(messages_tfidf, data['class'])\n",
    "all_predictions = spam_detector.predict(messages_tfidf)\n",
    "\n",
    "print('accuracy', accuracy_score(data['class'], all_predictions))\n",
    "print('confusion matrix\\n', confusion_matrix(data['class'], all_predictions))\n",
    "print('(row=expected, col=predicted)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. Named Entity Recognition\n",
    "    Features:\n",
    "            we are intrested in creating a vectorized obejct from our data set, which will take into consideration the\n",
    "            (1) word-form (2)the POS of the word (3) ORT, (4) perfix1 ,(5) perfix2, (6) perfix3, (7) suffix1 \n",
    "            (8) suffix2, (9) suffix3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "first let's load and split our data set to test and train sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split data set to train and test data sets\n",
    "train_sents = list(nltk.corpus.conll2002.iob_sents('esp.train'))\n",
    "test_sents = list(nltk.corpus.conll2002.iob_sents('esp.testb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " we've noticed that our data is built as a list of sentences, all of which are constructed from a list of tripules in the following foramt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Melbourne', 'NP', 'B-LOC'),\n",
       " ('(', 'Fpa', 'O'),\n",
       " ('Australia', 'NP', 'B-LOC'),\n",
       " (')', 'Fpt', 'O'),\n",
       " (',', 'Fc', 'O'),\n",
       " ('25', 'Z', 'O'),\n",
       " ('may', 'NC', 'O'),\n",
       " ('(', 'Fpa', 'O'),\n",
       " ('EFE', 'NC', 'B-ORG'),\n",
       " (')', 'Fpt', 'O'),\n",
       " ('.', 'Fp', 'O')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='padding: 10px;'><code>[  [(&lt;WORD&gt;, &lt;POS&gt;, &lt;CLASS&gt;),....],<br />&nbsp;....[]&nbsp;...<br/>]</code>\n",
    "</div><br/>\n",
    "We would like to add another features, and will do that in a manner simmilar to the one being done\n",
    "in the <a href=\"http://nbviewer.ipython.org/github/tpeng/python-crfsuite/blob/master/examples/CoNLL%202002.ipynb\" target=\"_blank\">CoNLL Classification</a> we'll build out a dictionary with all the\n",
    "wanted features and use <i><u>DictVectorizer</u></i> to get a vectorized representation of the word\n",
    "according to it's features. \n",
    "In order to do that let's define the following function to extract the features from our data-set, first, let's deine a function to extract ortographic data based on the word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ort(word):\n",
    "    if (re.match(\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\", word) != None):\n",
    "        return \"URL\"\n",
    "    if (re.match(\"^-?\\+?[0-9]+(.[0-9]+)?$\", word) != None):\n",
    "        return \"number\"\n",
    "    if (re.search(\".*[0-9]+.*\", word) != None):\n",
    "        return \"contains-digit\"\n",
    "    if (word.find(\"-\") != -1):\n",
    "        return \"contains-hyphen\"\n",
    "    if word.isupper():\n",
    "        return \"all-capitals\"\n",
    "    if (re.match(\"^[A-Z].*\", word) != None):\n",
    "        return \"capitalized\"\n",
    "    if (re.match(\"^[,;.-/!/?/*/+]+$\", word) != None):\n",
    "        return \"punctuation\"\n",
    "    return \"regular\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will return a string value representing one ortographic feature for a word. For simplicity any word has a single orthograpic feature, and if it doesn't contain any we will say the word has a regular stracture. Now based on this function let's define a function to extract the features from a given word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "    features = {\n",
    "        'WORD-FROM': word.lower(),\n",
    "        'POS': postag,\n",
    "        'ORT': get_ort(word),\n",
    "        'PREFIX1': word.lower()[:1],\n",
    "        'PREFIX2': word.lower()[:2],\n",
    "        'PREFIX3': word.lower()[:3],\n",
    "        'SUFFIX1': word.lower()[-1:],\n",
    "        'SUFFIX2': word.lower()[-2:],\n",
    "        'SUFFIX3': word.lower()[-3:]}\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This function will extract the wanted features for each index in te sentence array provided to it. The features will be sent as a dictionary, which is the prefered format of the <b><u>DictVectorizer</u></b> which will later convert is to a one-hot vector.\n",
    "<br />Let's also define a function to easly extract the NER tag from a sentence:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word2label(sent, i):\n",
    "    return sent[i][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now construct our model's pipeline based on the <b>DictVectorizer</b> and the <b>Logistic Regression Classifier</b>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_pipeline():\n",
    "    pipeline = Pipeline([\n",
    "        ('vectorize', DictVectorizer(sparse=True)),\n",
    "        ('classify', LogisticRegression())\n",
    "    ])\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first use the Spanish data-set for training and testing, let's define our sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_sents = list(nltk.corpus.conll2002.iob_sents('esp.train'))\n",
    "test_sents = list(nltk.corpus.conll2002.iob_sents('esp.testb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this data set is pretty big, (we acturally have tried training our pipline with simple array - which abviusly failed badly due to lack of memory) in order to overcome this problem, we'll load the data-set to a numpy arrays, and along the way will already build it in a format which will be easer for us to extract our features from. <br />\n",
    "First let's re-use the progress function from the SPAM note-book, so we won't be bored while the data is processed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def progress(i, end_val, bar_length=50):\n",
    "    '''\n",
    "    Print a progress bar of the form: Percent: [#####      ]\n",
    "    i is the current progress value expected in a range [0..end_val]\n",
    "    bar_length is the width of the progress bar on the screen.\n",
    "    '''\n",
    "    percent = float(i) / end_val\n",
    "    hashes = '#' * int(round(percent * bar_length))\n",
    "    spaces = ' ' * (bar_length - len(hashes))\n",
    "    sys.stdout.write(\"\\rPercent: [{0}] {1}%\".format(hashes + spaces, int(round(percent * 100))))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will write a function to build our data in a numpy DataFrame while extracting out our features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_data(data, feature_extractor=(lambda sent, i: word2features(sent, i))):\n",
    "    df = DataFrame({'features': [], 'class': []})\n",
    "    print(\"Starting To Build Data.\")\n",
    "    for i, sent in enumerate(data):\n",
    "        data_frame, nrows = build_data_frame(i, len(data), sent, feature_extractor)\n",
    "        df = df.append(data_frame)\n",
    "    print(\"\\nDone!\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will return a data frame with 'features' holding our dictionary of features (based on our feature extractor - the default one will be the one defined earlier) and 'class' will hold the values to be learned by the classifier - in our case the NER tag.\n",
    "and Here is the <i>build_data_frame</i> function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_data_frame(l, len_data, sent, feature_extractor):\n",
    "    rows = []\n",
    "    index = []\n",
    "    for i in range(len(sent)):\n",
    "        rows.append({'features': feature_extractor(sent, i), 'class': word2label(sent, i)})\n",
    "        index.append(sent)\n",
    "    progress(l, len_data)\n",
    "    data_frame = DataFrame(rows, index=index)\n",
    "    return data_frame, len(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are redy for our training function, again we should keep in mind that our data-set it pretty big, so in order to easely work with our data we will split the data set using <b><u>KFold</u></b> simmilar to the way it's being done with the SPAM detector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(data_sents=None, data_frame=None, n_folds=6):\n",
    "    if data_frame is None and data_sents is None:\n",
    "        raise Exception('No data was provided to train!')\n",
    "    elif data_frame is None:\n",
    "        data_frame = build_data(data_sents)\n",
    "\n",
    "    k_fold = KFold(n=len(data_frame), n_folds=n_folds)\n",
    "    pipeline = build_pipeline()\n",
    "\n",
    "    print(\"Training with %d folds\" % n_folds)\n",
    "    for i, (train_indices, test_indices) in enumerate(k_fold):\n",
    "        x_train = data_frame.iloc[train_indices]['features'].values\n",
    "        y_train = data_frame.iloc[train_indices]['class'].values.astype(str)\n",
    "        if (q2Verbose):\n",
    "            print(\"Training for fold %d\" % i)\n",
    "        pipeline.fit(x_train, y_train)\n",
    "    print('Total classified:', len(data_frame))\n",
    "    return pipeline, data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we're already at-it let's also write another feature extractor, one which will also take into consideration the previouse and next words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we can play a bit with the features\n",
    "def word2features2(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "    fetures = {\n",
    "        'WORD-FROM': word.lower(),\n",
    "        'POS': postag,\n",
    "        'ORT': get_ort(word),\n",
    "        'PREFIX1': word.lower()[:1],\n",
    "        'PREFIX2': word.lower()[:2],\n",
    "        'PREFIX3': word.lower()[:3],\n",
    "        'SUFFIX1': word.lower()[-1:],\n",
    "        'SUFFIX2': word.lower()[-2:],\n",
    "        'SUFFIX3': word.lower()[-3:]}\n",
    "\n",
    "    if i < (len(sent) - 1):\n",
    "        fetures['NEXT_WORD_FORM'] = sent[i + 1][0]\n",
    "        fetures['NEXT_POS'] = sent[i + 1][1]\n",
    "    else:\n",
    "        fetures['NEXT_WORD_FORM'] = '*'\n",
    "        fetures['NEXT_POS'] = '*'\n",
    "\n",
    "    if i > 0:\n",
    "        fetures['PREV_WORD_FORM'] = sent[i - 1][0]\n",
    "        fetures['PREV_POS'] = sent[i - 1][1]\n",
    "    else:\n",
    "        fetures['PREV_WORD_FORM'] = '*'\n",
    "        fetures['PREV_POS'] = '*'\n",
    "\n",
    "    return fetures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have also added an ending, and beginning padding with '*' to mark words at a beginning of a sentence or words without followers. Now we are ready to load our data, and extract it's features. this might take a while (up to 10 minutes) <br />\n",
    "<b>The first one is the default extractor (without next and prev word features)</b> we'll call it data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting To Build Data.\n",
      "Percent: [##################################################] 100%\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "data1 = build_data(train_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The second is the more advanced extractor (with next and prev word features)</b> we'll call it data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting To Build Data.\n",
      "Percent: [##################################################] 100%\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "data2 = build_data(train_sents, word2features2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train both of our models (based on diffrent features) and see the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 6 folds\n",
      "Training for fold 0\n",
      "Training for fold 1\n",
      "Training for fold 2\n",
      "Training for fold 3\n",
      "Training for fold 4\n",
      "Training for fold 5\n",
      "Total classified: 264715\n"
     ]
    }
   ],
   "source": [
    "model1, data1 = train(None, data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 6 folds\n",
      "Training for fold 0\n",
      "Training for fold 1\n",
      "Training for fold 2\n",
      "Training for fold 3\n",
      "Training for fold 4\n",
      "Training for fold 5\n",
      "Total classified: 264715\n"
     ]
    }
   ],
   "source": [
    "model2, data2 = train(None, data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the outputs of both of the models, first we'll define those two helper function to extract data from the data-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [word2label(sent, i) for i in range(len(sent))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_test = sum([sent2features(s) for s in test_sents], [])\n",
    "y_test = sum([sent2labels(s) for s in test_sents], [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will build us two arrays of features and NER tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing model1..\n",
      "First model (without looking on previous and next word tags) scored 0.937961 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/noam/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:756: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n"
     ]
    }
   ],
   "source": [
    "print(\"testing model1..\")\n",
    "predictions1 = model1.predict(x_test)\n",
    "score1 = f1_score(y_test, predictions1)\n",
    "print(\"First model (without looking on previous and next word tags) scored %f \" %score1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's extract featues according to the <b>second feature extractor</b>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sent2features2(sent):\n",
    "    return [word2features2(sent, i) for i in range(len(sent))]\n",
    "\n",
    "x_test2 = sum([sent2features2(s) for s in test_sents], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing model2..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/noam/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:756: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n"
     ]
    }
   ],
   "source": [
    "print(\"testing model2..\")\n",
    "predictions2 = model2.predict(x_test2)\n",
    "score2 = f1_score(y_test, predictions2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have used the f1 (AKA F_Masure) score to check our models, let's compare the outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First model (without looking on previous and next word tags) scored 0.937961 \n",
      "After adding to the feature extraction better features we were managed to score  0.958372\n",
      "Here is both confusions matrix of the first one:\n",
      " [[  48    0    2    0    2    4    4    5    8]\n",
      " [   0   10    1    2    0    1    1    0    8]\n",
      " [  12    2   86    7    0    1    4    2    2]\n",
      " [   4    0    0   33    2    0    0   15    1]\n",
      " [   4    0    3    2    2    0    3    2    6]\n",
      " [   2    0    1    1    0    7    0    3   10]\n",
      " [   9    5    4    4    0    1   32    2   34]\n",
      " [   3    0    0    9    1    1    8   25    2]\n",
      " [   0    0    0    0    0    0    1    0 3257]]\n",
      "And Here is the second:\n",
      " [[  47    0    2    6    3    3    3    1    8]\n",
      " [   0   11    3    0    0    1    2    1    5]\n",
      " [  13    2   87    4    1    1    0    3    5]\n",
      " [   4    0    2   41    0    0    1    6    1]\n",
      " [   1    0    2    0   10    0    2    2    5]\n",
      " [   2    0    1    0    1   10    2    0    8]\n",
      " [   3    0    3    0    0    5   59    6   15]\n",
      " [   0    0    0    0    0    0    3   44    2]\n",
      " [   0    0    0    0    0    2    1    0 3255]]\n"
     ]
    }
   ],
   "source": [
    "print(\"First model (without looking on previous and next word tags) scored %f \" %score1 )\n",
    "print(\"After adding to the feature extraction better features we were managed to score  %f\" %score2 )\n",
    "\n",
    "print (\"Here is both confusions matrix of the first one:\\n\", confusion_matrix(y_test, predictions1))\n",
    "print (\"And Here is the second:\\n\", confusion_matrix(y_test, predictions2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now clearly see that adding the context to the feature extraction have added a 7% better f_masure score! now we can see if this is also the case with a diffrent language, let's load the dutch dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dutch_train_sents = list(nltk.corpus.conll2002.iob_sents('ned.train'))\n",
    "dutch_test_sents_a = list(nltk.corpus.conll2002.iob_sents('ned.testa'))\n",
    "dutch_test_sents_a = list(nltk.corpus.conll2002.iob_sents('ned.testb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract our features to numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dutch = build_data(dutch_train_sents, word2features2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_dutch, data_dutch = train(None, data_dutch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test it wit testa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test_dutch = sum([sent2features2(s) for s in dutch_test_sents_a], [])\n",
    "y_test = sum([sent2labels(s) for s in dutch_test_sents_a], [])\n",
    "predictions_dutcha = model_dutch.predict(x_test_dutch)\n",
    "score_dutch1 = f1_score(y_test, predictions_dutcha)\n",
    "print(score_dutch1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add testb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test_dutchb = sum([sent2features2(s) for s in dutch_test_sents_b], [])\n",
    "y_testb = sum([sent2labels(s) for s in dutch_test_sents_b], [])\n",
    "predictions_dutchb = model_dutch.predict(x_test_dutchb)\n",
    "score_dutch2 = f1_score(y_test, predictions_dutchb)\n",
    "print(score_dutch2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the average score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg_dutch_score = (score_dutch2+score_dutch1)/2\n",
    "print(\"Our NER tagger scored %f in dutch\", % avg_dutch_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see that our NER tagger had scored X in dutch, while scoring Y in Spanish. We guess this as a lot to do with spanish articles since they have unique atricles for locations and placed.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
