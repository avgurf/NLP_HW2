{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Our import: \n",
    "import nltk\n",
    "from nltk.probability import ConditionalFreqDist\n",
    "import pandas as pd\n",
    "import math\n",
    "from nltk import word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "##Our defines: \n",
    "q1Verbose=1\n",
    "q113_verbose=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. Document Classification\n",
    "#### Q1.1. Reuters Dataset\n",
    "\n",
    "##### Q1.1.1 Turn the code of the Sklearn tutorial above into a notebook.\n",
    "\n",
    "This code is taken from the out of core classification guide given in the assigmnent. \n",
    "http://scikit-learn.org/dev/auto_examples/applications/plot_out_of_core_classification.html#example-applications-plot-out-of-core-classification-py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Authors: Eustache Diemert <eustache@diemert.fr>\n",
    "#          @FedericoV <https://github.com/FedericoV/>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from glob import glob\n",
    "import itertools\n",
    "import os.path\n",
    "import re\n",
    "import tarfile\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "\n",
    "from sklearn.externals.six.moves import html_parser\n",
    "from sklearn.externals.six.moves import urllib\n",
    "from sklearn.datasets import get_data_home\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _not_in_sphinx():\n",
    "    # Hack to detect whether we are running by the sphinx builder\n",
    "    return '__file__' in globals()\n",
    "\n",
    "%matplotlib inline\n",
    "###############################################################################\n",
    "# Reuters Dataset related routines\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "class ReutersParser(html_parser.HTMLParser):\n",
    "    \"\"\"Utility class to parse a SGML file and yield documents one at a time.\"\"\"\n",
    "\n",
    "    def __init__(self, encoding='latin-1'):\n",
    "        html_parser.HTMLParser.__init__(self)\n",
    "        self._reset()\n",
    "        self.encoding = encoding\n",
    "\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        method = 'start_' + tag\n",
    "        getattr(self, method, lambda x: None)(attrs)\n",
    "\n",
    "    def handle_endtag(self, tag):\n",
    "        method = 'end_' + tag\n",
    "        getattr(self, method, lambda: None)()\n",
    "\n",
    "    def _reset(self):\n",
    "        self.in_title = 0\n",
    "        self.in_body = 0\n",
    "        self.in_topics = 0\n",
    "        self.in_topic_d = 0\n",
    "        self.title = \"\"\n",
    "        self.body = \"\"\n",
    "        self.topics = []\n",
    "        self.topic_d = \"\"\n",
    "\n",
    "    def parse(self, fd):\n",
    "        self.docs = []\n",
    "        for chunk in fd:\n",
    "            self.feed(chunk.decode(self.encoding))\n",
    "            for doc in self.docs:\n",
    "                yield doc\n",
    "            self.docs = []\n",
    "        self.close()\n",
    "\n",
    "    def handle_data(self, data):\n",
    "        if self.in_body:\n",
    "            self.body += data\n",
    "        elif self.in_title:\n",
    "            self.title += data\n",
    "        elif self.in_topic_d:\n",
    "            self.topic_d += data\n",
    "\n",
    "    def start_reuters(self, attributes):\n",
    "        pass\n",
    "\n",
    "    def end_reuters(self):\n",
    "        self.body = re.sub(r'\\s+', r' ', self.body)\n",
    "        self.docs.append({'title': self.title,\n",
    "                          'body': self.body,\n",
    "                          'topics': self.topics})\n",
    "        self._reset()\n",
    "\n",
    "    def start_title(self, attributes):\n",
    "        self.in_title = 1\n",
    "\n",
    "    def end_title(self):\n",
    "        self.in_title = 0\n",
    "\n",
    "    def start_body(self, attributes):\n",
    "        self.in_body = 1\n",
    "\n",
    "    def end_body(self):\n",
    "        self.in_body = 0\n",
    "\n",
    "    def start_topics(self, attributes):\n",
    "        self.in_topics = 1\n",
    "\n",
    "    def end_topics(self):\n",
    "        self.in_topics = 0\n",
    "\n",
    "    def start_d(self, attributes):\n",
    "        self.in_topic_d = 1\n",
    "\n",
    "    def end_d(self):\n",
    "        self.in_topic_d = 0\n",
    "        self.topics.append(self.topic_d)\n",
    "        self.topic_d = \"\"\n",
    "\n",
    "\n",
    "def stream_reuters_documents(data_path=None):\n",
    "    \"\"\"Iterate over documents of the Reuters dataset.\n",
    "\n",
    "    The Reuters archive will automatically be downloaded and uncompressed if\n",
    "    the `data_path` directory does not exist.\n",
    "\n",
    "    Documents are represented as dictionaries with 'body' (str),\n",
    "    'title' (str), 'topics' (list(str)) keys.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    DOWNLOAD_URL = ('http://archive.ics.uci.edu/ml/machine-learning-databases/'\n",
    "                    'reuters21578-mld/reuters21578.tar.gz')\n",
    "    ARCHIVE_FILENAME = 'reuters21578.tar.gz'\n",
    "\n",
    "    if data_path is None:\n",
    "        data_path = os.path.join(get_data_home(), \"reuters\")\n",
    "    if not os.path.exists(data_path):\n",
    "        \"\"\"Download the dataset.\"\"\"\n",
    "        print(\"downloading dataset (once and for all) into %s\" %\n",
    "              data_path)\n",
    "        os.mkdir(data_path)\n",
    "\n",
    "        def progress(blocknum, bs, size):\n",
    "            total_sz_mb = '%.2f MB' % (size / 1e6)\n",
    "            current_sz_mb = '%.2f MB' % ((blocknum * bs) / 1e6)\n",
    "            if _not_in_sphinx():\n",
    "                print('\\rdownloaded %s / %s' % (current_sz_mb, total_sz_mb),\n",
    "                      end='')\n",
    "\n",
    "        archive_path = os.path.join(data_path, ARCHIVE_FILENAME)\n",
    "        urllib.request.urlretrieve(DOWNLOAD_URL, filename=archive_path,\n",
    "                                   reporthook=progress)\n",
    "        if _not_in_sphinx():\n",
    "            print('\\r', end='')\n",
    "        print(\"untarring Reuters dataset...\")\n",
    "        tarfile.open(archive_path, 'r:gz').extractall(data_path)\n",
    "        print(\"done.\")\n",
    "\n",
    "    parser = ReutersParser()\n",
    "    for filename in glob(os.path.join(data_path, \"*.sgm\")):\n",
    "        for doc in parser.parse(open(filename, 'rb')):\n",
    "            yield doc\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Main\n",
    "###############################################################################\n",
    "# Create the vectorizer and limit the number of features to a reasonable\n",
    "# maximum\n",
    "vectorizer = HashingVectorizer(decode_error='ignore', n_features=2 ** 18,\n",
    "                               non_negative=True)\n",
    "\n",
    "\n",
    "# Iterator over parsed Reuters SGML files.\n",
    "data_stream = stream_reuters_documents()\n",
    "\n",
    "# We learn a binary classification between the \"acq\" class and all the others.\n",
    "# \"acq\" was chosen as it is more or less evenly distributed in the Reuters\n",
    "# files. For other datasets, one should take care of creating a test set with\n",
    "# a realistic portion of positive instances.\n",
    "all_classes = np.array([0, 1])\n",
    "positive_class = 'acq'\n",
    "\n",
    "# Here are some classifiers that support the `partial_fit` method\n",
    "partial_fit_classifiers = {\n",
    "    'SGD': SGDClassifier(),\n",
    "    'Perceptron': Perceptron(),\n",
    "    'NB Multinomial': MultinomialNB(alpha=0.01),\n",
    "    'Passive-Aggressive': PassiveAggressiveClassifier(),\n",
    "}\n",
    "\n",
    "\n",
    "def get_minibatch(doc_iter, size, pos_class=positive_class):\n",
    "    \"\"\"Extract a minibatch of examples, return a tuple X_text, y.\n",
    "\n",
    "    Note: size is before excluding invalid docs with no topics assigned.\n",
    "\n",
    "    \"\"\"\n",
    "    data = [(u'{title}\\n\\n{body}'.format(**doc), pos_class in doc['topics'])\n",
    "            for doc in itertools.islice(doc_iter, size)\n",
    "            if doc['topics']]\n",
    "    if not len(data):\n",
    "        return np.asarray([], dtype=int), np.asarray([], dtype=int)\n",
    "    X_text, y = zip(*data)\n",
    "    return X_text, np.asarray(y, dtype=int)\n",
    "\n",
    "\n",
    "def iter_minibatches(doc_iter, minibatch_size):\n",
    "    \"\"\"Generator of minibatches.\"\"\"\n",
    "    X_text, y = get_minibatch(doc_iter, minibatch_size)\n",
    "    while len(X_text):\n",
    "        yield X_text, y\n",
    "        X_text, y = get_minibatch(doc_iter, minibatch_size)\n",
    "\n",
    "\n",
    "# test data statistics\n",
    "test_stats = {'n_test': 0, 'n_test_pos': 0}\n",
    "\n",
    "# First we hold out a number of examples to estimate accuracy\n",
    "n_test_documents = 1000\n",
    "tick = time.time()\n",
    "X_test_text, y_test = get_minibatch(data_stream, 1000)\n",
    "parsing_time = time.time() - tick\n",
    "tick = time.time()\n",
    "X_test = vectorizer.transform(X_test_text)\n",
    "vectorizing_time = time.time() - tick\n",
    "test_stats['n_test'] += len(y_test)\n",
    "test_stats['n_test_pos'] += sum(y_test)\n",
    "print(\"Test set is %d documents (%d positive)\" % (len(y_test), sum(y_test)))\n",
    "\n",
    "\n",
    "def progress(cls_name, stats):\n",
    "    \"\"\"Report progress information, return a string.\"\"\"\n",
    "    duration = time.time() - stats['t0']\n",
    "    s = \"%20s classifier : \\t\" % cls_name\n",
    "    s += \"%(n_train)6d train docs (%(n_train_pos)6d positive) \" % stats\n",
    "    s += \"%(n_test)6d test docs (%(n_test_pos)6d positive) \" % test_stats\n",
    "    s += \"accuracy: %(accuracy).3f \" % stats\n",
    "    s += \"in %.2fs (%5d docs/s)\" % (duration, stats['n_train'] / duration)\n",
    "    return s\n",
    "\n",
    "\n",
    "cls_stats = {}\n",
    "\n",
    "for cls_name in partial_fit_classifiers:\n",
    "    stats = {'n_train': 0, 'n_train_pos': 0,\n",
    "             'accuracy': 0.0, 'accuracy_history': [(0, 0)], 't0': time.time(),\n",
    "             'runtime_history': [(0, 0)], 'total_fit_time': 0.0}\n",
    "    cls_stats[cls_name] = stats\n",
    "\n",
    "get_minibatch(data_stream, n_test_documents)\n",
    "# Discard test set\n",
    "\n",
    "# We will feed the classifier with mini-batches of 1000 documents; this means\n",
    "# we have at most 1000 docs in memory at any time.  The smaller the document\n",
    "# batch, the bigger the relative overhead of the partial fit methods.\n",
    "minibatch_size = 1000\n",
    "\n",
    "# Create the data_stream that parses Reuters SGML files and iterates on\n",
    "# documents as a stream.\n",
    "minibatch_iterators = iter_minibatches(data_stream, minibatch_size)\n",
    "total_vect_time = 0.0\n",
    "\n",
    "# Main loop : iterate on mini-batchs of examples\n",
    "for i, (X_train_text, y_train) in enumerate(minibatch_iterators):\n",
    "\n",
    "    tick = time.time()\n",
    "    X_train = vectorizer.transform(X_train_text)\n",
    "    total_vect_time += time.time() - tick\n",
    "\n",
    "    for cls_name, cls in partial_fit_classifiers.items():\n",
    "        tick = time.time()\n",
    "        # update estimator with examples in the current mini-batch\n",
    "        cls.partial_fit(X_train, y_train, classes=all_classes)\n",
    "\n",
    "        # accumulate test accuracy stats\n",
    "        cls_stats[cls_name]['total_fit_time'] += time.time() - tick\n",
    "        cls_stats[cls_name]['n_train'] += X_train.shape[0]\n",
    "        cls_stats[cls_name]['n_train_pos'] += sum(y_train)\n",
    "        tick = time.time()\n",
    "        cls_stats[cls_name]['accuracy'] = cls.score(X_test, y_test)\n",
    "        cls_stats[cls_name]['prediction_time'] = time.time() - tick\n",
    "        acc_history = (cls_stats[cls_name]['accuracy'],\n",
    "                       cls_stats[cls_name]['n_train'])\n",
    "        cls_stats[cls_name]['accuracy_history'].append(acc_history)\n",
    "        run_history = (cls_stats[cls_name]['accuracy'],\n",
    "                       total_vect_time + cls_stats[cls_name]['total_fit_time'])\n",
    "        cls_stats[cls_name]['runtime_history'].append(run_history)\n",
    "\n",
    "        if i % 3 == 0:\n",
    "            print(progress(cls_name, cls_stats[cls_name]))\n",
    "    if i % 3 == 0:\n",
    "        print('\\n')\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Plot results\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "def plot_accuracy(x, y, x_legend):\n",
    "    \"\"\"Plot accuracy as a function of x.\"\"\"\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    plt.title('Classification accuracy as a function of %s' % x_legend)\n",
    "    plt.xlabel('%s' % x_legend)\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.grid(True)\n",
    "    plt.plot(x, y)\n",
    "\n",
    "rcParams['legend.fontsize'] = 10\n",
    "cls_names = list(sorted(cls_stats.keys()))\n",
    "\n",
    "# Plot accuracy evolution\n",
    "plt.figure()\n",
    "for _, stats in sorted(cls_stats.items()):\n",
    "    # Plot accuracy evolution with #examples\n",
    "    accuracy, n_examples = zip(*stats['accuracy_history'])\n",
    "    plot_accuracy(n_examples, accuracy, \"training examples (#)\")\n",
    "    ax = plt.gca()\n",
    "    ax.set_ylim((0.8, 1))\n",
    "plt.legend(cls_names, loc='best')\n",
    "\n",
    "plt.figure()\n",
    "for _, stats in sorted(cls_stats.items()):\n",
    "    # Plot accuracy evolution with runtime\n",
    "    accuracy, runtime = zip(*stats['runtime_history'])\n",
    "    plot_accuracy(runtime, accuracy, 'runtime (s)')\n",
    "    ax = plt.gca()\n",
    "    ax.set_ylim((0.8, 1))\n",
    "plt.legend(cls_names, loc='best')\n",
    "\n",
    "# Plot fitting times\n",
    "plt.figure()\n",
    "fig = plt.gcf()\n",
    "cls_runtime = []\n",
    "for cls_name, stats in sorted(cls_stats.items()):\n",
    "    cls_runtime.append(stats['total_fit_time'])\n",
    "\n",
    "cls_runtime.append(total_vect_time)\n",
    "cls_names.append('Vectorization')\n",
    "bar_colors = rcParams['axes.color_cycle'][:len(cls_names)]\n",
    "\n",
    "ax = plt.subplot(111)\n",
    "rectangles = plt.bar(range(len(cls_names)), cls_runtime, width=0.5,\n",
    "                     color=bar_colors)\n",
    "\n",
    "ax.set_xticks(np.linspace(0.25, len(cls_names) - 0.75, len(cls_names)))\n",
    "ax.set_xticklabels(cls_names, fontsize=10)\n",
    "ymax = max(cls_runtime) * 1.2\n",
    "ax.set_ylim((0, ymax))\n",
    "ax.set_ylabel('runtime (s)')\n",
    "ax.set_title('Training Times')\n",
    "\n",
    "\n",
    "def autolabel(rectangles):\n",
    "    \"\"\"attach some text vi autolabel on rectangles.\"\"\"\n",
    "    for rect in rectangles:\n",
    "        height = rect.get_height()\n",
    "        ax.text(rect.get_x() + rect.get_width() / 2.,\n",
    "                1.05 * height, '%.4f' % height,\n",
    "                ha='center', va='bottom')\n",
    "\n",
    "autolabel(rectangles)\n",
    "plt.show()\n",
    "\n",
    "# Plot prediction times\n",
    "plt.figure()\n",
    "#fig = plt.gcf()\n",
    "cls_runtime = []\n",
    "cls_names = list(sorted(cls_stats.keys()))\n",
    "for cls_name, stats in sorted(cls_stats.items()):\n",
    "    cls_runtime.append(stats['prediction_time'])\n",
    "cls_runtime.append(parsing_time)\n",
    "cls_names.append('Read/Parse\\n+Feat.Extr.')\n",
    "cls_runtime.append(vectorizing_time)\n",
    "cls_names.append('Hashing\\n+Vect.')\n",
    "bar_colors = rcParams['axes.color_cycle'][:len(cls_names)]\n",
    "\n",
    "ax = plt.subplot(111)\n",
    "rectangles = plt.bar(range(len(cls_names)), cls_runtime, width=0.5,\n",
    "                     color=bar_colors)\n",
    "\n",
    "ax.set_xticks(np.linspace(0.25, len(cls_names) - 0.75, len(cls_names)))\n",
    "ax.set_xticklabels(cls_names, fontsize=8)\n",
    "plt.setp(plt.xticks()[1], rotation=30)\n",
    "ymax = max(cls_runtime) * 1.2\n",
    "ax.set_ylim((0, ymax))\n",
    "ax.set_ylabel('runtime (s)')\n",
    "ax.set_title('Prediction Times (%d instances)' % n_test_documents)\n",
    "autolabel(rectangles)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q1.1.2 Explore how many documents are in the dataset, how many categories, how many documents per categories, provide mean and standard deviation, min and max. (Hint: use the pandas library to explore the dataset, use the dataframe.describe() method.)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Note: installed modules: pandas, request, dict, public, self, get, query_string, post\n",
    "~~Tried to import panda instead of pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of df is:  <class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>title</th>\n",
       "      <th>topics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chrysler Corp said car sales for the March 21-...</td>\n",
       "      <td>CHRYSLER &lt;C&gt; LATE MARCH U.S. CAR SALES UP</td>\n",
       "      <td>[usa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Compaq Computer Corp, IBM's chief rival in the...</td>\n",
       "      <td>WALL STREET STOCKS/COMPAQ COMPUTER &lt;CPQ&gt;</td>\n",
       "      <td>[usa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;Noranda Inc&gt; said production will remain shut...</td>\n",
       "      <td>NORANDA SETS TEMPORARY MINE SHUTDOWN</td>\n",
       "      <td>[copper, canada]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Canadian government's budget deficit rose ...</td>\n",
       "      <td>CANADA BUDGET DEFICIT RISES IN JANUARY</td>\n",
       "      <td>[canada]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CIS Technologies Inc said it executed a formal...</td>\n",
       "      <td>CIS TECHNOLOGIES&lt;CIH&gt; TO SELL SHARES TO SWISS CO</td>\n",
       "      <td>[acq, usa, switzerland]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Qtly div 42 cts vs 41.5 cts prior Payable APri...</td>\n",
       "      <td>COPLEY PROPERTIES INC &lt;COP&gt; INCREASES DIVIDEND</td>\n",
       "      <td>[earn, usa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Colombia's cost of living index rose 2.71 pct ...</td>\n",
       "      <td>COLOMBIAN INFLATION STABLE AT AROUND 20 PCT</td>\n",
       "      <td>[cpi, colombia]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Federal Home Loan Bank Board said home mor...</td>\n",
       "      <td>FHLBB SAYS MORTGAGE RATES CONTINUE DECLINE</td>\n",
       "      <td>[interest, usa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The New York Stock Exchange said a seat on the...</td>\n",
       "      <td>NYFE SEAT SELLS FOR 1,500 DLRS</td>\n",
       "      <td>[usa, nyse]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td></td>\n",
       "      <td>CANADIAN MONEY SUPPLY M-1 FALLS 291 MLN DLRS I...</td>\n",
       "      <td>[money-supply, canada]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Beneficial Corp said the sale of its American ...</td>\n",
       "      <td>BENEFICIAL &lt;BNL&gt; UNIT SALE APPROVED</td>\n",
       "      <td>[acq, usa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>European options exchanges will see spectacula...</td>\n",
       "      <td>LARGER VOLUME SEEN ON EUROPEAN OPTIONS EXCHANGES</td>\n",
       "      <td>[netherlands, ase, cboe]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Tierco Group INc said it sold at par to the Ku...</td>\n",
       "      <td>TIERCO &lt;TIER&gt; SELLS NOTE</td>\n",
       "      <td>[usa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Shr 5.56 dlrs vs 3.88 dlrs Net 47.5 mln vs 33....</td>\n",
       "      <td>&lt;ITT CANADA LTD&gt; YEAR NET</td>\n",
       "      <td>[earn, canada]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>California Micro Devices Corp said an addition...</td>\n",
       "      <td>CALIFORNIA MICRO DEVICES &lt;CAMD&gt; IN DEFENSE DEAL</td>\n",
       "      <td>[usa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Stewart INformation Services Corp said it resc...</td>\n",
       "      <td>STEWART INFORMATION RESCHEDULES ANNUAL MEETING</td>\n",
       "      <td>[usa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>FIserve Inc said 14 savings and loans with 1.5...</td>\n",
       "      <td>FISERVE &lt;FISV&gt; GETS BUSINESS WORTH ONE MLN DLRS</td>\n",
       "      <td>[usa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Mony Real Estate Investors Trust said its inve...</td>\n",
       "      <td>MONY REAL &lt;MYM&gt; REPORTS PORTFOLIO</td>\n",
       "      <td>[usa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Canadian narrowly-defined money supply M-1 fel...</td>\n",
       "      <td>CANADIAN MONEY SUPPLY FALLS IN WEEK</td>\n",
       "      <td>[money-supply, canada]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Copley Properties Inc said the company will in...</td>\n",
       "      <td>COPLEY PROPERTIES &lt;COP&gt; TO INVEST IN JOINT PACT</td>\n",
       "      <td>[usa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>The United Food And Commercial Workers said th...</td>\n",
       "      <td>UNION TO PROTEST DART'S SUPERMARKETS &lt;SGL&gt; BID</td>\n",
       "      <td>[acq, usa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Pan Am Corp's Pan American World Airways said ...</td>\n",
       "      <td>PAN AM &lt;PN&gt; MARCH LOAD FACTOR ROSE TO 60.6 PCT</td>\n",
       "      <td>[usa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>FCS Laboratories Inc said merger discussions w...</td>\n",
       "      <td>FCS LABORATORIES &lt;FCSI&gt; TERMINATES DEAL TALKS</td>\n",
       "      <td>[acq, usa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>The White House said the rise in interest rate...</td>\n",
       "      <td>WHITE HOUSE SAYS INTEREST RATES REFLECT MARKET</td>\n",
       "      <td>[interest, usa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Shr loss nine cts Net loss 1.4 mln Revs 630,11...</td>\n",
       "      <td>REGAL PETROLEUM LTD &lt;RPLO&gt; YEAR</td>\n",
       "      <td>[earn, usa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Shr 1.1 cts vs 1.7 cts Net 26,708 vs 35,084 Re...</td>\n",
       "      <td>CANTERBURY PRESS INC YEAR NOV 30</td>\n",
       "      <td>[earn, usa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Shr nine cts vs 19 cts Net 188,000 vs 362,000 ...</td>\n",
       "      <td>GRAPHIC MEDIA INC &lt;GMED&gt; YEAR</td>\n",
       "      <td>[earn, usa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Burlington Northern Inc said its Burlington No...</td>\n",
       "      <td>BURLINGTON &lt;BNI&gt; UNIT SETTLES BONDHOLDER SUIT</td>\n",
       "      <td>[usa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21548</th>\n",
       "      <td>Hundreds of marines were on alert at 11 key Br...</td>\n",
       "      <td>BRAZIL SEAMEN CONTINUE STRIKE AFTER COURT DECI...</td>\n",
       "      <td>[ship, brazil]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21549</th>\n",
       "      <td>Indonesia's Armed Forces Commander General Ben...</td>\n",
       "      <td>INDONESIA SAID TO BE STABLE AHEAD OF ELECTIONS</td>\n",
       "      <td>[indonesia]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21550</th>\n",
       "      <td>The suspension of Ecuador's crude oil shipment...</td>\n",
       "      <td>ECUADOR TO EXPORT NO OIL FOR FOUR MONTHS, OFFI...</td>\n",
       "      <td>[crude, ecuador]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21551</th>\n",
       "      <td>Any European Community decision to liberalise ...</td>\n",
       "      <td>EC FARM LIBERALISATION SEEN HURTING THAI TAPIOCA</td>\n",
       "      <td>[tapioca, meal-feed, thailand, ec]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21552</th>\n",
       "      <td>China has raised the prices it pays farmers fo...</td>\n",
       "      <td>CHINA RAISES CROP PRICES TO INCREASE OUTPUT</td>\n",
       "      <td>[cotton, sugar, veg-oil, grain, china]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21553</th>\n",
       "      <td>Australia sold 180,000 tonnes of raw sugar to ...</td>\n",
       "      <td>AUSTRALIA SELLS 180,000 TONNES OF SUGAR TO USSR</td>\n",
       "      <td>[sugar, ussr, australia]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21554</th>\n",
       "      <td>Thai natural rubber exports rose to 763,331 to...</td>\n",
       "      <td>THAI NATURAL RUBBER EXPORTS RISE IN 1986</td>\n",
       "      <td>[rubber, thailand]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21555</th>\n",
       "      <td>Australian beef output declined to 104,353 ton...</td>\n",
       "      <td>AUSTRALIAN BEEF OUTPUT DECLINES IN JANUARY</td>\n",
       "      <td>[carcass, livestock, australia]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21556</th>\n",
       "      <td>Increased federal government borrowing needs a...</td>\n",
       "      <td>GERMAN GOVERNMENT NEEDS SEEN RAISING BOND YIELDS</td>\n",
       "      <td>[interest, west-germany]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21557</th>\n",
       "      <td>Philippine President Corazon Aquino swore in f...</td>\n",
       "      <td>AQUINO SWEARS IN FOUR CABINET SECRETARIES</td>\n",
       "      <td>[philippines, aquino]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21558</th>\n",
       "      <td>Kenya and Shenzhen Electronics Group (SEG) of ...</td>\n",
       "      <td>CHINA, KENYA IN ELECTRONIC GOODS JOINT VENTURE</td>\n",
       "      <td>[china, kenya]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21559</th>\n",
       "      <td>The Soviet Union has agreed to cut its coking ...</td>\n",
       "      <td>USSR TO CUT COAL PRICE FOR JAPANESE STEELMILLS</td>\n",
       "      <td>[iron-steel, japan, ussr]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21560</th>\n",
       "      <td>&lt;Placer Pacific Ltd&gt; said it hopes the Papua N...</td>\n",
       "      <td>PLACER PACIFIC HOPES FOR MISIMA GOLD APPROVAL ...</td>\n",
       "      <td>[gold, australia, papua-new-guinea]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21561</th>\n",
       "      <td>The special prosecutor in the Iran arms scanda...</td>\n",
       "      <td>PAPER SAYS INDICTMENTS IN IRAN CASE EXPECTED</td>\n",
       "      <td>[usa, iran]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21562</th>\n",
       "      <td>The Bangladesh trade deficit narrowed to 1.91 ...</td>\n",
       "      <td>BANGLADESH TRADE DEFICIT NARROWS IN OCTOBER 1986</td>\n",
       "      <td>[trade, bangladesh]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21563</th>\n",
       "      <td>The outlook for Australian industrial investme...</td>\n",
       "      <td>AUSTRALIAN INVESTMENT OUTLOOK SEEN AS UNCERTAIN</td>\n",
       "      <td>[australia]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21564</th>\n",
       "      <td>The central bank has issued 12 billion Taiwan ...</td>\n",
       "      <td>TAIWAN ISSUES 12 BILLION DLRS OF BONDS</td>\n",
       "      <td>[taiwan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21565</th>\n",
       "      <td>Three Japanese credit rating agencies are ente...</td>\n",
       "      <td>JAPAN RATING AGENCIES IN BATTLE WITH U.S. GIANTS</td>\n",
       "      <td>[usa, japan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21566</th>\n",
       "      <td>Most commodity agreements are close to collaps...</td>\n",
       "      <td>GROUP-77 OFFICIALS SET AGENDA FOR DHAKA MEETING</td>\n",
       "      <td>[bangladesh]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21567</th>\n",
       "      <td>Swiss industrial output rose nine pct in the f...</td>\n",
       "      <td>SWISS INDUSTRIAL OUTPUT RISES IN FOURTH QQUARTER</td>\n",
       "      <td>[ipi, switzerland]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21568</th>\n",
       "      <td>Japanese securities houses are thinking of all...</td>\n",
       "      <td>FOREIGN BROKERS MAY GET MORE ACCESS TO JAPAN B...</td>\n",
       "      <td>[usa, japan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21569</th>\n",
       "      <td>The United States has openly attacked Japan's ...</td>\n",
       "      <td>U.S. SEEKS MAJOR JAPANESE ECONOMIC CHANGE IN A...</td>\n",
       "      <td>[japan, usa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21570</th>\n",
       "      <td>The Asian dollar market continued to expand in...</td>\n",
       "      <td>ASIAN DOLLAR ASSETS EXCEED 200 BILLION DLRS</td>\n",
       "      <td>[money-fx, singapore]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21571</th>\n",
       "      <td>Singapore's M-1 money supply rose 3.7 pct duri...</td>\n",
       "      <td>SINGAPORE M-1 MONEY SUPPLY UP 3.7 PCT IN DECEMBER</td>\n",
       "      <td>[money-supply, singapore]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21572</th>\n",
       "      <td>The value of China's industrial output in Janu...</td>\n",
       "      <td>CHINESE INDUSTRIAL GROWTH RATE UP AFTER WEAK 1986</td>\n",
       "      <td>[ipi, china]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21573</th>\n",
       "      <td>The Dutch central bank said it has accepted bi...</td>\n",
       "      <td>NEW DUTCH ADVANCES TOTAL 6.5 BILLION GUILDERS</td>\n",
       "      <td>[money-fx, interest, netherlands]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21574</th>\n",
       "      <td>Union Bank of Finland is issuing a 10 billion ...</td>\n",
       "      <td>UNION BANK OF FINLAND ISSUES EUROYEN BOND</td>\n",
       "      <td>[uk, finland]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21575</th>\n",
       "      <td>Japanese customers have bought nearly six mln ...</td>\n",
       "      <td>IRAN SELLING DISCOUNTED CRUDE, JAPAN TRADERS SAY</td>\n",
       "      <td>[crude, japan, iran]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21576</th>\n",
       "      <td>Imperial Chemical Industries Plc &lt;ICI.L&gt; said ...</td>\n",
       "      <td>ICI SELLS STAKE IN LISTER AND CO</td>\n",
       "      <td>[acq, uk]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21577</th>\n",
       "      <td>Unilever Australia Ltd is issuing 40 mln Austr...</td>\n",
       "      <td>UNILEVER UNIT ISSUES 40 MLN AUSTRALIAN DLR BOND</td>\n",
       "      <td>[uk]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21578 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    body  \\\n",
       "0      Chrysler Corp said car sales for the March 21-...   \n",
       "1      Compaq Computer Corp, IBM's chief rival in the...   \n",
       "2      <Noranda Inc> said production will remain shut...   \n",
       "3      The Canadian government's budget deficit rose ...   \n",
       "4      CIS Technologies Inc said it executed a formal...   \n",
       "5      Qtly div 42 cts vs 41.5 cts prior Payable APri...   \n",
       "6      Colombia's cost of living index rose 2.71 pct ...   \n",
       "7      The Federal Home Loan Bank Board said home mor...   \n",
       "8      The New York Stock Exchange said a seat on the...   \n",
       "9                                                          \n",
       "10     Beneficial Corp said the sale of its American ...   \n",
       "11     European options exchanges will see spectacula...   \n",
       "12     Tierco Group INc said it sold at par to the Ku...   \n",
       "13     Shr 5.56 dlrs vs 3.88 dlrs Net 47.5 mln vs 33....   \n",
       "14     California Micro Devices Corp said an addition...   \n",
       "15     Stewart INformation Services Corp said it resc...   \n",
       "16     FIserve Inc said 14 savings and loans with 1.5...   \n",
       "17     Mony Real Estate Investors Trust said its inve...   \n",
       "18     Canadian narrowly-defined money supply M-1 fel...   \n",
       "19     Copley Properties Inc said the company will in...   \n",
       "20     The United Food And Commercial Workers said th...   \n",
       "21     Pan Am Corp's Pan American World Airways said ...   \n",
       "22     FCS Laboratories Inc said merger discussions w...   \n",
       "23     The White House said the rise in interest rate...   \n",
       "24     Shr loss nine cts Net loss 1.4 mln Revs 630,11...   \n",
       "25     Shr 1.1 cts vs 1.7 cts Net 26,708 vs 35,084 Re...   \n",
       "26     Shr nine cts vs 19 cts Net 188,000 vs 362,000 ...   \n",
       "27                                                         \n",
       "28                                                         \n",
       "29     Burlington Northern Inc said its Burlington No...   \n",
       "...                                                  ...   \n",
       "21548  Hundreds of marines were on alert at 11 key Br...   \n",
       "21549  Indonesia's Armed Forces Commander General Ben...   \n",
       "21550  The suspension of Ecuador's crude oil shipment...   \n",
       "21551  Any European Community decision to liberalise ...   \n",
       "21552  China has raised the prices it pays farmers fo...   \n",
       "21553  Australia sold 180,000 tonnes of raw sugar to ...   \n",
       "21554  Thai natural rubber exports rose to 763,331 to...   \n",
       "21555  Australian beef output declined to 104,353 ton...   \n",
       "21556  Increased federal government borrowing needs a...   \n",
       "21557  Philippine President Corazon Aquino swore in f...   \n",
       "21558  Kenya and Shenzhen Electronics Group (SEG) of ...   \n",
       "21559  The Soviet Union has agreed to cut its coking ...   \n",
       "21560  <Placer Pacific Ltd> said it hopes the Papua N...   \n",
       "21561  The special prosecutor in the Iran arms scanda...   \n",
       "21562  The Bangladesh trade deficit narrowed to 1.91 ...   \n",
       "21563  The outlook for Australian industrial investme...   \n",
       "21564  The central bank has issued 12 billion Taiwan ...   \n",
       "21565  Three Japanese credit rating agencies are ente...   \n",
       "21566  Most commodity agreements are close to collaps...   \n",
       "21567  Swiss industrial output rose nine pct in the f...   \n",
       "21568  Japanese securities houses are thinking of all...   \n",
       "21569  The United States has openly attacked Japan's ...   \n",
       "21570  The Asian dollar market continued to expand in...   \n",
       "21571  Singapore's M-1 money supply rose 3.7 pct duri...   \n",
       "21572  The value of China's industrial output in Janu...   \n",
       "21573  The Dutch central bank said it has accepted bi...   \n",
       "21574  Union Bank of Finland is issuing a 10 billion ...   \n",
       "21575  Japanese customers have bought nearly six mln ...   \n",
       "21576  Imperial Chemical Industries Plc <ICI.L> said ...   \n",
       "21577  Unilever Australia Ltd is issuing 40 mln Austr...   \n",
       "\n",
       "                                                   title  \\\n",
       "0              CHRYSLER <C> LATE MARCH U.S. CAR SALES UP   \n",
       "1               WALL STREET STOCKS/COMPAQ COMPUTER <CPQ>   \n",
       "2                   NORANDA SETS TEMPORARY MINE SHUTDOWN   \n",
       "3                 CANADA BUDGET DEFICIT RISES IN JANUARY   \n",
       "4       CIS TECHNOLOGIES<CIH> TO SELL SHARES TO SWISS CO   \n",
       "5         COPLEY PROPERTIES INC <COP> INCREASES DIVIDEND   \n",
       "6            COLOMBIAN INFLATION STABLE AT AROUND 20 PCT   \n",
       "7             FHLBB SAYS MORTGAGE RATES CONTINUE DECLINE   \n",
       "8                         NYFE SEAT SELLS FOR 1,500 DLRS   \n",
       "9      CANADIAN MONEY SUPPLY M-1 FALLS 291 MLN DLRS I...   \n",
       "10                   BENEFICIAL <BNL> UNIT SALE APPROVED   \n",
       "11      LARGER VOLUME SEEN ON EUROPEAN OPTIONS EXCHANGES   \n",
       "12                              TIERCO <TIER> SELLS NOTE   \n",
       "13                             <ITT CANADA LTD> YEAR NET   \n",
       "14       CALIFORNIA MICRO DEVICES <CAMD> IN DEFENSE DEAL   \n",
       "15        STEWART INFORMATION RESCHEDULES ANNUAL MEETING   \n",
       "16       FISERVE <FISV> GETS BUSINESS WORTH ONE MLN DLRS   \n",
       "17                     MONY REAL <MYM> REPORTS PORTFOLIO   \n",
       "18                   CANADIAN MONEY SUPPLY FALLS IN WEEK   \n",
       "19       COPLEY PROPERTIES <COP> TO INVEST IN JOINT PACT   \n",
       "20        UNION TO PROTEST DART'S SUPERMARKETS <SGL> BID   \n",
       "21        PAN AM <PN> MARCH LOAD FACTOR ROSE TO 60.6 PCT   \n",
       "22         FCS LABORATORIES <FCSI> TERMINATES DEAL TALKS   \n",
       "23        WHITE HOUSE SAYS INTEREST RATES REFLECT MARKET   \n",
       "24                       REGAL PETROLEUM LTD <RPLO> YEAR   \n",
       "25                      CANTERBURY PRESS INC YEAR NOV 30   \n",
       "26                         GRAPHIC MEDIA INC <GMED> YEAR   \n",
       "27                                                         \n",
       "28                                                         \n",
       "29         BURLINGTON <BNI> UNIT SETTLES BONDHOLDER SUIT   \n",
       "...                                                  ...   \n",
       "21548  BRAZIL SEAMEN CONTINUE STRIKE AFTER COURT DECI...   \n",
       "21549     INDONESIA SAID TO BE STABLE AHEAD OF ELECTIONS   \n",
       "21550  ECUADOR TO EXPORT NO OIL FOR FOUR MONTHS, OFFI...   \n",
       "21551   EC FARM LIBERALISATION SEEN HURTING THAI TAPIOCA   \n",
       "21552        CHINA RAISES CROP PRICES TO INCREASE OUTPUT   \n",
       "21553    AUSTRALIA SELLS 180,000 TONNES OF SUGAR TO USSR   \n",
       "21554           THAI NATURAL RUBBER EXPORTS RISE IN 1986   \n",
       "21555         AUSTRALIAN BEEF OUTPUT DECLINES IN JANUARY   \n",
       "21556   GERMAN GOVERNMENT NEEDS SEEN RAISING BOND YIELDS   \n",
       "21557          AQUINO SWEARS IN FOUR CABINET SECRETARIES   \n",
       "21558     CHINA, KENYA IN ELECTRONIC GOODS JOINT VENTURE   \n",
       "21559     USSR TO CUT COAL PRICE FOR JAPANESE STEELMILLS   \n",
       "21560  PLACER PACIFIC HOPES FOR MISIMA GOLD APPROVAL ...   \n",
       "21561       PAPER SAYS INDICTMENTS IN IRAN CASE EXPECTED   \n",
       "21562   BANGLADESH TRADE DEFICIT NARROWS IN OCTOBER 1986   \n",
       "21563    AUSTRALIAN INVESTMENT OUTLOOK SEEN AS UNCERTAIN   \n",
       "21564             TAIWAN ISSUES 12 BILLION DLRS OF BONDS   \n",
       "21565   JAPAN RATING AGENCIES IN BATTLE WITH U.S. GIANTS   \n",
       "21566    GROUP-77 OFFICIALS SET AGENDA FOR DHAKA MEETING   \n",
       "21567   SWISS INDUSTRIAL OUTPUT RISES IN FOURTH QQUARTER   \n",
       "21568  FOREIGN BROKERS MAY GET MORE ACCESS TO JAPAN B...   \n",
       "21569  U.S. SEEKS MAJOR JAPANESE ECONOMIC CHANGE IN A...   \n",
       "21570        ASIAN DOLLAR ASSETS EXCEED 200 BILLION DLRS   \n",
       "21571  SINGAPORE M-1 MONEY SUPPLY UP 3.7 PCT IN DECEMBER   \n",
       "21572  CHINESE INDUSTRIAL GROWTH RATE UP AFTER WEAK 1986   \n",
       "21573      NEW DUTCH ADVANCES TOTAL 6.5 BILLION GUILDERS   \n",
       "21574          UNION BANK OF FINLAND ISSUES EUROYEN BOND   \n",
       "21575   IRAN SELLING DISCOUNTED CRUDE, JAPAN TRADERS SAY   \n",
       "21576                   ICI SELLS STAKE IN LISTER AND CO   \n",
       "21577    UNILEVER UNIT ISSUES 40 MLN AUSTRALIAN DLR BOND   \n",
       "\n",
       "                                       topics  \n",
       "0                                       [usa]  \n",
       "1                                       [usa]  \n",
       "2                            [copper, canada]  \n",
       "3                                    [canada]  \n",
       "4                     [acq, usa, switzerland]  \n",
       "5                                 [earn, usa]  \n",
       "6                             [cpi, colombia]  \n",
       "7                             [interest, usa]  \n",
       "8                                 [usa, nyse]  \n",
       "9                      [money-supply, canada]  \n",
       "10                                 [acq, usa]  \n",
       "11                   [netherlands, ase, cboe]  \n",
       "12                                      [usa]  \n",
       "13                             [earn, canada]  \n",
       "14                                      [usa]  \n",
       "15                                      [usa]  \n",
       "16                                      [usa]  \n",
       "17                                      [usa]  \n",
       "18                     [money-supply, canada]  \n",
       "19                                      [usa]  \n",
       "20                                 [acq, usa]  \n",
       "21                                      [usa]  \n",
       "22                                 [acq, usa]  \n",
       "23                            [interest, usa]  \n",
       "24                                [earn, usa]  \n",
       "25                                [earn, usa]  \n",
       "26                                [earn, usa]  \n",
       "27                                         []  \n",
       "28                                         []  \n",
       "29                                      [usa]  \n",
       "...                                       ...  \n",
       "21548                          [ship, brazil]  \n",
       "21549                             [indonesia]  \n",
       "21550                        [crude, ecuador]  \n",
       "21551      [tapioca, meal-feed, thailand, ec]  \n",
       "21552  [cotton, sugar, veg-oil, grain, china]  \n",
       "21553                [sugar, ussr, australia]  \n",
       "21554                      [rubber, thailand]  \n",
       "21555         [carcass, livestock, australia]  \n",
       "21556                [interest, west-germany]  \n",
       "21557                   [philippines, aquino]  \n",
       "21558                          [china, kenya]  \n",
       "21559               [iron-steel, japan, ussr]  \n",
       "21560     [gold, australia, papua-new-guinea]  \n",
       "21561                             [usa, iran]  \n",
       "21562                     [trade, bangladesh]  \n",
       "21563                             [australia]  \n",
       "21564                                [taiwan]  \n",
       "21565                            [usa, japan]  \n",
       "21566                            [bangladesh]  \n",
       "21567                      [ipi, switzerland]  \n",
       "21568                            [usa, japan]  \n",
       "21569                            [japan, usa]  \n",
       "21570                   [money-fx, singapore]  \n",
       "21571               [money-supply, singapore]  \n",
       "21572                            [ipi, china]  \n",
       "21573       [money-fx, interest, netherlands]  \n",
       "21574                           [uk, finland]  \n",
       "21575                    [crude, japan, iran]  \n",
       "21576                               [acq, uk]  \n",
       "21577                                    [uk]  \n",
       "\n",
       "[21578 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#From the code, accessing the reuters document data base. \n",
    "data_stream = stream_reuters_documents()\n",
    "#Experimenting with functions given in hint. \n",
    "df = pd.DataFrame(data_stream)\n",
    "print(\"The type of df is: \", type(df))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many documents in the dataset? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21578"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['title'].describe()['count']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Chrysler Corp said car sales for the March 21-...\n",
       "1        Compaq Computer Corp, IBM's chief rival in the...\n",
       "2        <Noranda Inc> said production will remain shut...\n",
       "3        The Canadian government's budget deficit rose ...\n",
       "4        CIS Technologies Inc said it executed a formal...\n",
       "5        Qtly div 42 cts vs 41.5 cts prior Payable APri...\n",
       "6        Colombia's cost of living index rose 2.71 pct ...\n",
       "7        The Federal Home Loan Bank Board said home mor...\n",
       "8        The New York Stock Exchange said a seat on the...\n",
       "9                                                         \n",
       "10       Beneficial Corp said the sale of its American ...\n",
       "11       European options exchanges will see spectacula...\n",
       "12       Tierco Group INc said it sold at par to the Ku...\n",
       "13       Shr 5.56 dlrs vs 3.88 dlrs Net 47.5 mln vs 33....\n",
       "14       California Micro Devices Corp said an addition...\n",
       "15       Stewart INformation Services Corp said it resc...\n",
       "16       FIserve Inc said 14 savings and loans with 1.5...\n",
       "17       Mony Real Estate Investors Trust said its inve...\n",
       "18       Canadian narrowly-defined money supply M-1 fel...\n",
       "19       Copley Properties Inc said the company will in...\n",
       "20       The United Food And Commercial Workers said th...\n",
       "21       Pan Am Corp's Pan American World Airways said ...\n",
       "22       FCS Laboratories Inc said merger discussions w...\n",
       "23       The White House said the rise in interest rate...\n",
       "24       Shr loss nine cts Net loss 1.4 mln Revs 630,11...\n",
       "25       Shr 1.1 cts vs 1.7 cts Net 26,708 vs 35,084 Re...\n",
       "26       Shr nine cts vs 19 cts Net 188,000 vs 362,000 ...\n",
       "27                                                        \n",
       "28                                                        \n",
       "29       Burlington Northern Inc said its Burlington No...\n",
       "                               ...                        \n",
       "21548    Hundreds of marines were on alert at 11 key Br...\n",
       "21549    Indonesia's Armed Forces Commander General Ben...\n",
       "21550    The suspension of Ecuador's crude oil shipment...\n",
       "21551    Any European Community decision to liberalise ...\n",
       "21552    China has raised the prices it pays farmers fo...\n",
       "21553    Australia sold 180,000 tonnes of raw sugar to ...\n",
       "21554    Thai natural rubber exports rose to 763,331 to...\n",
       "21555    Australian beef output declined to 104,353 ton...\n",
       "21556    Increased federal government borrowing needs a...\n",
       "21557    Philippine President Corazon Aquino swore in f...\n",
       "21558    Kenya and Shenzhen Electronics Group (SEG) of ...\n",
       "21559    The Soviet Union has agreed to cut its coking ...\n",
       "21560    <Placer Pacific Ltd> said it hopes the Papua N...\n",
       "21561    The special prosecutor in the Iran arms scanda...\n",
       "21562    The Bangladesh trade deficit narrowed to 1.91 ...\n",
       "21563    The outlook for Australian industrial investme...\n",
       "21564    The central bank has issued 12 billion Taiwan ...\n",
       "21565    Three Japanese credit rating agencies are ente...\n",
       "21566    Most commodity agreements are close to collaps...\n",
       "21567    Swiss industrial output rose nine pct in the f...\n",
       "21568    Japanese securities houses are thinking of all...\n",
       "21569    The United States has openly attacked Japan's ...\n",
       "21570    The Asian dollar market continued to expand in...\n",
       "21571    Singapore's M-1 money supply rose 3.7 pct duri...\n",
       "21572    The value of China's industrial output in Janu...\n",
       "21573    The Dutch central bank said it has accepted bi...\n",
       "21574    Union Bank of Finland is issuing a 10 billion ...\n",
       "21575    Japanese customers have bought nearly six mln ...\n",
       "21576    Imperial Chemical Industries Plc <ICI.L> said ...\n",
       "21577    Unilever Australia Ltd is issuing 40 mln Austr...\n",
       "Name: body, dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['body']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 21,578 as seen below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['body', 'title', 'topics']\n",
      "count     21578\n",
      "unique    18763\n",
      "top            \n",
      "freq       2535\n",
      "Name: body, dtype: object\n",
      "0                                         [usa]\n",
      "1                                         [usa]\n",
      "2                              [copper, canada]\n",
      "3                                      [canada]\n",
      "4                       [acq, usa, switzerland]\n",
      "5                                   [earn, usa]\n",
      "6                               [cpi, colombia]\n",
      "7                               [interest, usa]\n",
      "8                                   [usa, nyse]\n",
      "9                        [money-supply, canada]\n",
      "10                                   [acq, usa]\n",
      "11                     [netherlands, ase, cboe]\n",
      "12                                        [usa]\n",
      "13                               [earn, canada]\n",
      "14                                        [usa]\n",
      "15                                        [usa]\n",
      "16                                        [usa]\n",
      "17                                        [usa]\n",
      "18                       [money-supply, canada]\n",
      "19                                        [usa]\n",
      "20                                   [acq, usa]\n",
      "21                                        [usa]\n",
      "22                                   [acq, usa]\n",
      "23                              [interest, usa]\n",
      "24                                  [earn, usa]\n",
      "25                                  [earn, usa]\n",
      "26                                  [earn, usa]\n",
      "27                                           []\n",
      "28                                           []\n",
      "29                                        [usa]\n",
      "                          ...                  \n",
      "21548                            [ship, brazil]\n",
      "21549                               [indonesia]\n",
      "21550                          [crude, ecuador]\n",
      "21551        [tapioca, meal-feed, thailand, ec]\n",
      "21552    [cotton, sugar, veg-oil, grain, china]\n",
      "21553                  [sugar, ussr, australia]\n",
      "21554                        [rubber, thailand]\n",
      "21555           [carcass, livestock, australia]\n",
      "21556                  [interest, west-germany]\n",
      "21557                     [philippines, aquino]\n",
      "21558                            [china, kenya]\n",
      "21559                 [iron-steel, japan, ussr]\n",
      "21560       [gold, australia, papua-new-guinea]\n",
      "21561                               [usa, iran]\n",
      "21562                       [trade, bangladesh]\n",
      "21563                               [australia]\n",
      "21564                                  [taiwan]\n",
      "21565                              [usa, japan]\n",
      "21566                              [bangladesh]\n",
      "21567                        [ipi, switzerland]\n",
      "21568                              [usa, japan]\n",
      "21569                              [japan, usa]\n",
      "21570                     [money-fx, singapore]\n",
      "21571                 [money-supply, singapore]\n",
      "21572                              [ipi, china]\n",
      "21573         [money-fx, interest, netherlands]\n",
      "21574                             [uk, finland]\n",
      "21575                      [crude, japan, iran]\n",
      "21576                                 [acq, uk]\n",
      "21577                                      [uk]\n",
      "Name: topics, dtype: object\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "#Some exploring of dataset: \n",
    "if q1Verbose:\n",
    "    print(list(df))\n",
    "    print(df['body'].describe())\n",
    "    # Since df['topics'].descibe() does not work, let's just print. \n",
    "    print(df['topics'])\n",
    "    print(type(df['topics']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Each document can belong to a few topics (we interperted topics for categories). We will feed all information into a FreqDist and receive statistics.\n",
    "\n",
    "As implied, this means a document can appear in several documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a list of all occurences of all topics and feed to FreqDist. \n",
    "freq_dist = nltk.FreqDist(sum(list(df['topics']), []))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many categories: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of categories is:  445\n"
     ]
    }
   ],
   "source": [
    "category_set=set(sum(list(df['topics']), []))\n",
    "num_of_categories = len(category_set)\n",
    "print(\"The number of categories is: \",num_of_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are using a Frequency distribution, the number of categories can also be retrieved by: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "445"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(freq_dist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many documents per category: \n",
    "Since we are using a Frequency distribution, the number of documents per category \n",
    "is the value in freq_dist[category]. \n",
    "\n",
    "We can print a list of all categories and the number of documents in them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category:  sourrouille has  4 Docs\n",
      "Category:  lin-oil has  2 Docs\n",
      "Category:  takeshita has  4 Docs\n",
      "Category:  petricioli has  5 Docs\n",
      "Category:  haiti has  8 Docs\n",
      "Category:  ipe has  2 Docs\n",
      "Category:  housing has  21 Docs\n",
      "Category:  oilseed has  192 Docs\n",
      "Category:  money-supply has  190 Docs\n",
      "Category:  ongpin has  25 Docs\n"
     ]
    }
   ],
   "source": [
    "cat_numOfDocs = [(category, freq_dist[category]) for category in category_set]\n",
    "for pair in cat_numOfDocs[:10]: \n",
    "    print('Category: ', pair[0], 'has ', pair[1], 'Docs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Provide mean and standard deviation, min and max. \n",
    "Mean: Mean number of documents per categorie. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#[num_of_docs for _, num_of_docs in cat_numOfDocs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Mean (Average) number of documents per category is:  89.87191011235954\n",
      "The category with maximum documents is: \" usa \"which has  12542  documents.\n",
      "The category with minimum documents are: ['lin-meal', 'mitterrand', 'bfr'] who have 1 documents each. \n",
      "The standard deviation in number of documents per category is: 643.9321684195971\n"
     ]
    }
   ],
   "source": [
    "#Mean: \n",
    "#Sum of number of documents per each category. \n",
    "sum_docs_cat = sum(num_of_docs for (cat, num_of_docs) in cat_numOfDocs)\n",
    "#\n",
    "#Mean expected number of documents per categorie. \n",
    "#mean_exp=sum(freq_dist.freq(cat)*num_of_docs for (cat, num_of_docs) in cat_numOfDocs)\n",
    "#mean2 = np.mean([num_of_docs for _, num_of_docs in cat_numOfDocs])\n",
    "#print('The Mean number of documents per categorie is: ', mean_exp)\n",
    "#print('The Mean2 number of documents per categorie is: ', mean2)\n",
    "mean = sum_docs_cat/len(cat_numOfDocs)\n",
    "print('The Mean (Average) number of documents per category is: ', mean)\n",
    "\n",
    "#Max:\n",
    "print('The category with maximum documents is: \"',freq_dist.max(), '\"which has ', freq_dist[freq_dist.max()], ' documents.')\n",
    "\n",
    "#Min:\n",
    "min_num_of_docs = sorted(cat_numOfDocs ,key=lambda x: x[1])[0][1]\n",
    "cats_w_min_num_of_docs = [cat for (cat, num_of_docs) in cat_numOfDocs if num_of_docs==min_num_of_docs]\n",
    "display = 3 #Display only part of categories, not all. \n",
    "print('The category with minimum documents are:',cats_w_min_num_of_docs[:display], 'who have', min_num_of_docs, 'documents each. ' )\n",
    "\n",
    "#Standard deviation: \n",
    "std_dev = math.sqrt(sum( (math.pow(num_of_docs-mean, 2) for (_, num_of_docs) in cat_numOfDocs))/len(cat_numOfDocs))\n",
    "print('The standard deviation in number of documents per category is:', std_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1.1.3 Explore how many characters and words are present in the documents of the dataset.\n",
    "\n",
    "first we consider all diffferent word tokens and characters in code, as in a set of elements. We then calculate \n",
    "the number of all tokens and characters all together, which is more relevent to our issues. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create sets of words and characters. \n",
    "#Takes a while to run, use with care :)\n",
    "if q113_verbose: \n",
    "    word_set=set()\n",
    "    word_list=[]\n",
    "    for i in range(len(df['body'])): \n",
    "        word_set.update(word_tokenize(df['body'][i]))\n",
    "        word_list += word_tokenize(df['body'][i])\n",
    "\n",
    "    char_set=set()\n",
    "    char_list=[]\n",
    "    for word in word_set: \n",
    "        for letter in word: \n",
    "            char_set.update(letter)\n",
    "            char_list += letter\n",
    "    print('There are %d different words in all documents. ' %len(word_set))\n",
    "    print('There are %d word tokens in all documents. ' %len(word_list))\n",
    "    print('There are %d different characters in all documents. ' %len(char_set))\n",
    "    print('There are %d characters in all documents. ' %len(char_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "Since runtime is long for the two above boxes, Output given here, no need to run. \n",
    "\n",
    "Output: \n",
    "There are 76886 different words in all documents. \n",
    "There are 2854622 word tokens in all documents. \n",
    "There are 89 different characters in all documents. \n",
    "There are 568599 characters in all documents. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now construct a dictionary, That maps from article index to {num_of_words: , num_of_chars: }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "article_2words_chars = {}\n",
    "for i in range(len(df['body'])): \n",
    "    article_2words_chars[i] = (len(word_tokenize(df['body'][i])), len(df['body'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def explore_doc(i):\n",
    "    print('Document with index %d has %d words and %d letters' % (2, article_2words_chars[x][0], article_2words_chars[x][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1.1.4 Explain informally what are the classifiers that support the \"partial-fit\" method discussed in the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Informally, the classifiers that support \"partial-fit\", are classifiers who do not need to \"hold\" all the\n",
    "information they are given, at every given moment. If we attempt a slightly more formal explanation, We \n",
    "can say that the state of the classifier is changed as it learns from more inputs, yet this input is not\n",
    "a state variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1.1.5 Explain what is the hashing vectorizer used in this tutorial.\n",
    "####            Why is it important to use this vectorizer to achieve \"streaming classification\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As We have seen, We are dealing with a large amount of data. In order to make our data easier to process, \n",
    "We turn it into a sparse matrix that improves our memory usage by changing words into corresponding integers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Q1.2 Spam Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy\n",
    "from pandas import DataFrame\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "NEWLINE = '\\n'\n",
    "\n",
    "HAM = 'ham'\n",
    "SPAM = 'spam'\n",
    "\n",
    "SOURCES = [\n",
    "    ('data/spam',        SPAM),\n",
    "    ('data/easy_ham',    HAM),\n",
    "    ('data/hard_ham',    HAM),\n",
    "    ('data/beck-s',      HAM),\n",
    "    ('data/farmer-d',    HAM),\n",
    "    ('data/kaminski-v',  HAM),\n",
    "    ('data/kitchen-l',   HAM),\n",
    "    ('data/lokay-m',     HAM),\n",
    "    ('data/williams-w3', HAM),\n",
    "    ('data/BG',          SPAM),\n",
    "    ('data/GP',          SPAM),\n",
    "    ('data/SH',          SPAM)\n",
    "]\n",
    "\n",
    "SKIP_FILES = {'cmds'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def read_files(path):\n",
    "    for root, dir_names, file_names in os.walk(path):\n",
    "        for path in dir_names:\n",
    "            read_files(os.path.join(root, path))\n",
    "        for file_name in file_names:\n",
    "            if file_name not in SKIP_FILES:\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                if os.path.isfile(file_path):\n",
    "                    past_header, lines = False, []\n",
    "                    f = open(file_path, encoding=\"latin-1\")\n",
    "                    for line in f:\n",
    "                        if past_header:\n",
    "                            lines.append(line)\n",
    "                        elif line == NEWLINE:\n",
    "                            past_header = True\n",
    "                    f.close()\n",
    "                    content = NEWLINE.join(lines)\n",
    "                    yield file_path, content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_data_frame(path, classification):\n",
    "    rows = []\n",
    "    index = []\n",
    "    for file_name, text in read_files(path):\n",
    "        rows.append({'text': text, 'class': classification})\n",
    "        index.append(file_name)\n",
    "\n",
    "    data_frame = DataFrame(rows, index=index)\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = DataFrame({'text': [], 'class': []})\n",
    "for path, classification in SOURCES:\n",
    "    data = data.append(build_data_frame(path, classification))\n",
    "\n",
    "data = data.reindex(numpy.random.permutation(data.index))\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('count_vectorizer',   CountVectorizer(ngram_range=(1, 2))),\n",
    "    ('classifier',         MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k_fold = KFold(n=len(data), n_folds=6)\n",
    "scores = []\n",
    "confusion = numpy.array([[0, 0], [0, 0]])\n",
    "for train_indices, test_indices in k_fold:\n",
    "    train_text = data.iloc[train_indices]['text'].values\n",
    "    train_y = data.iloc[train_indices]['class'].values.astype(str)\n",
    "\n",
    "    test_text = data.iloc[test_indices]['text'].values\n",
    "    test_y = data.iloc[test_indices]['class'].values.astype(str)\n",
    "\n",
    "    pipeline.fit(train_text, train_y)\n",
    "    predictions = pipeline.predict(test_text)\n",
    "\n",
    "    confusion += confusion_matrix(test_y, predictions)\n",
    "    score = f1_score(test_y, predictions, pos_label=SPAM)\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Total emails classified:', len(data))\n",
    "print('Score:', sum(scores)/len(scores))\n",
    "print('Confusion matrix:')\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1.2.1 The vectorizer used in Zac Stewart's code is a CountVectorizer with unigrams and bigrams. Report the number of unigrams and bigrams used in this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1.2.1 The vectorizer used in Zac Stewart's code is a CountVectorizer with unigrams and bigrams. Report the number of unigrams and bigrams used in this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1973279 unigrams and bigrams, used in this model. \n"
     ]
    }
   ],
   "source": [
    "# Retreive the count vectorizer used in the model. \n",
    "p=pipeline.get_params()\n",
    "CountV=p['count_vectorizer']\n",
    "#Access features: \n",
    "uni_bi_grams = CountV.get_feature_names()\n",
    "print(\"There are %d unigrams and bigrams, used in this model. \" %len(uni_bi_grams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Q1.2.2 What are the 50 most frequent unigrams and bigrams in the dataset?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the', 270657.0)\n",
      "('to', 188408.0)\n",
      "('and', 139349.0)\n"
     ]
    }
   ],
   "source": [
    "def most_freq_feat(classifier, count_vector, n=50):\n",
    "    index = 0\n",
    "    features_c1_c2_count = []\n",
    "\n",
    "    for feat, c1, c2 in zip(count_vector.get_feature_names(), classifier.feature_count_[0], classifier.feature_count_[1]):\n",
    "        features_c1_c2_count.append((feat, c1 + c2))\n",
    "        index+=1\n",
    "\n",
    "    for i in sorted(features_c1_c2_count, key = lambda x: x[1], reverse=True)[:n]:     \n",
    "        print(i)\n",
    "    \n",
    "\n",
    "    \n",
    "most_freq_feat(p['classifier'], p['count_vectorizer'], n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are the 50 most frequent unigrams and bigrams per class (ham and spam)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 most occurring features in class spam: \n",
      "('the', 202071.0, 68586.0)\n",
      "1 most occurring features in class ham: \n",
      "('font', 11332.0, 91677.0)\n"
     ]
    }
   ],
   "source": [
    "#Create a list of feature name and amount of occurrences in each class. \n",
    "#Sort according to different class counter to get occurrences per class. \n",
    "def most_occurring_feat_per_class(classifier, count_vector, n=50):\n",
    "    index = 0\n",
    "    features_c1_c2_count = []\n",
    "\n",
    "    for feat, c1, c2 in zip(count_vector.get_feature_names(), classifier.feature_count_[0], classifier.feature_count_[1]):\n",
    "        features_c1_c2_count.append((feat, c1, c2))\n",
    "        index+=1\n",
    "\n",
    "    print(\"%d most occurring features in class spam: \" %n )    \n",
    "    for i in sorted(features_c1_c2_count, key = lambda x: x[1], reverse=True)[:n]:     \n",
    "        print(i)\n",
    "\n",
    "    print(\"%d most occurring features in class ham: \" %n )    \n",
    "    for i in sorted(features_c1_c2_count, key = lambda x: x[2], reverse=True)[:n]:     \n",
    "        print(i)\n",
    "\n",
    "\n",
    "    \n",
    "most_occurring_feat_per_class(p['classifier'], p['count_vectorizer'], n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Q1.2.4 List the 20 most useful features in the Naive Bayes classifier to distinguish between spam and ham (20 features for each class). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham -16.0793596043 00 005\n",
      "ham -16.0793596043 00 00am\n",
      "\n",
      "spam -4.65332188763 font\n",
      "spam -4.80685101935 br\n"
     ]
    }
   ],
   "source": [
    "#Since each features coefficient links it to it's class, and smaller coefficients classify spam and larger ham, \n",
    "#we sort according to coefficient, once normaly and once reversed, to get most informative features. \n",
    "def most_informative_feature_for_binary_classification(vectorizer, classifier, n=20):\n",
    "    class_labels = classifier.classes_\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    topn_class1 = sorted(zip(classifier.coef_[0], feature_names))[:n]\n",
    "    topn_class2 = sorted(zip(classifier.coef_[0], feature_names))[-n:]\n",
    "    \n",
    "    counter=0\n",
    "    for coef, feat in topn_class1:\n",
    "        print(class_labels[0], coef, feat)\n",
    "        if counter==20: \n",
    "            break\n",
    "    print()\n",
    "    \n",
    "    counter=0\n",
    "    for coef, feat in reversed(topn_class2):\n",
    "        print(class_labels[1], coef, feat)\n",
    "        counter+=1\n",
    "        if counter==20: \n",
    "            break\n",
    "\n",
    "most_informative_feature_for_binary_classification(p['count_vectorizer'], p['classifier'], n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1.2.5 There seems to be an imbalance in the length of spam and ham messages (see the plot in the attached notebook). We want to add a feature based on the number of words in the message in the text representation. Should the length attribute be normalized before fitting the Naive Bayes classifier? (See Sklearn pre-processing for examples.) Do you expect Logistic Regression to perform better with the new feature? Explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_data_frame(path, classification):\n",
    "    rows = []\n",
    "    index = []\n",
    "    for file_name, text in read_files(path):\n",
    "        rows.append({'text': text,'len': len(nltk.tokenize.word_tokenize(text)), 'class': classification})\n",
    "        index.append(file_name)\n",
    "\n",
    "    data_frame = DataFrame(rows, index=index)\n",
    "    return data_frame\n",
    "\n",
    "data1 = DataFrame({'text': [], 'len': [], 'class': []})\n",
    "for path, classification in SOURCES:\n",
    "    data1 = data1.append(build_data_frame(path, classification))\n",
    "\n",
    "data1 = data1.reindex(numpy.random.permutation(data.index))\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('count_vectorizer',   CountVectorizer(ngram_range=(1, 2))),\n",
    "    ('classifier',         MultinomialNB())\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-139-16e714dc9914>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdata1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'len'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'class'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassification\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mSOURCES\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mdata1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuild_data_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassification\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdata1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-130-8db0b079974a>\u001b[0m in \u001b[0;36mbuild_data_frame\u001b[1;34m(path, classification)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mread_files\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mrows\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'len'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'class'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mclassification\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-54-25822540355e>\u001b[0m in \u001b[0;36mread_files\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     10\u001b[0m                     \u001b[0mpast_header\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m                     \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"latin-1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m                     \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mpast_header\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                             \u001b[0mlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/noam/anaconda3/lib/python3.5/encodings/latin_1.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlatin_1_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data1 = DataFrame({'text': [], 'len': [], 'class': []})\n",
    "for path, classification in SOURCES:\n",
    "    data1 = data1.append(build_data_frame(path, classification))\n",
    "\n",
    "data1 = data1.reindex(numpy.random.permutation(data.index))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-7cf9d9b3c014>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFeatureUnion\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m pipeline = Pipeline([\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m#  ('features', FeatureUnion([\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#        ('body_stats', Pipeline([\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "pipeline = Pipeline([\n",
    "#  ('features', FeatureUnion([\n",
    "#        ('body_stats', Pipeline([\n",
    "#                ('stats', TextStats()),  # returns a list of dicts\n",
    "#                ('vect', DictVectorizer()),  # list of dicts -> feature matrix\n",
    "#                                ])),\n",
    "        ('count_vectorizer',   CountVectorizer(ngram_range=(1, 2))),   \n",
    "#                            ])),\n",
    "  ('classifier', MultinomialNB())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'KFold' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-f71976a04121>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mk_fold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKFold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_folds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mconfusion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtrain_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_indices\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mk_fold\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mtrain_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'KFold' is not defined"
     ]
    }
   ],
   "source": [
    "k_fold = KFold(n=len(data), n_folds=6)\n",
    "scores = []\n",
    "confusion = numpy.array([[0, 0], [0, 0]])\n",
    "for train_indices, test_indices in k_fold:\n",
    "    train_text = data.iloc[train_indices]['text'].values\n",
    "    train_y = data.iloc[train_indices]['class'].values.astype(str)\n",
    "\n",
    "    test_text = data.iloc[test_indices]['text'].values\n",
    "    test_y = data.iloc[test_indices]['class'].values.astype(str)\n",
    "\n",
    "    pipeline.fit(train_text, train_y)\n",
    "    predictions = pipeline.predict(test_text)\n",
    "\n",
    "    confusion += confusion_matrix(test_y, predictions)\n",
    "    score = f1_score(test_y, predictions, pos_label=SPAM)\n",
    "    scores.append(score)\n",
    "    \n",
    "\n",
    "print('Total emails classified:', len(data))\n",
    "print('Score:', sum(scores)/len(scores))\n",
    "print('Confusion matrix:')\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'array' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-132-ab075b9890f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m      {'height': 1, 'length': 3, 'width': 2}]\n\u001b[0;32m      6\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m array([[ 1.,  0.,  1.],   # obs.2\n\u001b[0m\u001b[0;32m      8\u001b[0m        \u001b[1;33m[\u001b[0m \u001b[1;36m2.\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;36m1.\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;36m0.\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m   \u001b[1;31m# obs.1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m        [ 1.,  3.,  2.]])  # obs.3\n",
      "\u001b[1;31mNameError\u001b[0m: name 'array' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    ">>> v = DictVectorizer(sparse=False)\n",
    ">>> d = [{'height': 1, 'length': 0, 'width': 1},\n",
    "...      {'height': 2, 'length': 1, 'width': 0},\n",
    "...      {'height': 1, 'length': 3, 'width': 2}]\n",
    ">>> v.fit_transform(d)\n",
    "array([[ 1.,  0.,  1.],   # obs.2\n",
    "       [ 2.,  1.,  0.],   # obs.1\n",
    "       [ 1.,  3.,  2.]])  # obs.3\n",
    "   # height, len., width   \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CountVectorizer(ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c=CountVectorizer(ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.fit(train_text, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin    \n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"For data grouped by feature, select subset of data at a provided key.\n",
    "\n",
    "    The data is expected to be stored in a 2D data structure, where the first\n",
    "    index is over features and the second is over samples.  i.e.\n",
    "\n",
    "    >> len(data[key]) == n_samples\n",
    "\n",
    "    Please note that this is the opposite convention to sklearn feature\n",
    "    matrixes (where the first index corresponds to sample).\n",
    "\n",
    "    ItemSelector only requires that the collection implement getitem\n",
    "    (data[key]).  Examples include: a dict of lists, 2D numpy array, Pandas\n",
    "    DataFrame, numpy record array, etc.\n",
    "\n",
    "    >> data = {'a': [1, 5, 2, 5, 2, 8],\n",
    "               'b': [9, 4, 1, 4, 1, 3]}\n",
    "    >> ds = ItemSelector(key='a')\n",
    "    >> data['a'] == ds.transform(data)\n",
    "\n",
    "    ItemSelector is not designed to handle data grouped by sample.  (e.g. a\n",
    "    list of dicts).  If your data is structured this way, consider a\n",
    "    transformer along the lines of `sklearn.feature_extraction.DictVectorizer`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    key : hashable, required\n",
    "        The key corresponding to the desired value in a mappable.\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextStats(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'length': len(text)}\n",
    "                for text in posts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_pipeline = Pipeline([\n",
    "  ('features', FeatureUnion([\n",
    "    ('count_vectorizer',   CountVectorizer(ngram_range=(1, 2))),\n",
    "    ('body_stats', Pipeline([\n",
    "                ('selector', ItemSelector(key='body')),\n",
    "                ('stats', TextStats()),  # returns a list of dicts\n",
    "                ('vect', DictVectorizer()),  # list of dicts -> feature matrix\n",
    "            ])),\n",
    "                            ])),\n",
    "  ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. Named Entity Recognition\n",
    "    Features:\n",
    "            we are intrested in creating a vectorized obejct from our data set, which will take into consideration the\n",
    "            (1) word-form (2)the POS of the word (3) ORT, (4) perfix1 ,(5) perfix2, (6) perfix3, (7) suffix1 \n",
    "            (8) suffix2, (9) suffix3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "first let's load and split our data set to test and train sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split data set to train and test data sets\n",
    "train_sents = list(nltk.corpus.conll2002.iob_sents('esp.train'))\n",
    "test_sents = list(nltk.corpus.conll2002.iob_sents('esp.testb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " we've noticed that our data is build as a list of sentences, all of which are constructed from a list of tripules in the following foramt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_sents[0]CountV.get_stop_words()\n",
    "CountV.get_params()['analyzer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='padding: 10px;'><code>[  [(&lt;WORD&gt;, &lt;POS&gt;, &lt;CLASS&gt;),....],<br />&nbsp;....[]&nbsp;...<br/>]</code>\n",
    "</div><br/>\n",
    "We would like to add another features, and will do that in a manner simmilar to the one being done\n",
    "in the <a href=\"http://nbviewer.ipython.org/github/tpeng/python-crfsuite/blob/master/examples/CoNLL%202002.ipynb\" target=\"_blank\">CoNLL Classification</a> we'll build out a dictionary with all the\n",
    "wanted features and use <i><u>DictVectorizer</u></i> to get a vectorized representation of the word\n",
    "according to it's features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
